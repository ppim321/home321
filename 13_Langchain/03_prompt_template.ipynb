{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model IO\n",
    "- 모델에 prompt를 만들어 입력하고 모델이 처리한 결과(output)을 받아서 처리하는 과정과 그 구성 요소들을 Model IO 라고 한다.     \n",
    "- Model IO는 **프롬프트 템플릿**, **언어 모델**, **출력 파서**로 구성된다\n",
    "\n",
    "\n",
    "![model id](figures/model_io.jpg)\n",
    "\n",
    "## 프롬프트(Prompt)\n",
    "- 생성형 AI 모델에게 작업을 요청하기 위해 전달하는 입력값을 말한다.\n",
    "- 일반적으로 사람이 사용하는 자연어를 사용한다.\n",
    "\n",
    "### 프롬프트 엔지니어링(Prompt Engineering)\n",
    "- 생성형 AI 모델로 부터 **원하는 결과물을 얻기 위해 프롬프트를 설계하고 최적화하는 기법** 을 말한다.\n",
    "    - 더 낳은 품질의 답변을 얻기 위해 prompt를 설계하고 최적화 하는 작업을 하는 것을 말한다.\n",
    "- 생성형 AI 모델들의 능력과 성격을 잘 이해하여 항상 일관되고 정확한 답을 하도록 작성하는 것에 중점을 둔다.\n",
    "    - 생성형 AI 모델들은 각기 다른 성능과 특성을 지닌다. 그래서 목적에 맞는 모델을 선택하고 또 모델에 맞는 프롬프트를 작성하는 것이 중요하다.\n",
    "    - 중요한 것은 **사용자 의도에 맞는 답이 일관되게 나오는 것이다.**\n",
    "        - 생성형 AI이 같은 질문에 다른 결과가 나올 수있다. 그래서 그 답변이 부분적으로 다를 수는 있지만 의미 자체가 바뀌면안된다. 이런 **일관성을 유지하도록 작성하는 것**이 중요하다.\n",
    "- 프롬프트 엔지니어링은 언어모델 구축에 소요되는 기간과 비용을 획기적으로 줄여준다.\n",
    "    - 생성형 AI 모델을 추가적으로 학습하지(fine tuning) 않고도 우리가 원하는 결과를 얻을 수있다.\n",
    "    - 프롬프트는 자연어로 작성되기 때문에 딥러닝이나 모델 구조에 대한 전문 지식 없이도 성능을 향상시킬 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 구성 요소\n",
    "\n",
    "프롬프트는 다음 네가지로 구성될 수있다. 물론 모든 프롬프트가 네가지 구성요소를 다 필요한 것은 아니다.\n",
    "\n",
    "- **지시(Instruction)**: 생성형 AI 모델이 수행할 작업에 대한 지시사항.\n",
    "- **문맥(Context)**: 더 나은 응답을 위해 모델에 제공하는 외부 정보나 추가 문맥.\n",
    "- **입력 데이터(Input Data)**: 생성형 AI 모델이 처리해야하거나 응답 해야 하는 결과에 대한 질문(query)\n",
    "- **출력 지시자(Ouptput Indicator)**: 생성형 AI 모델의 응답 출력결과의 유형이나 형식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  프롬프트 작성 기법\n",
    "1. 간단한 프롬프트로 시작해서 답변을 확인하며 다양한 요소와 context(맥락정보)들을 추가해 가면서 개선해 나간다.\n",
    "    - 한번에 좋은 프롬프트를 작성하는 것이 아니라 반복적으로 결과를 보면서 개선해 나간다.\n",
    "1. **구체적으로 작성한다.**\n",
    "    - 모델이 처리하기를 원하는 지시사항과 작업에 대해 아주 구체적으로 작성한다. 더 기술적으고 자세할 수록 결과도 더 좋다.\n",
    "    - \"인공지능에 대해 알려줘. \" -> \"인공지능이 의료분야에 어떻게 사용되고 있는지 500자 이내로 알려줘.\"\n",
    "1. 불필요한 내용은 빼고 **꼭 필요한 내용만 작성**한다.\n",
    "   - 구체적으로 작성하 되 불필요한 내용이 들어가지 않도록 한다.\n",
    "1. **명확한 작업 지시(Instruction)** 를 한다.\n",
    "    - 사용자가 얻고자 하는 정보나 수행하고자 하는 작업 즉 목적을 명확하게 기술한다. \n",
    "    - \"다음 사항을 충족하는 프로그램을 파이썬으로 작성하라\", \"다음 내용을 번역하라\", \"다음 기사를 요약하라\", \"다음 내용을 긍정,부정으로 분류하라\"와 같은 명확한 지시 명령어를 사용한다.\n",
    "1. **열린 질문 사용**\n",
    "    - 구체적인 지시 대신 AI에게 필요한 정보를 묻는 열린 질문을 통해 **우리가 생각지 못했던 다양한 관점에서 문제를 해결**할 수 있다.\n",
    "    - \"이 문제를 해결하기 위해 어떤 정보가 필요할까?\", \"인공지능에 대한 최신 연구 동향은 어떻게 되나?\"\n",
    "1. **원하는 출력 형식이 있을 경우 예시를 작성한다.**\n",
    "    ```\n",
    "     인공지능이 어떤 분야에서 사용되고 있는지 다음 형식으로 알려줘.\n",
    "     분야1, 분야2, 분야3\n",
    "    ```\n",
    "1. **출력 토큰 수 제한**\n",
    "    - 출력 토큰이 너무 길면 LLM 모델은 답변에 부연설명과 같은 꼭 필요하지 않은 내용까지 넣는 경우가 있다.     \n",
    "1. **맥락(context) 제공**\n",
    "    -  요청하는 작업의 배경을 설명한다. 이를 통해 모델이 맥락을 이해하고, 주어진 맥락 내에서 정확한 답변을 제공하는데 도움이 된다. \n",
    "1. **페르소나(역할) 지정**\n",
    "   - 질문을 하기 전에 AI에게 역할을 지정한다. \n",
    "   - ex) `너는 딥러닝 전문가야`, `너는 인사관리 분야의 전문가야`\n",
    "1. **제약 조건을 명시한다.**\n",
    "    - 글자수나 꼭 들어가야 할 것, 들어가면 안되는 것이 있을 경우 그  조건을 명시한다.     \n",
    "1. **예시를 제시한다.**\n",
    "   - **Zero-Shot Prompt**: 어떠한 예시없이 질문하는 방식\n",
    "     - ex) `1 + 1 은 얼마인가?`\n",
    "   - **One-Shot Prompt**: 어떻게 답할 지 한개의 예시를 제시한다.\n",
    "     - ex) \n",
    "     ```\n",
    "     1 + 1 = 2\n",
    "     2 + 3 는 얼마인가\n",
    "     ```\n",
    "   - **Few-Shot Prompt**: 적은 양의 예시를 제시한다.\n",
    "     - ex)\n",
    "     ```\n",
    "        1 + 1 = 2\n",
    "        2 + 2 = 4\n",
    "        3 + 3 = 6\n",
    "        4 + 4 는 얼마인가?\n",
    "     ```\n",
    "1.  **Chain of Thought(생각의 사슬-CoT)**\n",
    "  - 인간처럼 단계적 사고 과정을 통해 문제를 해결하도록 설계된 AI 모델의 사고 방식을 말한다. \n",
    "  - 복잡한 질문에 대한 답변을 생성할 때 모델이 논리적 단계들을 따라가며 점진적으로 결론에 도달하도록 하는 기법\n",
    "  - LLM에게 인과관계를 추론하는 문제의 해결을 요청할 때 전체 내용을 프롬프트로 작성해서 전달 하지 않고 **문답 형식(질문-답-연결된 질문-답-연결된 질문-답-...)으로 풀어서 요청**했을 때 더 좋은 추론능력을 보여준다.\n",
    "      - **Zero-shot Chain of Thought**\n",
    "          - 프롬프트에 \"단계적으로 풀어보자\", \"차근 차근 생각해보자\", \"Think step by step\" 같은 말을 입력해 모델이 스스로 단계를 밟아가며 추론하도록 한다.\n",
    "      -  **Few-shot Chain of Thought**\n",
    "          -  직접 문답 형식의 예를 작성해서 프롬프트에 넣는다.\n",
    "          ```\n",
    "          질문: 가게에서 사과를 4개 샀고, 한 개에 3달러입니다. 총 비용은 얼마인가요?\n",
    "          CoT:  1. 사과 1개의 가격은 3달러이다.\n",
    "                2. 사과는 총 4개를 샀다.\n",
    "                3. 4개의 사과 가격은 4 × 3 = 12달러이다.\n",
    "          답: 총 비용은 12달러입니다.\n",
    "\n",
    "          질문: 하나에 32000원인 음식을 3명이 먹었다면 총비용은 얼마인가요?\n",
    "          ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 공유 사이트\n",
    "- [Langchain Hub](https://smith.langchain.com/hub)\n",
    "- [Promry](https://www.promry.com/ko)\n",
    "- [오픈프롬프트](https://www.prpt.ai)\n",
    "- [GPT테이블](https://gptable.net)\n",
    "- [Prompt Hero](https://prompthero.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "- 프롬프트 템플릿은 언어 모델에 입력할 프롬프트를 생성하는 재사용 가능한 방식이다.\n",
    "- 변수를 사용한 템플릿을 만들고 동적으로 입력이 가능하게 프롬프트의 재사용성을 높인다.\n",
    "\n",
    "### 종류\n",
    "- PromptTemplate\n",
    "    - 지시형 프롬프트 생성시 사용하는 가장 기본적인 텍스트 기반 템플릿\n",
    "    - 단순 텍스트 포맷팅에 사용\n",
    "    - 변수를 중괄호 {}로 표시\n",
    "- ChatPromptTemplate\n",
    "    - 대화형(chat) 형식의 프롬프트 생성시 사용되는 템플릿\n",
    "    - 시스템, 사용자, AI 메시지 등 다양한 역할 지정 가능\n",
    "    - 대화 맥락 유지에 효과적\n",
    "- FewShotPromptTemplate\n",
    "    - 예제를 포함한 프롬프트 생성시 사용되는 템플릿\n",
    "    \n",
    "### 사용법\n",
    "- template은 **문자열로 정의**하고 변수는 `{변수명}` 형식으로 지정한다. (변수는 차후 입력할 값으로 대체된다)\n",
    "    - 예) \"{country}의 수도는 어디인가요?\"\n",
    "- 생성한 template 문자열을 이용해 Prompt Template 객체를 생성한다.\n",
    "- **공통 method**\n",
    "    - template 생성 메소드\n",
    "        - `from_template()`\n",
    "            - 지시형 template 정의.\n",
    "            - string 타입 prompt를 생성한다.\n",
    "            - PromptTemplate, ChatPromptTemplate의 메소드.\n",
    "        - `from_messages()`\n",
    "            - 대화형 template 정의.\n",
    "            - Message 타입 객체로 prompt를 생성한다.\n",
    "            - ChatPromptTemplate의 메소드.\n",
    "    - 템플릿 완성해서 **prompt 생성하는 메소드**\n",
    "        - `format(변수=넣을값, 변수=넣을값, ..)`\n",
    "            - template 변수에 넣을 값들을 keyword arguments로 전달.\n",
    "            - Prompt template을 string으로 반환\n",
    "        - `format_messages(변수=넣을값, 변수=넣을값, ...)`\n",
    "            - ChatMessageTemplate에서 사용\n",
    "            - template 변수에 넣을 값들을 keyword arguments로 전달.\n",
    "            - Prompt를 list로 반환. List[Message] \n",
    "        - `invoke(dict)`\n",
    "            - template 변수에 넣을 값들을 dictionary로 모아서 전달. `key: 변수이름, value: 넣을값\n",
    "         \n",
    "> ### invoke() 메소드\n",
    "> - **[Runnable](05_chaing_LECL.ipynb#Runnable)** 타입의 공통 메소드\n",
    ">     - Model IO 를 구성하는 클래스들은 모두 Runnable를 구현한 하위클래스다. \n",
    "> - Runnable은 LECL 을 이용한 chain(연속된 작업들을 순서대로 연결한 것)을 만들 때 **Chain을 구성하는 작업단위 클래스**들의 최상의 클래스다.\n",
    "> - **invoke() 메소드는 입력 데이터를 처리하여 그 결과를 반환하는 Runnable 객체의 공통 메소드다.**\n",
    ">     - invoke() 의 반환 값은 다음 작업단위의 invoke() 메소드로 전달된다.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "message = \"{country}의 수도는 어디입니까?\"    # {변수명}: 값으로 바뀔 위치.  -> 변수\n",
    "# PromptTemplate 생성\n",
    "\n",
    "# initializer를 이용해 생성.\n",
    "# PromptTemplate.from_template()\n",
    "prompt_template = PromptTemplate(\n",
    "    template=message,\n",
    "    # input_variables=[\"country\"]  # 변수를 등록 -> ㅍ0.3에서는 생략 가능.\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수들: ['country']\n",
      "template: {country}의 수도는 어디입니까?\n"
     ]
    }
   ],
   "source": [
    "print(\"변수들:\", prompt_template.input_variables)\n",
    "print(\"template:\", prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 수도는 어디입니까?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(country=\"한국\")  # 변수명= 넣을값.  -> wkargs(keyword 인자)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영국의 수도는 어디입니까?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = prompt_template.format(country=\"영국\")\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='미국의 수도는 어디입니까?')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3 = prompt_template.invoke({\"country\":\"미국\"})  # invoke(dictionary로 변수: 넣을 값 들을 묶어서 전달)\n",
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "res = model.invoke(prompt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 워싱턴 D.C.입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 서울과 부산 간의 거리는 얼마인가요?\n",
      "답변: 서울과 부산 간의 거리는 약 325킬로미터(약 202마일)입니다. 이 거리는 도로를 통해 이동할 때의 거리이며, 기차나 비행기를 이용할 경우 약간의 차이가 있을 수 있습니다. 고속철도(KTX)를 이용하면 약 2시간 30분에서 3시간 정도 소요됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 변수는 여러개가 올 수 있다.\n",
    "template = \"{place1}과 {place2} 간의 거리는 얼마인가요?\"\n",
    "prompt_template= PromptTemplate(template=template)\n",
    "prompt1 = prompt_template.format(place1=\"서울\", place2=\"부산\")\n",
    "prompt1\n",
    "print(\"질문:\", prompt1)\n",
    "print(\"답변:\", model.invoke(prompt1).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: text='서울과 런던 간의 거리는 얼마인가요?'\n",
      "답변: 서울과 런던 간의 거리는 대략 8,900킬로미터(약 5,500마일)입니다. 이 거리는 직선 거리(대권거리)로 측정된 것이며, 실제 비행 경로는 항로와 기상 상황에 따라 다를 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "prompt2 = prompt_template.invoke({\"place1\":\"서울\", \"place2\":\"런던\"})\n",
    "print(\"질문:\", prompt2)\n",
    "print(\"답변:\", model.invoke(prompt2).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  ChatPromptTemplate 생성\n",
    "# Chatting 용 -> 한개의 메세지가 \"role\"과 \"content\"로 구성됨.\n",
    "# role : \"ai\", \"assiatant\" -> 인공지능 모델, \"user\", \"human\": 사용자,\n",
    "#        \"system\": AI 에게 보내는 지시어로 대화 전반에 영향을 주는 메세지\n",
    "# 한개 메세지:  tuple(str, str)  (\"role\", \"content\")\n",
    "# 한번에 여러개ㅑ의 메세지를 전달 하루 수 있다. 메세지들을 리스트로 묶어서 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = [\n",
    "    (\"system\", \"당신은 {domain} 전문가 입니다. 답변은 {char_length} 글자 이하로 해주세요.\"),  # persona(역할) 부여.\n",
    "    (\"human\", \"{question}\")\n",
    "]\n",
    "\n",
    "# 생성 - 생성자 이용, from_message() 메소드 이용\n",
    "# prompt_template = ChatPromptTemplate(   \n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    messages=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수들: ['char_length', 'domain', 'question']\n",
      "messages: [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['char_length', 'domain'], input_types={}, partial_variables={}, template='당신은 {domain} 전문가 입니다. 답변은 {char_length} 글자 이하로 해주세요.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(\"변수들:\", prompt_template.input_variables)\n",
    "print(\"messages:\", prompt_template.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 당신은 와인 전문가 입니다. 답변은 200 글자 이하로 해주세요.\n",
      "Human: 해산물 요리에 어울리는 와인을 추천해주세요.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate - format() :  string 반환\n",
    "#                                  format_messaeges() : 메세지별로 Message객체 넣어서 반환.\n",
    "#                                  invoke()\n",
    "\n",
    "prompt1 = prompt_template.format(domain=\"와인\", char_length=\"200\", question=\"해산물 요리에 어울리는 와인을 추천해주세요.\")\n",
    "print(prompt1) # \"role: 메세지/n role:메세지\"\n",
    "print(type(prompt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 와인 전문가 입니다. 답변은 100 글자 이하로 해주세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='고기 요리에 어울리는 와인은?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "prompt2 = prompt_template.format_messages(domain=\"와인\", char_length=\"100\", question=\"고기 요리에 어울리는 와인은?\")\n",
    "print(prompt2)\n",
    "print(type(prompt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='당신은 와인 전문가 입니다. 답변은 500 글자 이하로 해주세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='가장 맛있는 와인 이름을 알려주세요.', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "prompt3 = prompt_template.invoke({\n",
    "    \"domain\":\"와인\", \"char_length\":\"500\", \"question\":\"가장 맛있는 와인 이름을 알려주세요.\"\n",
    "})\n",
    "print(prompt3)\n",
    "print(type(prompt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(prompt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "\"가장 맛있는 와인\"은 개인의 취향에 따라 다르기 때문에 특정 와인을 지목하기는 어렵습니다. 그러나 몇 가지 유명한 와인을 추천할 수 있습니다.\n",
      "\n",
      "1. **샤토 마고 (Château Margaux)**: 프랑스 보르도 지역의 대표적인 와인으로, 우아함과 복합성을 지닌 레드 와인입니다.\n",
      "\n",
      "2. **로마네 콩티 (Romanée-Conti)**: 부르고뉴 지역의 매우 희귀하고 비싼 피노 누아 와인으로, 풍부한 향과 깊은 맛이 특징입니다.\n",
      "\n",
      "3. **오프리드 (Opus One)**: 미국 나파밸리에서 생산되는 프랑스-미국 합작 와인으로, 풀바디의 복합적인 맛이 매력적입니다.\n",
      "\n",
      "4. **수르 뒤 (Sauternes)**: 프랑스 보르도의 디저트 와인으로, 달콤하고 풍부한 맛이 특징입니다.\n",
      "\n",
      "5. **바롤로 (Barolo)**: 이탈리아의 네비올로 품종으로 만든 풀바디 레드 와인으로, 복합적인 향과 긴 여운을 자랑합니다.\n",
      "\n",
      "각 와인은 특정 음식과 잘 어울리거나, 특별한 경우에 더 맛있게 느껴질 수 있습니다. 자신만의 ‘맛있는 와인’을 찾기 위해 다양한 와인을 시도해보는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "print(type(res))\n",
    "res\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8035aa66-8501-4007-a537-ddf89f5fd5bc-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n",
    "- 메시지 리스트를 전달하기 위한 특별한 프롬프트 템플릿으로, 채팅 히스토리나 예제 메시지를 포함할 때 사용된다.\n",
    "  - 변수가 프롬프트 문장안에 일부내용을 입력받을 때 사용된다면 **MessagePlaceHolder** 는 여러개의 메세지 문장들을 입력받는 데 사용된다.\n",
    "- 보통 Chat Template의 채팅 히스토리나 예제 메시지를 포함할 때 사용된다.\n",
    "- 객체 생성시 파라미터\n",
    "  - variable_name: str: 변수명 지정\n",
    "  - optional: bool = False\n",
    "    - True일 경우 생략 할 수있다. \n",
    "  - n_messages: int \n",
    "    - 최근 N개의 메세지만 포함시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "template = [\n",
    "    (\"system\", \"당신은 수학 전문가입니다.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\", origianl=True), \n",
    "    (\"human\", \"{question}\")\n",
    "]\n",
    "prompt_template = ChatPromptTemplate(template)\n",
    "# ....채팅 history 가 있는경우\n",
    "chatting_history = [\n",
    "    (\"human\", \"1+1의 결과는?\"),\n",
    "    (\"ai\", \"2입니다.\"),\n",
    "    (\"human\", \"5 * 7 의 결과는?\"),\n",
    "    (\"ai\", \"21입니다.\")\n",
    "]\n",
    "\n",
    "\n",
    "prompt = prompt_template.invoke({\n",
    "    \"question\":\"위의 결과에 4제곱한 결과는?\",\n",
    "    \"history\": chatting_history             # MessagesPlaceHolder 넣어줄 대화들\n",
    "})\n",
    "\n",
    "# 변수 -> 문자열, MessagesPlaceHolder -> 대화목록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 수학 전문가입니다.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='1+1의 결과는?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='2입니다.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='5 * 7 의 결과는?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='21입니다.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='위의 결과에 4제곱한 결과는?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "pprint(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21의 4제곱은 \\( 21^4 = 194481 \\)입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Template \n",
    "Few-Shot Prompt를 작성하기 위한 Prompt Template\n",
    "- **FewShotPromptTemplate**: 예시를 넣은 지시형 template을 정의한다.\n",
    "- **FewShotChatMessagePromptTemplate**: 예시를 넣은 대화형(chat) prompt template을 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FewShotPromptTemplate\n",
    "- 지시형 template message 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:Good morning?\n",
      "Korean:좋은 아침입니다.\n",
      "\n",
      "English:Good afternoon?\n",
      "Korean:좋은 오후되세요.\n",
      "\n",
      "English:Thank you?\n",
      "Korean:고맙습니다.\n",
      "\n",
      "English:Good Evening?\n",
      "Korean:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# #예제\n",
    "# English:Good morning?\\nKorean:좋은 아침입니다.\n",
    "# English:Good afternoon?\\nKorean:즐거운 오후 되세요.\n",
    "# English:Thank you?\\nkorean:고맙습니다.\n",
    "\n",
    "# # 질문\n",
    "# English:Good evening?\\nKorean:  -> 질문\n",
    "\n",
    "# 예제(Example)을 만드는 프롬프트 템플랏.\n",
    "example_template = PromptTemplate(\n",
    "    template=\"English:{english}\\nKorean:{korean}\"\n",
    ")\n",
    "# 예제 템플릿에 넣을 값들을 만들기.\n",
    "example = [\n",
    "    {\"english\":\"Good morning?\", \"korean\":\"좋은 아침입니다.\"},  # dict 1개 -> 한개의 예제\n",
    "    {\"english\":\"Good afternoon?\", \"korean\":\"좋은 오후되세요.\"},\n",
    "    {\"english\":\"Thank you?\", \"korean\":\"고맙습니다.\"},\n",
    "]\n",
    "\n",
    "# FewShotPromptTemplate 생성 -> 최종 template\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=example,                    # examples_template 에 넣어줄 값들\n",
    "    example_prompt=example_template,     # 예제 만들 프롬프트\n",
    "    suffix=\"English:{input}\\nKorean:\",   # Example 들 뒤에 붙인 prompt (실제 요청할  query)\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(input=\"Good Evening?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'좋은 저녁입니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FewShotChatMessagePromptTemplate\n",
    "대화형 template message 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"english\":\"Good morning?\", \"korean\":\"좋은 아침입니다.\"},  # dict 1개 -> 한개의 예제\n",
    "    {\"english\":\"Good afternoon?\", \"korean\":\"좋은 오후되세요.\"},\n",
    "    {\"english\":\"Thank you?\", \"korean\":\"고맙습니다.\"},\n",
    "]\n",
    "\n",
    "# example을 만들 prompt template 생성 -> 대화 메세지 형식 구성\n",
    "example_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"{english}\"), \n",
    "        (\"ai\", \"{korean}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# example을 생성.\n",
    "fewshot_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,  # 예제 메세지에 넣을 값들\n",
    "    example_prompt=example_template # 예제를 만들 chat prompt template\n",
    "    # input_variables=[\"english\", \"korean\"] #v0.2 에서는 wlwjd\n",
    ")\n",
    "\n",
    "# 예제에 질문을 포함한 최종 ChatPromptTemplate 생성.\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"당신은 영어를 한국어로 번역하는 번역가입니다. 최대한 자연스럽고 간결하게 번역해주세요.\"),\n",
    "        fewshot_template,   # 예제\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"input\":\"Good evening?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 영어를 한국어로 번역하는 번역가입니다. 최대한 자연스럽고 간결하게 번역해주세요.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Good morning?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='좋은 아침입니다.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Good afternoon?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='좋은 오후되세요.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Thank you?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='고맙습니다.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Good evening?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "pprint(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좋은 저녁입니다.\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='좋은 저녁입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 92, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-d6b48916-f4ba-4ca3-9959-09b3f2ab7036-0', usage_metadata={'input_tokens': 92, 'output_tokens': 6, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(res.content)\n",
    "print(\"==================================\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt 조합(Prompt Composition)\n",
    "- 여러개의 프롬프트를 합치거나 프롬프트 내에 다른 프롬프트를 포함시켜 새로운 프롬프트를 만들 수 있다.\n",
    "\n",
    "#### 프롬프트 합치기\n",
    "- `+` 연산자로 프롬프트를 합친다.\n",
    "    - `PromptTemplate + PromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = PromptTemplate(template=\"당신은 {country}의 여행가이드입니다.\")\n",
    "prompt_template2 = PromptTemplate(template=\"{country}의 {interest}을(를) 소개해주세요.\")\n",
    "prompt_template = prompt_template1 + prompt_template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'당신은 {country}의 여행가이드입니다.{country}의 {interest}을(를) 소개해주세요.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country', 'interest']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'당신은 한국의 여행가이드입니다.한국의 포토스팟을(를) 소개해주세요.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(country=\"한국\", interest=\"포토스팟\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PipelinePromptTemplate\n",
    "- 프롬프트 안에 다른 프롬프트를 넣어서 만든다.\n",
    "- `+` 와는 달리 단순히 합치는 것이 아니라 프롬프트 안에 다른 프롬프트들을 추가하는 개념.\n",
    "- 프롬프르트를 재사용해서 새로운 프롬프트를 만들때 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "\n",
    "full_template = PromptTemplate(\n",
    "    template=\"{role}\\n{question}\\n10개 이하의 단어로 답해주세요\"\n",
    ")\n",
    "role_template = PromptTemplate(template=\"당신은 {country}의 여행가이드 입니다.\")\n",
    "question_template = PromptTemplate(template=\"{country}의 {interest}에 대해 알려주세요.\")\n",
    "input_templates = [\n",
    "    (\"role\", role_template),         # (full_template의 변수, 포함시킬 템플릿)\n",
    "    (\"question\", question_template)\n",
    "]\n",
    "\n",
    "prompt_template = PipelinePromptTemplate(\n",
    "    final_prompt=full_template,  # 모든 것들을 포함할 템플릿.\n",
    "    pipeline_prompts=input_templates  # final_prompt에 포함시킬 템플릿들 : list(type(str, template))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interest', 'country']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 미국의 여행가이드 입니다.\n",
      "미국의 유명한 여행지에 대해 알려주세요.\n",
      "10개 이하의 단어로 답해주세요\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(country=\"미국\", interest=\"유명한 여행지\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='당신은 영국의 여행가이드 입니다.\\n영국의 유명한 관광지에 대해 알려주세요.\\n10개 이하의 단어로 답해주세요')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"country\":\"영국\", \"interest\":\"유명한 관광지\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그랜드 캐니언, 뉴욕, 요세미티, 라스베가스, 하와이, 디즈니랜드.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain-Hub 에서 Prompt 받아오기\n",
    "- https://smith.langchain.com/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\langchain\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set the LANGCHAIN_API_KEY environment varialbe \n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"smithing-gold/assumption-checker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your job is to identify key assumptions in a query and then form fact-checkable \\nquestions which challenge these assumptions. \\nYour questions will be used to search our DB with semantic search capabilities (optimize accordingly). \\n\\nThe user will not see your searches - so do not address them. Keep assumptions concise. \\n\\nGenerate questions that question the foundational assumptions behind the user query. Fact Checks should explore the basic existence or availability of the services or features mentioned in the question, using varied wording and sentence structures to maximize search scope. '), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='How can I unlock the unused 90% of my brain to become smarter?\"\\n'), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Assumption: The assumption is that we only use 10% of our brain and that unlocking the rest will make us smarter.\\nAssumption: Intelligence is solely a function of brain utilization.\\n\\nFact Check: Is it true that humans only use 10% of their brains?\\nFact Check: What is the scientific consensus on the percentage of brain utilization and its relation to intelligence?'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your job is to identify key assumptions in a query and then form fact-checkable \\nquestions which challenge these assumptions. \\nYour questions will be used to search our DB with semantic search capabilities (optimize accordingly). \\n\\nThe user will not see your searches - so do not address them. Keep assumptions concise. \\n\\nGenerate questions that question the foundational assumptions behind the user query. Fact Checks should explore the basic existence or availability of the services or features mentioned in the question, using varied wording and sentence structures to maximize search scope. ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How can I unlock the unused 90% of my brain to become smarter?\"\\n', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Assumption: The assumption is that we only use 10% of our brain and that unlocking the rest will make us smarter.\\nAssumption: Intelligence is solely a function of brain utilization.\\n\\nFact Check: Is it true that humans only use 10% of their brains?\\nFact Check: What is the scientific consensus on the percentage of brain utilization and its relation to intelligence?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ㄹㄹㄹㄹㄹㄹㄹㄹ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\":\"ㄹㄹㄹㄹㄹㄹㄹㄹ\"}).messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
