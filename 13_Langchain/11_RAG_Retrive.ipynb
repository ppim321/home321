{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74df2380-7ec3-41d5-b030-5df2d83c10ce",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "- 비정형 질의(query)를 입력 받아 Vector store에서 관련된 내용을 검색하는 기능을 제공하는 인터페이스로 다양한 데이터 소스에서 정보를 검색하여 대규모 언어 모델(LLM) 기반 애플리케이션의 **정확성을** 향상시키는 데 핵심적인 역할을 한다.\n",
    "\n",
    "![RAG](figures/rag2.png)\n",
    "\n",
    "## 주요 특징\n",
    "- **다양한 데이터 소스 지원**\n",
    "\t- Retriever는 벡터 스토어, 그래프 데이터베이스, 관계형 데이터베이스 등 여러 종류의 검색 시스템과 상호작용할 수 있는 통일된 인터페이스를 제공.\n",
    "- **간단한 인터페이스**: Retriever는 문자열 형태의 쿼리를 입력받아 관련 문서의 리스트를 반환한다. 이러한 단순한 구조 덕분에 다양한 검색 시스템과 쉽게 통합할 수 있다. \n",
    "\n",
    "## 다양한 Retriever 유형\n",
    "   - **벡터 스토어(Vector Store) Retriever**\n",
    "\t\t- 텍스트 조각마다 임베딩을 생성하여 유사도 검색을 수행합니다. 빠르고 간단한 검색 시스템을 구축할 때 적합하다.\n",
    "   - **ParentDocument Retriever**\n",
    "\t\t- 문서를 여러 청크로 나누어 인덱싱한 후, 가장 유사한 청크를 찾아 전체 원본 문서를 반환한다. 작은 정보 조각들이 모여 하나의 문서를 구성할 때 유용하다.\n",
    "   - **멀티 벡터(Multi Vector) Retriever**\n",
    "\t\t- 각 문서에 대해 요약이나 가상의 질문 생성과 수동 추가 방식을 통해 문서당 여러 벡터를 생성하여 인덱싱한다. 텍스트 자체보다 더 관련성 있는 정보를 추출할 수 있을 때 사용한다.\n",
    "   - **Self Query Retriever**\n",
    "\t\t- LLM을 활용하여 사용자 입력을 텍스트 검색어와 메타데이터 필터로 변환한다. 문서의 메타데이터에 대한 질문을 처리할 때 효과적.\n",
    "   - **Contextual Compression Retriever**\n",
    "\t\t- 검색된 문서에서 불필요한 정보를 제거하고, 쿼리와 관련된 핵심 내용만을 추출한다. \n",
    "\n",
    "## **고급 검색 패턴 지원**:\n",
    "   - **앙상블(Ensemble) Retriever**: 여러 Retriever의 검색 결과를 결합하여 더 정확한 결과를 제공한다.\n",
    "   - **소스 문서 보존(Source Document Retention)**: 인덱싱 과정에서 변환된 문서와 원본 문서 간의 연결을 유지하여, 검색 시 원본 문서를 반환할 수 있게 한다. \n",
    "\n",
    "> **인덱싱**\n",
    "> - 인덱싱은 벡터화된 문서들을 효율적으로 저장하고 관리하여, 유사성 검색 시 빠르게 원하는 정보를 찾을 수 있도록 하는 과정을 말한다. 이를 통해 벡터화된 데이터의 신속하게 찾을 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5b8a9-c216-4115-acfa-b5f187698883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455f582-74bf-47b8-9276-58f978936629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db555d-ca62-445d-999a-d46713cb5f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263fa34-618f-482a-adb3-110b419cb732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce6b3aa-ecf9-4874-b71d-35d74c9e6574",
   "metadata": {},
   "source": [
    "# TODO 다음을 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b567e3a0-df09-4ebc-91ad-34d1a56ebca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:14:52.978789Z",
     "iopub.status.busy": "2024-12-02T12:14:52.977794Z",
     "iopub.status.idle": "2024-12-02T12:14:56.143202Z",
     "shell.execute_reply": "2024-12-02T12:14:56.142870Z",
     "shell.execute_reply.started": "2024-12-02T12:14:52.978789Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "\n",
    "collection_name = \"olmpic_docs\"\n",
    "persist_directory = \"vector_store/chroma/olympic\"\n",
    "\n",
    "# Text Loading\n",
    "\n",
    "\n",
    "# Split\n",
    "\n",
    "\n",
    "# Vector Store 생성\n",
    "\n",
    "\n",
    "# Retriever 생성 - \"mmr\" 방식\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace68f09-666b-4e17-83e7-d5ecb64ef5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template 생성\n",
    "messages = [\n",
    "        (\"ai\", \"\"\"\n",
    "        You are a helpful assistanct. Answer question using only the following context. \n",
    "        If you don't know the answer, just say you don't know. Don't make it up. \n",
    "        Answer in Korean.\n",
    "        \n",
    "        {context}\")\"\"\",\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Chain 구성\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82baeb6-6a5f-4026-aa98-1873a65dfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain을 이용해 질의\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af6f92-e2b3-4e11-a671-76c7fbf936d6",
   "metadata": {},
   "source": [
    "## Map Reduce 방식\n",
    "\n",
    "- RAG에서 질문과 유사한 document들을 찾아  그대로 질문과 함께 전달하는 방식을 **stuff** 방식이라고 한다.\n",
    "- Map reduce 방식은 document들에서 질문을 답하는데 적합한 document를 llm을 이용해 찾은 뒤 전달하는 방식이다.\n",
    "  1. vectorstore에서 질문과 유사한 document들을 유사도 검색으로 찾음\n",
    "     - 이것들은 단순 유사도라서 질문과 직접 관계 없는 것도 있다.\n",
    "  2. document들이 질문을 답하는데 적합한지 llm에게 질문해서 관련있는 것들만 문자열로 묶어준다.\n",
    "  3. 질문과 `2`에서 찾은 내용을 context로해서 llm에 질문하여 최종 답을 받는다.\n",
    "- context를 좀더 질문과 관련된 내용으로 전달 할 수있는 장점이 있는 반면 비용이 드는 폐쇄형 llm을 사용하는 경우 비용이 추가로 드는 단점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7a682-893f-4304-a2c3-9848f327a7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d6867-f0bc-42c4-9035-51e8d69a5657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e6199-b17e-486d-a316-eb0e3d6bbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",  \"\"\"\n",
    "Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
    "Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "-------\n",
    "{context}\n",
    "\"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e34c98-1b69-415a-ad88-62b54e2d86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8ef3d-696a-4a35-a666-a2d98a7b5a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858ec88-2ea1-4028-bcf4-5105efd970b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b551faf-f1ce-44dc-bbfe-c0ad84fbcdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177863b-e4d9-44c3-9af2-0acbb4482548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
