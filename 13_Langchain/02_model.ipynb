{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain은 다양한 LLM 모델을 지원한다.\n",
    "\n",
    "- 대규모 언어 모델(LLM) 개발 회사들은 사용자들이 자신의 애플리케이션에서 LLM 모델을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "- 각 LLM은 API를 호출 library를 제공하고 있다. 그런데 개발자 입장에서는 같은 작업을 하는데 LLM에 따라 다른 코드를 사용해야하는 어려움이 있다.\n",
    "- Langchain은  다양한 LLM의 API 호출할 수 있도록 지원한다.\n",
    "    - 여러 LLM 사용을 같은 interface를 사용해 호출 할 수있게 하여 특정 모델에 종속되지 않으며,  필요에 따라 쉽게 교체할 수 있다. \n",
    "    - Langchain 지원 LLM 모델: https://python.langchain.com/docs/integrations/chat/#featured-providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain_core langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T02:20:44.354209Z",
     "iopub.status.busy": "2024-11-23T02:20:44.354209Z",
     "iopub.status.idle": "2024-11-23T02:20:52.373409Z",
     "shell.execute_reply": "2024-11-23T02:20:52.373409Z",
     "shell.execute_reply.started": "2024-11-23T02:20:44.354209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain langchain_core langchain-community  -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install python-dotenv -qU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install ipywidgets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com/\n",
    "\n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 1. 로그인 -> 2. Dashboard -> 3. API Keys -> 4. Create New Secreat Key\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 환경변수를 로드한다.\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://openai.com/api/pricing/\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다.\n",
    "-  2024/11 현재 **gpt-4o-mini** 가 성능 대비 가장 저렴하다. (100만 토큰당 $0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install openai -qU\n",
    "pip install langchain-openai -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai   # langchain-openai 설치시 같이 설치됨.\n",
    "from dotenv import load_dotenv  # API 키들을 환경 변수로 등록.\n",
    "\n",
    "load_dotenv()  # working directory 의  '.env'에 작성된 환경변수들을 읽어서 os 환경변수로 등록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']  # 환경 변수 조회. 별도로 작성하지 않아도 환경변수로 지정하면 찾아서 읽어옴.\n",
    "os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Lib를 이용해서 GPT-4o-mini 모델에 질의\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # 환경변수에 API 키가 등록만 되어있으면 직접 넣어서 생성함.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # 사용할 모델 종류 선택\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"OpenAI의 LLM 모델에 대해 설명해줘.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  {role\":\"채팅 주제\", \"content\": LLM 에 전달할 내용을 text로 작성.\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AZvSGW2XhC6xahbGoAwbSzDnv93zv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대규모 언어 모델)은 자연어 처리(NLP)와 관련된 다양한 작업을 수행할 수 있는 AI 모델입니다. 이러한 모델은 주로 텍스트 데이터를 기반으로 훈련되어 있으며, 인간과 유사한 방식으로 언어를 이해하고 생성하는 능력을 가지고 있습니다. OpenAI의 모델에는 GPT(Generative Pre-trained Transformer) 시리즈가 포함되어 있습니다. 여기에는 여러 버전이 있으며, 각 버전은 더 많은 데이터와 더 나은 알고리즘을 바탕으로 성능이 향상되었습니다.\\n\\n### 주요 특징\\n\\n1. **사전 훈련 및 파인튜닝**:\\n   - LLM은 방대한 양의 텍스트 데이터에서 사전 훈련됩니다. 이 과정에서 모델은 언어의 구조와 패턴을 학습하게 됩니다.\\n   - 이후, 특정 작업에 맞추어 모델의 성능을 개선하기 위해 파인튜닝이 이루어질 수 있습니다.\\n\\n2. **Transformer 아키텍처**:\\n   - OpenAI의 LLM은 Transformer 아키텍처를 기반으로 합니다. 이 구조는 셀프 어텐션 메커니즘을 사용하여 텍스트의 의미를 학습하고 문맥을 이해하는 데 매우 효과적입니다.\\n\\n3. **다양한 활용 사례**:\\n   - LLM은 텍스트 생성, 번역, 요약, 질문 응답, 대화형 AI 등 다양한 NLP 작업을 수행할 수 있습니다. 사용자는 자연어로 질문을 하고 모델은 그에 대한 답변을 생성할 수 있습니다.\\n\\n4. **사용자 인터페이스**:\\n   - OpenAI는 API 형태로 LLM을 제공하며, 이를 통해 개발자들이 다양한 어플리케이션에 모델을 통합할 수 있도록 지원합니다.\\n\\n5. **정확성 및 안전성 향상**:\\n   - OpenAI는 LLM의 결과가 정확하고 안전하도록 지속적으로 연구하고 개선하고 있으며, 사용자의 피드백을 반영하여 모델의 성능을 향상시키고 있습니다.\\n\\n### 윤리적 고려사항\\nLLM의 사용은 다양한 윤리적 문제를 동반할 수 있습니다. 예를 들어, 가짜 뉴스 생성, 편향된 출력, 개인 정보 침해 등의 우려가 존재합니다. 이에 따라 OpenAI는 사용 가이드라인을 철저히 설정하고 있으며, 책임 있는 사용을 강조하고 있습니다.\\n\\n이러한 특성과 기능 덕분에 OpenAI의 LLM은 연구자와 개발자, 일반 사용자 등 다양한 사용자들에게 유용한 도구로 자리잡고 있습니다.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733125700, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=551, prompt_tokens=19, total_tokens=570, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대규모 언어 모델)은 자연어 처리(NLP)와 관련된 다양한 작업을 수행할 수 있는 AI 모델입니다. 이러한 모델은 주로 텍스트 데이터를 기반으로 훈련되어 있으며, 인간과 유사한 방식으로 언어를 이해하고 생성하는 능력을 가지고 있습니다. OpenAI의 모델에는 GPT(Generative Pre-trained Transformer) 시리즈가 포함되어 있습니다. 여기에는 여러 버전이 있으며, 각 버전은 더 많은 데이터와 더 나은 알고리즘을 바탕으로 성능이 향상되었습니다.\n",
      "\n",
      "### 주요 특징\n",
      "\n",
      "1. **사전 훈련 및 파인튜닝**:\n",
      "   - LLM은 방대한 양의 텍스트 데이터에서 사전 훈련됩니다. 이 과정에서 모델은 언어의 구조와 패턴을 학습하게 됩니다.\n",
      "   - 이후, 특정 작업에 맞추어 모델의 성능을 개선하기 위해 파인튜닝이 이루어질 수 있습니다.\n",
      "\n",
      "2. **Transformer 아키텍처**:\n",
      "   - OpenAI의 LLM은 Transformer 아키텍처를 기반으로 합니다. 이 구조는 셀프 어텐션 메커니즘을 사용하여 텍스트의 의미를 학습하고 문맥을 이해하는 데 매우 효과적입니다.\n",
      "\n",
      "3. **다양한 활용 사례**:\n",
      "   - LLM은 텍스트 생성, 번역, 요약, 질문 응답, 대화형 AI 등 다양한 NLP 작업을 수행할 수 있습니다. 사용자는 자연어로 질문을 하고 모델은 그에 대한 답변을 생성할 수 있습니다.\n",
      "\n",
      "4. **사용자 인터페이스**:\n",
      "   - OpenAI는 API 형태로 LLM을 제공하며, 이를 통해 개발자들이 다양한 어플리케이션에 모델을 통합할 수 있도록 지원합니다.\n",
      "\n",
      "5. **정확성 및 안전성 향상**:\n",
      "   - OpenAI는 LLM의 결과가 정확하고 안전하도록 지속적으로 연구하고 개선하고 있으며, 사용자의 피드백을 반영하여 모델의 성능을 향상시키고 있습니다.\n",
      "\n",
      "### 윤리적 고려사항\n",
      "LLM의 사용은 다양한 윤리적 문제를 동반할 수 있습니다. 예를 들어, 가짜 뉴스 생성, 편향된 출력, 개인 정보 침해 등의 우려가 존재합니다. 이에 따라 OpenAI는 사용 가이드라인을 철저히 설정하고 있으며, 책임 있는 사용을 강조하고 있습니다.\n",
      "\n",
      "이러한 특성과 기능 덕분에 OpenAI의 LLM은 연구자와 개발자, 일반 사용자 등 다양한 사용자들에게 유용한 도구로 자리잡고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model_name = \"gpt-4o-mini\"\n",
    "model = ChatOpenAI(\n",
    "    model=model_name\n",
    ")\n",
    "res = model.invoke(\"안녕하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-0424f7e5-5a39-4daf-ad86-eafc8a593ef7-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떻게 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "\n",
    "[답변형식]\n",
    "- 한국어 이름(영어 이름) : 행성에 대한 간단히 설명\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury) : 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차가 매우 큽니다.\n",
      "  \n",
      "- 금성(Venus) : 지구와 유사한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 온실 효과가 강하게 나타나 매우 높은 온도를 유지합니다.\n",
      "  \n",
      "- 지구(Earth) : 생명체가 존재하는 유일한 행성으로, 액체 상태의 물이 풍부하게 존재합니다.\n",
      "  \n",
      "- 화성(Mars) : 붉은색을 띠는 행성으로, 과거에 물이 흐른 흔적이 발견되었으며, 탐사와 정착 가능성이 연구되고 있습니다.\n",
      "  \n",
      "- 목성(Jupiter) : 태양계에서 가장 큰 행성으로, 가스 행성이며 강력한 자기장을 가지고 있습니다. 다수의 위성을 거느리고 있습니다.\n",
      "  \n",
      "- Saturn(토성) : 아름다운 고리로 유명한 행성으로, 가스 행성이며, 많은 위성을 가지고 있습니다.\n",
      "  \n",
      "- 천왕성(Uranus) : 특징적으로 옆으로 기울어진 축을 가지고 있는 행성으로, 차가운 대기와 암석으로 이루어진 핵을 가집니다.\n",
      "  \n",
      "- 해왕성(Neptune) : 태양계에서 가장 먼 행성으로, 강한 바람과 짙은 파란색 대기를 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"백합을 주제로 힙합 음악의 가사를 작성해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)  \n",
      "백합처럼 순수한 나의 마음,  \n",
      "어두운 밤에도 빛을 잃지 않아,  \n",
      "너와 나의 사랑, 그 향기가 퍼져,  \n",
      "이 도시의 거리를 다 감싸 안아.  \n",
      "\n",
      "(Chorus)  \n",
      "백합, 백합, 우린 꽃처럼 피어나,  \n",
      "세상의 모든 아픔, 함께 이겨내자.  \n",
      "백합, 백합, 우리의 꿈을 향해,  \n",
      "끝없는 밤하늘, 함께 날아가자.  \n",
      "\n",
      "(Verse 2)  \n",
      "비바람 속에서도 난 꺾이지 않아,  \n",
      "강한 뿌리로 너를 지켜줄게,  \n",
      "희망의 꽃잎, 매일 새롭게 피어나,  \n",
      "우린 서로의 빛, 절대 사라지지 않아.  \n",
      "\n",
      "(Chorus)  \n",
      "백합, 백합, 우린 꽃처럼 피어나,  \n",
      "세상의 모든 아픔, 함께 이겨내자.  \n",
      "백합, 백합, 우리의 꿈을 향해,  \n",
      "끝없는 밤하늘, 함께 날아가자.  \n",
      "\n",
      "(Bridge)  \n",
      "어둠 속에서 빛을 찾아,  \n",
      "우린 함께라면 두렵지 않아,  \n",
      "이 순간을 영원히 간직해,  \n",
      "백합의 향기처럼 퍼져나가.  \n",
      "\n",
      "(Chorus)  \n",
      "백합, 백합, 우린 꽃처럼 피어나,  \n",
      "세상의 모든 아픔, 함께 이겨내자.  \n",
      "백합, 백합, 우리의 꿈을 향해,  \n",
      "끝없는 밤하늘, 함께 날아가자.  \n",
      "\n",
      "(Outro)  \n",
      "백합의 이야기가 계속 이어져,  \n",
      "우릴 감싸는 이 사랑은 영원해.  \n",
      "백합, 백합, 우리의 노래를 들어,  \n",
      "함께하는 순간, 그게 진정한 자유.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.  \n",
    "\n",
    "    - Default 로 gpt-3.5-turbo 사용  \n",
    "\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message  \n",
    "\n",
    "- **OpenAI**\n",
    "    - 문장 완성 모델. (text completion) model  \n",
    "\n",
    "    - Default로 gpt-3.5-turbo-instruct 사용  \n",
    "\n",
    "      - instruct 모델만 사용가능  \n",
    "\n",
    "    - llm전달 입력과 llm 응답 출력 타입: str  \n",
    "\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다.  \n",
    "\n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7  \n",
    "\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.  \n",
    "\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정 (생략시 default모델 사용됨)  \n",
    "\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.  \n",
    "\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.  \n",
    "\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다.   \n",
    "\n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.  \n",
    "\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다.   \n",
    "\n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=0,  # 무작위성으로 0 지정.\n",
    "    max_tokens=100, # 응답 토큰수에 제한을 줌.\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "\n",
    "[답변형식]\n",
    "- 한국어 이름(영어 이름) : 행성에 대한 간단히 설명\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury) : 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "\n",
      "- 금성(Venus) : 두 번째 행성으로, 두꺼운 대기와 강한 온실 효과로 인해 매우 높은 온도를 유지하고 있습니다.\n",
      "\n",
      "- 지구(Earth) : 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"백합을 주제로 힙합 음악의 가사를 작성해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)  \n",
      "이 도시 밤하늘, 별이 빛나  \n",
      "내 마음속에 피어난 백합, 그 향기 같아  \n",
      "흰 꽃잎처럼 순수해, 눈부셔  \n",
      "내 꿈과 현실, 계속 춤을 춰\n",
      "\n",
      "(Chorus)  \n",
      "백합처럼 피어나는 나의 비트  \n",
      "거센 바람에도 흔들리지 않아, keep it sweet  \n",
      "너와 나, 이 순간을 함께 느껴  \n",
      "백합이 피어나는 이 밤에, let it flow\n",
      "\n",
      "(Verse 2)  \n",
      "내 안의 열정, 불꽃처럼 타올라  \n",
      "어둠 속에서도 희망을 찾아, 계속 달려가  \n",
      "고독한 밤, 하지만 두렵지 않아  \n",
      "백합의 힘, 나를 지켜줄 거야\n",
      "\n",
      "(Chorus)  \n",
      "백합처럼 피어나는 나의 비트  \n",
      "거센 바람에도 흔들리지 않아, keep it sweet  \n",
      "너와 나, 이 순간을 함께 느껴  \n",
      "백합이 피어나는 이 밤에, let it flow\n",
      "\n",
      "(Bridge)  \n",
      "단단한 줄기, 나를 지탱해줘  \n",
      "힘든 순간에도, 넌 항상 내 곁에 있어  \n",
      "세상에 휘둘리지 않을 거야, 고개 들고  \n",
      "백합처럼 자생하는 나의 길을 걸어가\n",
      "\n",
      "(Outro)  \n",
      "이제는 나의 꽃길, 더 이상 두렵지 않아  \n",
      "백합의 향기 잊지 말고, 모두 함께해  \n",
      "이 도시에서 피어난 우리의 꿈  \n",
      "백합처럼 영원히, 함께 피어나길 바라.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.invoke(\"백합을 주제로 힙합 음악의 가사를 작성해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)  \n",
      "여기 백합, 순수한 향기와 색,  \n",
      "하얀 꽃잎처럼, 세상의 빛을 밝혀,  \n",
      "어두운 길에서도 넌 내 가이드,  \n",
      "이 심장에서 피어나는 꽃이야, 절대 제껴.\n",
      "\n",
      "(Chorus)  \n",
      "백합처럼 우아하게, 그래도 강하게,  \n",
      "어둠을 뚫고 나아가, 절대로 무너지지 않아,  \n",
      "우리는 꽃, 이 도시의 정원,  \n",
      "힙합 비트 위에 피어난 꿈, 우린 계속 달려.\n",
      "\n",
      "(Verse 2)  \n",
      "아침 이슬에 물든 꿈의 시작,  \n",
      "내일은 더 빛날 테니, 걱정은 잊어,  \n",
      "하얀 백합, 너의 이름을 부르며,  \n",
      "내 발걸음에 네가 스며들어, 매일매일 계속해.\n",
      "\n",
      "(Bridge)  \n",
      "이 눈부신 순간, 함께할 수 있어,  \n",
      "백합의 힘으로 이겨낼 수 있어,  \n",
      "비록 시련이 와도, 잊지 마,  \n",
      "너와 나의 연결, 영원히 이어져.\n",
      "\n",
      "(Chorus)  \n",
      "백합처럼 우아하게, 그래도 강하게,  \n",
      "어둠을 뚫고 나아가, 절대로 무너지지 않아,  \n",
      "우리는 꽃, 이 도시의 정원,  \n",
      "힙합 비트 위에 피어난 꿈, 우린 계속 달려.\n",
      "\n",
      "(Outro)  \n",
      "다시 한 번, 세상을 향해 핀 백합,  \n",
      "우리는 멈추지 않아, 이 길을 함께 가,  \n",
      "세상의 소음 속에, 나의 목소리를,  \n",
      "백합의 메아리로, 계속 울려 퍼져라.\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "- LLM은 상태 비저장형이지만 정확한 응답을 위해서는 이전 대화내용(context)를 저장해야 할 수있다.\n",
    "- Langchain의 메모리 모듈을 이용하면 이런 작업을 도와준다.\n",
    "- 컴퓨터에 메모리에 대화내용을 저장하여 대화 도중의 context를 기억하거나 database를 이용해 영구적로 저장할 수도 있다.\n",
    "\n",
    "![langchain_memory.png](figures/langchain_memory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury) : 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "\n",
      "- 금성(Venus) : 두 번째 행성으로, 두꺼운 대기와 강한 온실 효과로 인해 매우 높은 온도를 유지합니다.\n",
      "\n",
      "- 지구(Earth) : 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "\n",
      "- 화성(Mars) : 붉은 색을 띠는 행성으로, 과거에 물이 존재했을 가능성이 있으며, 현재 탐사 missions이 활발히 진행되고 있습니다.\n",
      "\n",
      "- 목성(Jupiter) : 태양계에서 가장 큰 행성으로, 거대한 가스 행성이며, 많은 위성을 가지고 있습니다.\n",
      "\n",
      "- 토성(Saturn) : 아름다운 고리로 유명한 행성으로, 목성과 마찬가지로 가스 행성입니다.\n",
      "\n",
      "- 천왕성(Uranus) : 독특하게도 옆으로 누워 있는 회전축을 가진 행성으로, 푸른색의 대기를 가지고 있습니다.\n",
      "\n",
      "- 해왕성(Neptune) : 태양계에서 가장 먼 행성으로, 강한 바람과 대기 패턴이 특징입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI의 API 사용 가격확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "with get_openai_callback() as usage:\n",
    "\n",
    "    # with 문 안에서 openai 모델에 요청을 하면 사용 비용이 usage에 저장됨.\n",
    "    res1 = model.invoke(\"소주 마시고 똑바로 걷는법을 알려주세요.\")\n",
    "    res2 = model.invoke(\"막걸리 마시고 취하지 않는 법을 알려주세요.\")\n",
    "    res3 = model.invoke(\"안 취하게 술 만드는 법을 알려주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 1200\n",
       "\tPrompt Tokens: 63\n",
       "\tCompletion Tokens: 1137\n",
       "Successful Requests: 3\n",
       "Total Cost (USD): $0.0006916499999999998"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006916499999999998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소주를 마신 후에 똑바로 걷는 것은 어려울 수 있지만, 몇 가지 팁을 통해 도움이 될 수 있습니다:\n",
      "\n",
      "1. **천천히 마시기**: 소주를 한꺼번에 많이 마시기보다는 천천히 즐기는 것이 중요합니다. 이렇게 하면 몸이 알코올에 적응할 시간을 가질 수 있습니다.\n",
      "\n",
      "2. **수분 섭취**: 소주와 함께 물이나 음료수를 함께 마시면 탈수를 예방할 수 있습니다. 수분을 충분히 섭취하면 정신을 맑게 유지하는 데 도움이 됩니다.\n",
      "\n",
      "3. **균형 잡기**: 걷기 전에 몸의 균형을 잡는 연습을 해보세요. 한쪽 발로 서거나, 발끝으로 서는 등의 간단한 운동이 도움이 될 수 있습니다.\n",
      "\n",
      "4. **고개 들기**: 걷는 동안 고개를 들어 앞을 바라보세요. 바닥을 쳐다보면 균형을 잃기 쉬우니, 시선을 올리고 걷는 것이 좋습니다.\n",
      "\n",
      "5. **천천히 걷기**: 빠르게 걷기보다는 느리게 걸음으로써 더 안정적으로 이동할 수 있습니다.\n",
      "\n",
      "6. **주변 환경 인식**: 주변을 잘 살피고 장애물을 피할 수 있도록 주의하세요. \n",
      "\n",
      "7. **혼자 걷지 않기**: 가능하다면 친구와 함께 걷거나, 누군가에게 도움을 요청하세요. \n",
      "\n",
      "8. **휴식하기**: 너무 피곤하거나 어지러움을 느낀다면 잠시 앉아서 휴식을 취하세요.\n",
      "\n",
      "이런 팁들을 기억하면 소주를 마신 후에도 좀 더 안정적으로 걷는 데 도움이 될 것입니다. 하지만 항상 책임감 있게 음주하고, 필요할 경우 대중교통이나 택시를 이용하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(res1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "막걸리를 마시고 취하지 않기 위해서는 몇 가지 방법을 고려할 수 있습니다. 다음은 도움이 될 수 있는 몇 가지 팁입니다:\n",
      "\n",
      "1. **천천히 마시기**: 막걸리를 천천히 음미하면서 마시면 알코올 흡수를 줄일 수 있습니다. 급하게 마시는 것보다 천천히 음주하는 것이 좋습니다.\n",
      "\n",
      "2. **음식과 함께 마시기**: 빈속에 술을 마시면 더 빨리 취할 수 있으므로, 막걸리와 함께 음식을 꼭 섭취하세요. 특히 단백질이나 지방이 포함된 음식을 먹는 것이 좋습니다.\n",
      "\n",
      "3. **수분 섭취**: 막걸리를 마시는 동안 물이나 무가당 음료수를 함께 마시면 알코올 농도를 낮출 수 있습니다. 수분을 충분히 섭취하세요.\n",
      "\n",
      "4. **알콜 도수 확인하기**: 막걸리의 알코올 도수는 다양하므로, 낮은 도수의 막걸리를 선택하면 취할 위험이 줄어듭니다.\n",
      "\n",
      "5. **신경 쓰지 않기**: 술을 마실 때 너무 취하는 것에 대해 걱정하지 않도록 하세요. 긴장을 풀면 자연스럽게 음주를 조절할 수 있습니다.\n",
      "\n",
      "6. **적당한 양**: 처음부터 과도하게 마시지 않도록 하고, 적당한 양을 정해두고 그 이상은 마시지 않도록 하세요.\n",
      "\n",
      "이 방법들을 통해 막걸리를 즐기면서도 취하지 않도록 할 수 있습니다. 하지만 개인의 체질에 따라 다를 수 있으니 항상 자신의 몸 상태를 잘 살피는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안 취하게 술을 만드는 방법은 주로 알코올 도수를 낮추는 것을 목표로 합니다. 아래는 저알콜 음료를 만드는 몇 가지 방법입니다.\n",
      "\n",
      "1. **저알콜 칵테일 만들기**:\n",
      "   - 기본적으로 알코올이 적거나 없는 재료를 사용하여 칵테일을 만듭니다.\n",
      "   - 예를 들어, 스파클링 워터, 주스, 과일, 허브 등을 조합하여 신선한 칵테일을 만들 수 있습니다.\n",
      "\n",
      "2. **음료 희석하기**:\n",
      "   - 일반적인 술을 다른 음료(예: 탄산수, 주스)와 섞어 도수를 낮춥니다.\n",
      "   - 예를 들어, 맥주를 탄산수와 섞어 '셔디'(Shandy)로 만들거나, 진과 토닉워터를 1:2 비율로 섞어 도수를 낮출 수 있습니다.\n",
      "\n",
      "3. **제조 시 저알콜 재료 사용하기**:\n",
      "   - 저알콜 맥주나 와인을 사용하여 음료를 만드는 방법도 있습니다.\n",
      "   - 요즘은 다양한 저알콜 옵션이 많이 나와 있어서 선택할 수 있는 폭이 넓습니다.\n",
      "\n",
      "4. **과일 또는 허브 첨가하기**:\n",
      "   - 과일이나 허브를 추가하여 풍미를 더하고 알코올은 줄이는 방법입니다.\n",
      "   - 예를 들어, 얼음과 함께 신선한 과일 조각이나 민트를 추가하여 상큼한 맛을 낼 수 있습니다.\n",
      "\n",
      "이런 방법들을 활용하면 취하지 않으면서도 즐겁게 음료를 즐길 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "## HuggingFaceEndpoint 를 이용해 API 호출\n",
    "- Inference API 를 지원하고 10GB 이하 크기의 모델은 HuggingFaceEndpoint 를 이용해 호출해서 사용할 수있다. \n",
    "- local에 다운 받지 않아도 되고 GPU가 없을 경우 local 보다 빠르게 실행할 수 있다.\n",
    "- Inference API 지원 여부 확인\n",
    "    - HuggingFace Hub의 개별 모델 페이지에서 \"Deploy - Inference API\" 탭을 확인한다.\n",
    "### API Key (Access Token) 생성\n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken.png)\n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken2.png)\n",
    "- 1. 로그인 -> 2. Profile -> 3. Access Tokens 선택\n",
    "- 생성할 때 `write` 권한을 선택한다.\n",
    "- `HUGGINGFACE_API_KEY=KEY값` 을 환경변수에 등록한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install transformers -qU\n",
    "pip install langchain-huggingface -qU\n",
    "pip install  huggingface_hub -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install  huggingface_hub -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 파인튜닝한 모델을 huggingface-hub 에 올리거나 End Point API  사용할떄  Acess token으로 로그인해야함.\n",
    "huggingface_apikey = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "# huggingface_apikey\n",
    "login(huggingface_apikey)\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cd47afbae04bcb89ba5cb9cf8270f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22e531e848d48cabd87302812420c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9f857986a84b1398e687161ce38ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3d4e56c7f4fec86706d8ca41f551d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b6a09a564947cf84c7655430715e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad5a05b605042ee9f80eae61f1e43ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1606fc521a4b4b91d548a58a5542bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### Local 모델을 사용.\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id, \n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\":50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"한국에서 가장 높은 건물은 어떤 것인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국에서 가장 높은 건물은 어떤 것인가요? 한국에서 가장 높은 건물은 서울의 삼성 tower(삼성전자의 본사)입니다. 2014년에 준공된 이 건물은 3,609미터로,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://www.anthropic.com/pricing#anthropic-api\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "3. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "![anthropic_apikey2.png](figures/anthropic_apikey2.png)\n",
    "\n",
    "4. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "5. 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing -> complete setup\n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "```bash\n",
    "pip install anthropic -qU\n",
    "pip install -qU langchain-anthropic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic library 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "\n",
    "model = \"clause-3-5-haiky-latest\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁하오.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install ollama`\n",
    "  - `pip install langchain-ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.2.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.20 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-ollama) (0.3.21)\n",
      "Collecting ollama<1,>=0.3.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.4.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.10.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (4.12.2)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama<1,>=0.3.0->langchain-ollama)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.2.3)\n",
      "Downloading langchain_ollama-0.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading ollama-0.4.2-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: httpx, ollama, langchain-ollama\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.0\n",
      "    Uninstalling httpx-0.28.0:\n",
      "      Successfully uninstalled httpx-0.28.0\n",
      "Successfully installed httpx-0.27.2 langchain-ollama-0.2.1 ollama-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama library 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "# 모델은 사전에 pull되어있어야 한다.\n",
    "\n",
    "model_id = \"qwen2:0.5b\"\n",
    "res = chat(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"한강의 수도는는 어디인가요?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.ChatResponse'>\n",
      "model='qwen2:0.5b' created_at='2024-12-03T02:43:09.350372Z' done=True done_reason='stop' total_duration=179041900 load_duration=18511800 prompt_eval_count=19 prompt_eval_duration=41000000 eval_count=9 eval_duration=117000000 message=Message(role='assistant', content='한강의 수도는 서울입니다.', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한강의 수도는 서울입니다.\n",
      "한강의 수도는 서울입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.message.content)  # [ ]로 조회가능.\n",
    "print(res[\"message\"]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=model_id,\n",
    ")\n",
    "res = model.invoke(\"你说的中文?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='对不起，我无法理解您的问题。您能否把问题详细地描述一下？' additional_kwargs={} response_metadata={'model': 'qwen2:0.5b', 'created_at': '2024-12-03T03:04:11.0852264Z', 'done': True, 'done_reason': 'stop', 'total_duration': 849602100, 'load_duration': 557055700, 'prompt_eval_count': 12, 'prompt_eval_duration': 83000000, 'eval_count': 18, 'eval_duration': 208000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-c1813a2b-de56-4584-b08b-0e1ce7e51e50-0' usage_metadata={'input_tokens': 12, 'output_tokens': 18, 'total_tokens': 30}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 로스앤젤레스입니다.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
