{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfee4328-7019-4f85-bb71-d7f172024ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d936d8f7-a203-4452-978f-e2da81b7dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_layer = nn.Embedding(\n",
    "    num_embeddings=10,  # vocab size (총 단어 개수/크기)\n",
    "    embedding_dim=5,    # embedding vector 의 차원수.( 한개의 단어를 몇개 값으로 표현할 것인지..)\n",
    "    padding_idx=0,      # padding 토큰의 index를 지정함.\n",
    "    #                    (pad는 자리만 채우는 토큰이므로 학습이 안되도록 처리하기 위해서.)\n",
    "\n",
    "\n",
    "    # [PAD] (padding) 토큰 : 문장들의 토큰 개수를 맞추기 위해서 사용하는 토큰.\n",
    "    # 예 : 모든 문장의 토큰수를 10개로 할 경우, 10개가 안되는 토큰은 나머지를 [PAD] 토큰으로 채운다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa4f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layer 의 weight 조회\n",
    "e_layer.weight  # word embedding vector들.\n",
    "e_layer.weight.shape # [10: 단어수, 5:embedding 차원]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 5])\n",
      "tensor([[[ 0.0387, -0.1732, -0.6709,  0.5620,  0.2283],\n",
      "         [ 0.2061,  0.3842, -0.3756,  0.1314,  0.7001],\n",
      "         [-0.4062,  0.8131, -1.2943, -0.6762,  0.6318],\n",
      "         [ 1.8482,  0.3350, -0.8926, -1.0007,  1.3751]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  입력 값 - 정수 tensor(LongTensor - int64)를 입력.\n",
    "## 한 개의 문서 : [1, 10, 7, 5]  -> 문서를 구성하는 개별 토큰 idx로 구성됨. 이것들을 1차원으로 묶어서 전달함.\n",
    "\n",
    "input_data = torch.tensor([[1, 3, 2, 7]], dtype=torch.int64)\n",
    "e = e_layer(input_data)\n",
    "print(e.shape)\n",
    "print(e)            # [1: 문서수, 4: 토큰(단어)수, 5: embedding vector 차원] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import torch  # PyTorch 라이브러리를 가져옵니다.\n",
    "\n",
    "- `e_layer`는 코드의 다른 부분에서 정의된 임베딩 레이어라고 가정합니다.\n",
    " 예를 들어, 다음과 같이 정의되었을 수 있습니다:\n",
    " e_layer = torch.nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
    " `num_embeddings`는 단어 사전(보케뷸러리)의 크기이고, `embedding_dim`은 각 임베딩 벡터의 차원 크기입니다.\n",
    "\n",
    " [1, 4] 형태를 가진 입력 텐서를 정의합니다. 이 텐서에는 단어 사전의 항목을 나타내는 인덱스가 들어 있습니다.\n",
    "input_data = torch.tensor([[1, 3, 2, 7]], dtype=torch.int64)\n",
    " `input_data`는 [1, 3, 2, 7] 인덱스를 포함합니다.\n",
    " 이러한 인덱스는 `e_layer`를 사용하여 각 항목에 해당하는 임베딩 벡터로 변환됩니다.\n",
    "\n",
    " 입력 데이터를 임베딩 레이어에 전달하여 대응하는 임베딩 벡터를 가져옵니다.\n",
    "e = e_layer(input_data)\n",
    " 임베딩 레이어는 `input_data`의 각 인덱스를 5차원 벡터로 매핑합니다.\n",
    " 결과로 나온 `e`의 크기는 [1, 4, 5]입니다:\n",
    "   - `1`: 배치 크기 (한 개의 배치가 입력으로 주어짐).\n",
    "   - `4`: 시퀀스 길이 (입력 시퀀스에 포함된 인덱스 개수).\n",
    "   - `5`: 임베딩 차원 (임베딩 레이어가 출력하는 벡터 크기).\n",
    "\n",
    "- 결과 텐서의 크기를 출력하여 차원을 확인합니다.\n",
    "print(e.shape)\n",
    "- 출력: torch.Size([1, 4, 5])\n",
    "\n",
    " 임베딩 텐서의 값을 출력합니다.\n",
    "print(e)\n",
    "- 출력: [1, 4, 5] 형태의 텐서이며, 각 5차원 벡터는 `input_data`에서 주어진 인덱스에 해당하는 임베딩입니다.\n",
    " 예를 들어, 인덱스 `1`의 임베딩은 [0.0387, -0.1732, -0.6709, 0.5620, 0.2283]입니다.\n",
    " 이러한 임베딩은 학습 가능한 파라미터로 모델 훈련 과정에서 업데이트됩니다.\n",
    "\n",
    "- 출력 예시:\n",
    " tensor([[[ 0.0387, -0.1732, -0.6709,  0.5620,  0.2283],  # 인덱스 1의 임베딩\n",
    "          [ 0.2061,  0.3842, -0.3756,  0.1314,  0.7001],  # 인덱스 3의 임베딩\n",
    "          [-0.4062,  0.8131, -1.2943, -0.6762,  0.6318],  # 인덱스 2의 임베딩\n",
    "          [ 1.8482,  0.3350, -0.8926, -1.0007,  1.3751]]], # 인덱스 7의 임베딩\n",
    "        grad_fn=<EmbeddingBackward0>)\n",
    " 참고: `grad_fn=<EmbeddingBackward0>`는 이 텐서가 연산 그래프의 일부이며, 이를 통해 그라디언트를 계산할 수 있음을 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d166e4e-4ced-4bc8-a188-12358670ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0387, -0.1732, -0.6709,  0.5620,  0.2283],\n",
       "        [ 0.2061,  0.3842, -0.3756,  0.1314,  0.7001],\n",
       "        [-0.4062,  0.8131, -1.2943, -0.6762,  0.6318],\n",
       "        [ 1.8482,  0.3350, -0.8926, -1.0007,  1.3751]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer.weight[[1, 3, 2, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 생성\n",
    "\n",
    "## Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e2ea3-6123-4ebd-8e98-27b1db6406ed",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcfa3a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: korpora in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: dataclasses>=0.6 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (0.6)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (4.67.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (2.32.3)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.46.0->korpora) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0534360-f0ee-48ce-bb46-24c004d63272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\Playdata\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\Playdata\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a3a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input = corpus.get_all_texts()    # input : 전체 댓글(전체)을 로딩함\n",
    "all_labels = corpus.get_all_labels()  # output :  0:부정, 1:긍정(전체)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8818cafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f57c0b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da20a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232a23c0-06c5-49b7-b601-1ede1198b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.train: size=150000\n",
       "  - NSMC.train.texts : list[str]\n",
       "  - NSMC.train.labels : list[int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43170e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.test: size=50000\n",
       "  - NSMC.test.texts : list[str]\n",
       "  - NSMC.test.labels : list[int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccd3cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 1, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.texts[:10]\n",
    "corpus.test.labels[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e54a-548d-4bf3-81aa-357ab249f41a",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "1. 형태소 단위 token화(분절)를 먼저 한다.\n",
    "    - konlpy로 token화 한 뒤 다시 한 문장으로 만든다.\n",
    "2. 1에서 처리한 corpus를 BPE 로 token화\n",
    "   \n",
    "### 전처리 함수\n",
    "\n",
    "#### 형태소 단위 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0bbb39b-9f49-4d29-a969-4839c01f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Mecab\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    1. 영문 -> 소문자로 변환\n",
    "    2. 구두점 제거\n",
    "    3. 형태소 기반 토큰화\n",
    "    4. 형태소로 토큰화 한 뒤 다시 하나의 문자열로 묶어서 반환.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)  # 구두점을 공백으로 변환.\n",
    "    tokens = okt.morphs(text, stem=True)  # stem : 원형 복원.\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292ded4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import string\n",
    "# list(string.punctuation)\n",
    "f\"[{string.punctuation}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f45e2f-f76a-4011-b963-d7feb214cc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'내가 어제 밥을 먹었다'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['내가', '어제', '밥을', '먹었다'])  # ' '을 기준으로 리스트의 문자열들을 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e658962-2fd6-4b1d-b4ef-a38de3ecc847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙 진짜 짜증나다 목소리'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing('아 더빙.. 진짜 짜증나네요 목소리')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18a25c93-7e26-4512-a0c0-56218fcda100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "### train set/ test set 전처리.\n",
    "train_input = corpus.train.texts\n",
    "\n",
    "\n",
    "\n",
    "train_texts = [text_preprocessing(txt) for txt in train_input]\n",
    "train_labels = corpus.train.labels\n",
    "\n",
    "test_input = corpus.test.texts\n",
    "test_texts = [text_preprocessing(txt) for txt in test_input]\n",
    "test_labels = corpus.test.get_all_labels\n",
    "\n",
    "\n",
    "## 전처리된 input을 합치기 (토큰화를 위해서)\n",
    "all_texts = train_texts + test_texts\n",
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c7cad7-23d9-4212-b07a-0e5da2ff73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e519a68-d3a0-4481-bcf7-b121d8ba813f",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "    - BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "    - https://huggingface.co/docs/tokenizers/quicktour\n",
    "    - `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c46fb56-1bff-45f8-ba76-30276c89286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000   # max vocab size \n",
    "min_frequency = 5    # 5회 이상 나온 단어들만 사전에 추가할 것.\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[UNK]')\n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    continuing_subword_prefix=\"##\"   # 시작하는 단어가 아닌경우 ## 을 앞에 붙인다. ex) 시작하는 -> 시작, ##하는\n",
    ")\n",
    "\n",
    "\n",
    "# 학습\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer)  # 학습데이터가 메모리에 있을때 사용하는 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f162bdf-fac9-4468-a264-c656e4b3164d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26739"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 vocab size\n",
    "\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de9b29e1-384a-44e8-ab19-e53f0c1303c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]의 id: 0\n",
      "北\n"
     ]
    }
   ],
   "source": [
    "print(f\"[PAD]의 id: {tokenizer.token_to_id('[PAD]')}\")\n",
    "print(tokenizer.id_to_token(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e998d38b-e762-4fd5-b37f-8f8a2b6f5849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나다 목소리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1986, 5881, 5426, 5667, 6087]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "print(all_texts[0])\n",
    "# tokenizer.encode(all_texts[0])\n",
    "r = tokenizer.encode(all_texts[0])\n",
    "r.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d74c80fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙', '진짜', '짜증나다', '목소리']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f44ccea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙 진짜 짜증나다 목소리'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1986, 5881, 5426, 5667, 6087])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cef86369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenizer를 저장\n",
    "os.makedirs(\"saved_model/nsmc\", exist_ok=True)\n",
    "tokenizer.save(\"saved_model/nsmc/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80332d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a2c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f5c31d-633c-4a31-8f66-dff2ecf8e86a",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff27280e-dfb7-4947-9192-777e6984286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "        labels: list - 댓글 감정 목록. \n",
    "        max_length: 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = [self.__pad_token_sequences(tokenizer.encode(text).ids) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    ###########################################################################################\n",
    "    # id로 구성된 개별 문장 tokenizer list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "    ############################################################################################\n",
    "    def __pad_token_sequences(self, token_sequences):   #  padding 처리하여 굴자수를 맞춰줌.\n",
    "        \"\"\"\n",
    "        token id로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "        max_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "            ex) [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.token_to_id('[PAD]')\n",
    "        seq_len = len(token_sequences)      # 문장의 token 갯수\n",
    "        result = None\n",
    "        if seq_len > self.max_length:    # 잘라내기\n",
    "            result = token_sequences[:self.max_length] \n",
    "\n",
    "        else:  # [PAD]토큰을 추가.\n",
    "            result = token_sequences + ([pad_token] * (self.max_length - seq_len))\n",
    "        return result\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "        Parameter\n",
    "            idx: int 조회할 index\n",
    "        Return\n",
    "            tuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "        \"\"\"\n",
    "        txt = self.texts[idx]    # idx 번째 댓글 문장 조회\n",
    "        label = self.labels[idx] # idx 번째 정답\n",
    "        # encode = self.tokenizer.encode(txt)   # 토큰화\n",
    "        # padding_encode = __pad_token_sequences(encode)\n",
    "\n",
    "        # (input, output) input : Enbedding Layer에 입력으로 들어감 -> int64 정수형의 'LongTensor'\n",
    "        # output: [label]   # 최종적으로 loss 함수에 입력할때, (batch, 1(label)) 형태가 되도록 하기위함.\n",
    "\n",
    "        return (torch.tensor(txt, dtype=torch.int64), torch.tensor([label], dtype=torch.float32))\n",
    "    \n",
    "\n",
    "# BCELoss() : 정답 shape (batch, 1)   -  [[1], [0], [0]]\n",
    "# CrossEntropyLoss(): 정답 shape (batch, ) - [1, 6, 3, ..]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9515bb6f-703d-4277-b645-8869267f5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(tokenizer.encode(text)) for text in all_texts]  # 모든 문장들의 토큰 개수 리스트로 변환됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "161f63e1-4ba8-4ccc-8c62-422b1f30b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 89)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:5]\n",
    "min(a), max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee963d58-0008-4fa6-b426-e3e332591f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 41.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.quantile(a, q=[0.9, 0.95])  # padding 갯수 결정 about 30개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 생성\n",
    "# max_token 수를 30개\n",
    "MAX_TOKEN = 30\n",
    "train_set = NSMCDataset(train_texts, train_labels, MAX_TOKEN, tokenizer)\n",
    "test_set = NSMCDataset(test_texts, test_labels, MAX_TOKEN, tokenizer)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e60c9f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([21904, 20139,  2203,  8676,  2107,  6000,  2178,  2107,  5484,  2602,\n",
       "          3162,   506,  5434,  1317,  2231,  5560,   988,  5651,  2206,  5414,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d62aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d735046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f647d0c2-829a-41f6-a922-ee33b2450f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac88afd6-5c8f-4ade-b930-86c9425e86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverCommentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, bidirectional=True, dropout_rate=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,    # vocab  크기 (총 단어수)를 모르기 때문에 , parameter 로 받을 얘정.\n",
    "            embedding_dim=embedding_dim,  # embedding vector 의 차원(의 개수) -> 개별 토큰의 차원수가 된다.\n",
    "            padding_idx= 0                # [PAD]의 index. [PAD]에 대한 weight 는 따로 학습하지 않는다.\n",
    "        )\n",
    "        # embedding layer 의 출력 shape:  (64 batch_size, seq_length: 문장의 토큰수, embedding vector의 차원수)\n",
    "        ##  (64, 30, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,  # 모두가 튜닝 대상으로 함수에 넣어서 받아서 사용.\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate\n",
    "\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 입력 : LSTM 의 마지막 timestep 의 output [ out, (h, c) = lstm(X) ] -> hidden state)값을 받아올 예정.\n",
    "        if bidirectional == True:\n",
    "            i_features = hidden_size * 2\n",
    "\n",
    "        else:\n",
    "            i_features - hidden_size\n",
    "        self.classifier = nn.Linear(i_features, 1)  # out_features : 1  -> 긍정일 확률\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X : [batch_size, seq_len(문장의 토큰수)]  [64, 30]\n",
    "        embedding_vector = self.embedding(X)\n",
    "        \n",
    "        # embedding_vector: [batch_size, seq_len, embedding_dim]  ->  [seq_len, batch_size, embedding_dim]\n",
    "        embedding_vector = embedding_vector.transpose(1, 0)   # batch 축과 seq_len 축을 바꿈.\n",
    "        output, _ = self.lstm(embedding_vector) # seq, batch, em_di\n",
    "        \n",
    "        # output : [seq_len, batch_size, hidden_size * (2 if bidirectional else 1)]  => 마지막  output을 추출\n",
    "        output = output[-1]  # shape : (batch_size, hidden_size)  마지막 sequence를 가져옴.\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        last_output = self.sigmoid(output)\n",
    "        return last_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9cbbd8-d8b4-4a51-ac7a-5765722f139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5]), torch.Size([5, 3, 2]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((3, 2, 5))\n",
    "print(a.shape)\n",
    "b = a.transpose(1, 0)\n",
    "c = a.permute(2, 0, 1)\n",
    "\n",
    "b.shape, c.shape\n",
    "\n",
    "# transpose : 값의 위치 자체를 바꿈.\n",
    "# reshape : 출력 값의 위치만 바꿈.\n",
    "# permute :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a0171-371e-4cb5-9bd5-ad1e3c14480d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7830d2b5-d5ed-4b53-a442-bebc83077aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaverCommentClassifier(\n",
      "  (embedding): Embedding(26739, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 32, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NaverCommentClassifier(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=32,\n",
    "    num_layers=2,\n",
    "    bidirectional=True, # (default 값이 True)\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "edbe5b3d-8b2f-4164-9b97-29e6aadced5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NaverCommentClassifier                   [10, 1]                   --\n",
       "├─Embedding: 1-1                         [10, 30, 100]             2,673,900\n",
       "├─LSTM: 1-2                              [30, 10, 64]              59,392\n",
       "├─Dropout: 1-3                           [10, 64]                  --\n",
       "├─Linear: 1-4                            [10, 1]                   65\n",
       "├─Sigmoid: 1-5                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 2,733,357\n",
       "Trainable params: 2,733,357\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 44.56\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 10.93\n",
       "Estimated Total Size (MB): 11.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(1, 10, (10, 30), dtype=torch.int64)\n",
    "summary(model, input_data=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a8d229e-9a99-4a28-9dfd-9582686ba052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습전) 추론\n",
    "x, y = next(iter(train_loader))\n",
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb3e84d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4804],\n",
       "        [0.4853],\n",
       "        [0.4777],\n",
       "        [0.4832],\n",
       "        [0.4699],\n",
       "        [0.4772],\n",
       "        [0.4844],\n",
       "        [0.4776],\n",
       "        [0.4792],\n",
       "        [0.4697],\n",
       "        [0.4833],\n",
       "        [0.4784],\n",
       "        [0.4780],\n",
       "        [0.4734],\n",
       "        [0.4752],\n",
       "        [0.4761],\n",
       "        [0.4745],\n",
       "        [0.4766],\n",
       "        [0.4840],\n",
       "        [0.4739],\n",
       "        [0.4714],\n",
       "        [0.4662],\n",
       "        [0.4738],\n",
       "        [0.4776],\n",
       "        [0.4780],\n",
       "        [0.4792],\n",
       "        [0.4733],\n",
       "        [0.4786],\n",
       "        [0.4656],\n",
       "        [0.4814],\n",
       "        [0.4891],\n",
       "        [0.4681],\n",
       "        [0.4761],\n",
       "        [0.4778],\n",
       "        [0.4772],\n",
       "        [0.4714],\n",
       "        [0.4789],\n",
       "        [0.4777],\n",
       "        [0.4753],\n",
       "        [0.4883],\n",
       "        [0.4747],\n",
       "        [0.4782],\n",
       "        [0.4819],\n",
       "        [0.4797],\n",
       "        [0.4748],\n",
       "        [0.4622],\n",
       "        [0.4676],\n",
       "        [0.4827],\n",
       "        [0.4772],\n",
       "        [0.4798],\n",
       "        [0.4780],\n",
       "        [0.4840],\n",
       "        [0.4827],\n",
       "        [0.4694],\n",
       "        [0.4714],\n",
       "        [0.4733],\n",
       "        [0.4668],\n",
       "        [0.4821],\n",
       "        [0.4681],\n",
       "        [0.4796],\n",
       "        [0.4765],\n",
       "        [0.4840],\n",
       "        [0.4831],\n",
       "        [0.4665]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46099bec-eee3-4cef-921b-ce9ee6cf0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device='cpu'):\n",
    "    # 1 epoch 학습\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 추론\n",
    "        pred = model(X)\n",
    "        \n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    return sum(loss_list)/len(dataloader)  # 1 epoch train loss : step loss 들의  평균을 반환.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c5eb3ea-78e8-4248-a8ce-d07a4c362d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_fn, device=\"cpu\"):\n",
    "    nodel = model.to(device)\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    acc_list = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_proba = model(X)  # 양성일 확률\n",
    "            pred_label = (pred_proba > 0.5).type(torch.int32)  # bool -> int (False: 0,  True: 1)\n",
    "            loss = loss_fn(pred_proba, y)\n",
    "            loss_list.append(loss.item())\n",
    "            acc_list += (y == pred_label).sum().item()\n",
    "\n",
    "        return mean(loss_list), acc_list/len(dataloader.dataset)  # 검증 loss, accuravy 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de42da4-8991-4bcb-9ce9-18155459dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8853d0-b137-47bb-8f0d-fc4f05700cf2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd806dda-5058-4c44-a3f4-28cadc8a90d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e18c2-17bf-4621-b630-b47e16c34bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6ec21-1d3c-48c2-b7c3-314daea6576c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c16618-1517-4371-8e37-ae3cf03428b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d41e8-0715-4f50-aa37-11a8e142706a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3de7ed5-f7f6-4206-b16f-f8535a03405c",
   "metadata": {},
   "source": [
    "# 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdaa3-008d-4a93-aee6-0877e829ef32",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2661a9-3964-4117-b273-e5d8bd4194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "    return ' '.join(morph_tokenizer.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315603df-159b-4317-9fb4-7897546b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "    \"\"\"padding 처리 메소드.\"\"\"\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')  \n",
    "    seq_length = len(token_sequences)           \n",
    "    result = None\n",
    "    if seq_length > max_length:                 \n",
    "        result = token_sequences[:max_length]\n",
    "    else:                                            \n",
    "        result = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73070a-0ee5-4f35-996c-b0d11ba08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "    \"\"\"\n",
    "    모델에 입력할 수있는 input data를 생성\n",
    "    Parameter:\n",
    "        text_list: list - 추론할 댓글리스트\n",
    "    Return\n",
    "        torch.LongTensor - 댓글 token_id tensor\n",
    "    \"\"\"\n",
    "   \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e19997-6b61-446f-ac72-376cd34ee495",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb00ab-60d0-45f2-bb16-505a5f5cc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c330d-69ff-43fb-9ee9-ad1c6054f9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273ac02-81f3-4391-b802-f9dca1f8d032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0510a47-9189-4c2a-a6e9-33b04971f2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
