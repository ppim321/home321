{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers 를 이용해 Backbone 사용\n",
    "\n",
    "## Transformers 설치\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3817d6ef-7239-4738-89c1-1d6a64d8a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "- 기본 모델 Loading  \n",
    "\n",
    "    1. **AutoModel**\n",
    "       - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "       - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.  \n",
    "\n",
    "    2. **AutoTokenizer**\n",
    "       - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "       - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.  \n",
    "       \n",
    "    3. **AutoConfig**\n",
    "       - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.  \n",
    "\n",
    "       - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`  \n",
    "\n",
    "- Task 처리 모델 Loading\n",
    "    - Pretrained backbone 모델에 각 task 에 맞는 estimator layer를 추가한 모델을 생성해 제공한다.\n",
    "    - 주요 모델들\n",
    "        1. **AutoModelForSequenceClassification**\n",
    "           - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`  \n",
    "\n",
    "        2. **AutoModelForQuestionAnswering**\n",
    "           - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`   \n",
    "           \n",
    "        3. **AutoModelForTokenClassification**\n",
    "           - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b9ab90-27b3-4e3a-b907-8c6345552ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd42e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef773cb-9947-4b0b-9deb-16bb7ea1b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875d4ebe85534f5c920478bc31e5e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\NLP\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa47771893d24a92936afee9d4dd2db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02fcecd421d419a8c1c4ea613d9c151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))    # pretrained 된 토크나이저\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "print(type(model))   # pretrained 된 모델\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(type(config))   # pretrained 된 모델 설정.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9261a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': array([[ 101, 1045, 2572, 1037, 2879, 1012,  102]]),\n",
      " 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]])}\n",
      "[[1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "text = \"I am a boy.\"\n",
    "tokens = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",  # 결과의 타입 지정함.(pt: torch.tensor, tf: tensorflow.tensor, np: ndarray, default: list)\n",
    ")\n",
    "pprint(tokens)\n",
    "print(tokens['attention_mask'])\n",
    "# attention_mask: 각 토큰이 실제 토큰인지 (1), padding(0) 구분하는 값들로 구성.\n",
    "# input_ids: 토큰 id\n",
    "# token_type_ids : 입력으로 두개 문장을 받았을때 몇번째 문장의 토큰인지를 구분. (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca386e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2572, 1037, 2879, 1012, 102]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)  # 토큰 id 만 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a197d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 1045, 2572, 1037, 2879, 102],\n",
      "               [101, 1045, 2572, 7501, 1012, 102],\n",
      "               [101, 1045, 2572, 2183, 2000, 2188, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "## 여러 문서(문장)들을 한번에 토큰화\n",
    "### -> max_length, truncation 설정\n",
    "### -> dafault 토큰 길이 : 개별 문장별 길이에 맞춰 토큰 생성.\n",
    "text_list = [\"I am a boy\", \"I am hungry.\", \"I am going to home.\"]\n",
    "token_list = tokenizer(\n",
    "    text_list,\n",
    "    max_length=7,    #최대 토큰 수\n",
    "    padding=True,    # padding 추가\n",
    "    truncation=True  # max_lenghth 가 넘을 경우 짤라낸다.\n",
    ")\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d0d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 1045, 2572, 1037, 2879, 102, 0, 0],\n",
      "               [101, 1045, 2572, 7501, 1012, 102, 0, 0],\n",
      "               [101, 1045, 2572, 2183, 2000, 2188, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "token_list = tokenizer(\n",
    "    text_list,\n",
    "    # max_length=7,    #최대 토큰 수\n",
    "    padding=True,    # max_length 설정 없이 'padding=True' -> 제일 긴 토큰을 가진 문서에 맞춘다.\n",
    "    # truncation=True  # max_lengh?th 가 넘을 경우 짤라낸다.\n",
    ")\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bd6fd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 어휘개수: 30522 30522\n",
      "token_id -> token 단어: [PAD]\n",
      "token_id -> token 단어: ['[UNK]', 'spring', 'despite']\n",
      "token_단어 -> token_id: 3500\n",
      "token_단어 -> token_id: [3500, 1045, 2572]\n"
     ]
    }
   ],
   "source": [
    "## 토크나이저의 정보를 조회\n",
    "print(\"총 어휘개수:\", tokenizer.vocab_size, len(tokenizer))\n",
    "print(\"token_id -> token 단어:\", tokenizer.convert_ids_to_tokens(0))\n",
    "print(\"token_id -> token 단어:\", tokenizer.convert_ids_to_tokens([100, 3500, 2750]))\n",
    "print(\"token_단어 -> token_id:\", tokenizer.convert_tokens_to_ids(\"spring\"))\n",
    "print(\"token_단어 -> token_id:\", tokenizer.convert_tokens_to_ids([\"spring\", \"i\", \"am\"]))\n",
    "# tokenizer.get_vocab() # 모든 토큰들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657061e6",
   "metadata": {},
   "source": [
    "### tokenizer 에 토큰들을 추가\n",
    "- tokenizer와 모델을 같이 받았고, tokenizer에 토큰들을 추가했을 경우 이것을 모델에 적용시켜야 한다.  \n",
    "\n",
    "- 모델의 embedding vector의 크기를 재조정해야 하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cfe4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30527\n"
     ]
    }
   ],
   "source": [
    "# 추가\n",
    "print(len(tokenizer))\n",
    "tokenizer.add_tokens([\"미이이이아라\", \"티치터치티칭\", \"##커치커피컴\", \"뿌ㅇㄹㄹ리링\", \"adlskdhlk\"])\n",
    "# 사이즈\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c38245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30527, 768, padding_idx=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 업데이트 (단어를 추가했기 때문에 모델도 업데이트가 필요함.)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 모델을 한글 텍스트로 학습 시킨 Pretrained model.\n",
    "    - BERT는 Transformer의 Encoder 부분을 이용해 구현된 언어모델\n",
    "    - https://arxiv.org/abs/1810.04805 \n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2518bce-6fd0-4512-9d59-0def0653095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = \"beomi/kcbert-base\"     # base model => Feature Extractor\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28171f1e-7588-4583-b027-224ab6e09c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"안녕하세요. 반갑습니다.\", \n",
    "             \"kcbert는 bert 모델을 한국어로 학습한 모델입니다.\", \n",
    "             \"토크나이저와 모델은 같은 ID의 것으로 받아야 합니다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5f465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokenizer(\n",
    "    text_list,\n",
    "    max_length=10,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "639e509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fa16d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    2, 19017,  8482,    17,  1483,  4981,  8046,    17,     3,     0],\n",
      "        [    2,    76,  4773,  4545, 13146,  4401,  4008,    67, 13146,     3],\n",
      "        [    2,  3160,  4147, 16991,  4488,  4196, 16505,  4057,  8066,     3]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa60edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5087a1-04bb-4b36-912f-c1c5bdd77101",
   "metadata": {},
   "source": [
    "### 입력값 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78342de-1a9c-4b70-a997-c7bca15432ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다.\",\n",
    "    \"2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918a72c-9240-4173-bd94-a734ee5e946e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac67024-c95b-48cf-8329-ae3b59f5a04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1ddf2-e148-43d6-ac7d-30dfa414aa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52498095-7fb8-4da1-9103-d22b10e9ef43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 context vector 추출\n",
    "#### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 context vector 이다.\n",
    "    - 이 값은 **문장을 입력받아 처리하는 task**(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42e0659e-22b0-47b0-96e6-037d9befad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(attention_mask=A, input_ids=I ,token_type_ids=TI)\n",
    "outputs = model(**token_list)   # token_list: dict -> 가변인자에 item 별로 나눠서 입력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64270e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a9f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cfae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape\n",
    "# [3: 문장수-batch, 10:토큰수-> seq-len, 768: 개별토큰의 feature개수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace79e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pooler_output'].shape\n",
    "# 문서(문장)별로 하나씩 -> 문서의 context vector 값 (문서에 대한 feature vector 역할.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60382141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a206af7-a0c1-4025-804d-8012b9eae71c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0e95cd2",
   "metadata": {},
   "source": [
    "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    " 'input_ids': tensor([[    2, 19017,  8482,    17,  1483,  4981,  8046,    17,     3,     0],\n",
    "        [    2,    76,  4773,  4545, 13146,  4401,  4008,    67, 13146,     3],\n",
    "        [    2,  3160,  4147, 16991,  4488,  4196, 16505,  4057,  8066,     3]]),\n",
    " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
