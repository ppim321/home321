{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집개요\n",
    "\n",
    "## 데이터 선정\n",
    "\n",
    "- 요구사항 분석을 통해 분석목표가 정해졌다면 어떤 데이터를 수집할지 선정해야 한다.\n",
    "    - 수집 대상 데이터는 그 목적과 직접적 관련이 있는 데이터과 간접적 관련이 있는 데이터가 있다.\n",
    "    - 예: 축구 승리에 미치는 영향을 주는 요인들\n",
    "        - **축구와 직접 관련된 데이터(요인)**\n",
    "            - 축구 기록 관련: 골, 득점 기대수치, 볼 점유율, 패스 성공율, 슈팅 시도, 태클 시도 및 성공등\n",
    "            - 선수 관련: 연봉, 나이, 경력, 최근 경기 성적등\n",
    "            - 기타: 홈 원정 여부, 경기 시작 시간, 최근 5경기 결과, 직전 경기 결과 등\n",
    "        - **간접 관련된 데이터(요인)**\n",
    "            - 경기 당일 날씨\n",
    "            - 관중수, 응원단 수\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집 방법 선정\n",
    "- 수집할 데이터를 선정했다면 그 다음은 어떻게 데이터를 수집할지 그 방법을 정해야 한다.\n",
    "- **어디서 구할 것인가?**\n",
    "    - 사내 데이터베이스\n",
    "    - 외부 데이터\n",
    "        - 외부 데이터일 경우 어디서 구할 수 있는지 조사해야한다.\n",
    "            - 공개 데이터셋\n",
    "            - 유료 데이터셋\n",
    "            - 데이터 크롤링(crawling)\n",
    "                - 크롤링시 법적 문제는 없는지 확인해야 한다.\n",
    "- **수집할 데이터의 양은 충분한지 확인**\n",
    "    - 의미 있는 결과를 얻으려면 다양한 패턴의 데이터를 의미 있는 양만큼 수집해야 한다.\n",
    "    - 수집 데이터가 한쪽에 편향 되지 않아야 한다. \n",
    "        - 스팸메일을 분석하기 위한 데이터를 수집할 때 정상메일이 스팸메일보다 훨씬 만다면?\n",
    "        - 지하철 호선별 평균이용량을 수집하는데 1호선은 7월, 2호선은 2월, 3호선은 12월 같이 다른 기준으로 데이터를 수집한다면?\n",
    "- **수집할 데이터가 신뢰할만 한 데이터인지 확인 필요**\n",
    "    - 4차산업혁명, 인터넷, SNS 발달로 데이터의 양이 급증가하여 데이터 수집이 쉬워짐.\n",
    "    - 쉬어진 만큼 신뢰하기 힘든 데이터들도 급증함. 그래서 **수집한 데이터가 신뢰할 만한 데이터인지 구별하는 것이 중요해졌다.**\n",
    "        - 특히 출처가 불분명한 데이터 (커뮤니티 글, 유튜브 영상, 지식IN 등)일 경우 확인이 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집 주기\n",
    "- 일회성인지 주기적으로 수집해야 하는 데이터인지에 따라 방법과 도구가 달라질 수 있다.\n",
    "- **일회성 데이터**\n",
    "    - 수집한 데이터를 csv, txt등의 파일형식으로 저장하여 활용한다.\n",
    "    - 변하지 않는 데이터셋으로 국가나 도시정보등이 있다.\n",
    "- **주기적으로 수집이 필요한 데이터**\n",
    "    - 자동화 시스템 구축을 하여 데이터베이스에 데이터를 주기적으로 수집, 저장한다.\n",
    "    - 변하는 데이터로 대부분이 여기에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집에 도움이 되는 사이트\n",
    "\n",
    "- **국가 통계포털**\n",
    "    - https://kosis.kr\n",
    "    - 통계청에서 관리하는 공공데이터 포털로 다양한 카테고리의 국가 통계데이터를 제공한다.\n",
    "- **공공데이터 포털**\n",
    "    - https://www.data.go.kr\n",
    "    - 행정 안전부에서 제공하는 정부 데이터 포털\n",
    "- **Kaggle**\n",
    "    - https://kaggle.com\n",
    "    - 데이터과학 관련 경진대회 플랫폼\n",
    "    - 다양한 데이터들을 제공한다.\n",
    "- **구글 데이터셋 서치**\n",
    "    - https://datasetsearch.research.google.com\n",
    "    - 구글에서 제공하는 데이터셋 검색 사이트\n",
    "    - 키워드를 이용해 다양한 데이터셋을 검색하고 다운로드 받을 수 있다.\n",
    "- **AI Hub**\n",
    "    - https://aihub.or.kr\n",
    "    - 국내외 기관/기업에서 추진한 지능정보산업 인프라 조성사업에서 공개한 AI 학습용 데이터셋들을 제공한다.\n",
    "- **Roboflow Universe**\n",
    "    - https://universe.roboflow.com/\n",
    "    - Roboflow 라는 인공지능 회사에서 운영하는 데이터 저장소 사이트로 컴퓨터비전 관련 데이터셋을 주로 제공한다.\n",
    "- 기타\n",
    "    - **지자체**: 서울시 열린 데이터광장, 경기 데이터 드림\n",
    "    - **금융관련**: 한국거래소, 금융통계정보시스템등\n",
    "    - **영화관련**: 영화진흥위원회\n",
    "    - **대중교통**: 국가교통데이터베이스, 교통카드 빅데이터 통합정보시스템등    \n",
    "    - **관광관련**: 한국 관광 데이터랩등\n",
    "    - **날씨정보**: 기상청 기상자료 개방포털, 네이버 날씨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [크롬개발자 도구](https://developers.google.com/web/tools/chrome-devtools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- Markup 언어 parsing 라이브러리\n",
    "    - HTML이나 XML 문서 내에서 원하는 정보를 가져오기 위한 파이썬 라이브러리.\n",
    "- https://www.crummy.com/software/BeautifulSoup/\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- 설치\n",
    "    - beautifulsoup4 설치\n",
    "        - pip install beautifulsoup4\n",
    "    - lxml 설치(html/xml parser)\n",
    "        - pip install lxml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 37.8 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코딩 패턴\n",
    "1. 조회할 HTML내용을 전달하여 BeautifulSoup 객체 생성 \n",
    "1. BeautifulSoup객체의 메소드들을 이용해 문서내에서 필요한 정보 조회\n",
    "    - 태그이름과 태그 속성으로 조회\n",
    "    - css selector를 이용해 조회\n",
    "    - . 표기법을 이용한 탐색(Tree 구조 순서대로 탐색)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 객체 생성\n",
    "- BeautifulSoup(html str [, 파서])\n",
    "    - 매개변수\n",
    "        1. 정보를 조회할 html을 string으로 전달\n",
    "        2. 파서\n",
    "            - html.parser(기본파서)\n",
    "            - lxml : 매우 빠르다. html, xml 파싱 가능(xml 파싱은 lxml만 가능)\n",
    "                - 사용시 install 필요 \n",
    "                - `conda install lxml`\n",
    "                - `pip install lxml`\n",
    "                - install 후 커널 restart 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--HTML(5) 문서 선언-->\n",
      "<!doctype html>\n",
      "<!-- ROOT 태그:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding='utf-8') as fr:\n",
    "    html_doc = fr.read()\n",
    "print(html_doc[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서내에서 원하는 정보 검색\n",
    "\n",
    "### Tag 객체\n",
    "- 하나의 태그(element)에 대한 정보를 다루는 객체.\n",
    "    - BeautifulSoup 조회 메소드들의 **조회결과의 반환타입.**\n",
    "    - 조회 함수들이 찾은 Element가 하나일 경우 **Tag 객체를, 여러개일 경우 Tag 객체들을 담은 List(ResultSet)**를 반환한다.\n",
    "    - Tag 객체는 찾은 정보를 제공하는 메소드와 Attribute를 가지고 있다. 또 찾은 Tag가 하위 element를 가질 경우 찾을 수 있는 조회 메소드를 제공한다.\n",
    "- 주요 속성/메소드\n",
    "    - **태그의 속성값 조회**\n",
    "        - tag객체.get('속성명') \n",
    "        - tag객체\\['속성명'\\]\n",
    "        - ex) tag.get('href') 또는 tag\\['href'\\]\n",
    "    - **태그내 text값 조회**\n",
    "        - tag객체.get_text()\n",
    "        - tag객체.text\n",
    "        - ex) tag.get_text() 또는 tag.text\n",
    "    - **contents 속성**\n",
    "        - 조회한 태그의 모든 자식 요소들을 리스트로 반환\n",
    "        - ex) child_list = tag.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조회 함수\n",
    "- **태그의 이름으로 조회**\n",
    "    - find_all()\n",
    "    - find()\n",
    "- **css selector를 이용해 조회**\n",
    "    - select(), select_one()\n",
    "- **`.` 표기법(dot notation)**\n",
    "    - dom tree 구조의 계층 순서대로 조회\n",
    "    - 위의 두방식으로 찾은 tag를 기준으로 그 주위의 element 들을 찾을 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그의 이름으로 조회\n",
    "- **find_all**(name=태그명, attrs={속성명:속성값, ..})\n",
    "   - 이름의 모든 태그 element들을 리스트에 담아 반환.\n",
    "   - 여러 이름의 태그를 조회할 경우 List에 태그명들을 묶어서 전달한다.\n",
    "   - 태그의 attribute 조건으로만 조회할 경우 name을 생략한다. \n",
    "- **find**(name=태그명, attrs={속성명:속성값})\n",
    "    - 이름의 태그중 첫번째 태그 element를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find_all(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"animal_info\" id=\"animal1\">\n",
       " <div class=\"name\">사자</div>\n",
       " <div>3마리</div>\n",
       " </div>,\n",
       " <div class=\"name\">사자</div>,\n",
       " <div>3마리</div>,\n",
       " <div class=\"animal_info\">\n",
       " <div class=\"name\">호랑이</div>\n",
       " <div>10마리</div>\n",
       " </div>,\n",
       " <div class=\"name\">호랑이</div>,\n",
       " <div>10마리</div>,\n",
       " <div class=\"animal_info\">\n",
       " <div class=\"name\">곰</div>\n",
       " <div>5마리</div>\n",
       " </div>,\n",
       " <div class=\"name\">곰</div>,\n",
       " <div>5마리</div>,\n",
       " <div class=\"animal_info\">\n",
       " <div class=\"name\">낙타</div>\n",
       " <div>6마리</div>\n",
       " </div>,\n",
       " <div class=\"name\">낙타</div>,\n",
       " <div>6마리</div>,\n",
       " <div id=\"potal\">\n",
       " <a href=\"http://www.naver.com\">네이버</a>\n",
       " <a href=\"http://www.naver.com\">다음</a>\n",
       " <a href=\"http://www.naver.com\">구글</a>\n",
       " <a href=\"http://www.naver.com\">구글2</a>\n",
       " <a href=\"http://www.naver.com\">구글3</a>\n",
       " <a href=\"http://www.naver.com\">구글4</a>\n",
       " </div>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: \n",
      "사자\n",
      "3마리\n",
      " \n",
      "사자\n",
      "3마리\n",
      "\n",
      "class속성값: ['animal_info'] ['animal_info']\n"
     ]
    }
   ],
   "source": [
    "tag1 = result[0]\n",
    "print(\"content:\", tag1.text, tag1.get_text())\n",
    "print(\"class속성값:\", tag1.get(\"class\"), tag1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "--------------------------------------------------\n",
      "<div class=\"animal_info\" id=\"animal1\">\n",
      "<div class=\"name\">사자</div>\n",
      "<div>3마리</div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "result = soup.find(\"div\")  # 1개\n",
    "print(type(result))\n",
    "print(\"-\"*50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content text: \n",
      "사자\n",
      "3마리\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"content text:\", result.text)\n",
    "# print(\"content text:\", result.get_text())\n",
    "# print(\"attribue의 value:\", result.get(\"class\"))\n",
    "# print(\"attribue의 value:\", result[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', <div class=\"name\">사자</div>, '\\n', <div>3마리</div>, '\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://grandpark.seoul.go.kr/main/ko.do\">서울 대공원</a>,\n",
      " <a href=\"https://www.everland.com/web/everland/favorite/zootopia/index.html\">에버랜드</a>,\n",
      " <a href=\"https://www.coexaqua.com\">코엑스아쿠아리움</a>,\n",
      " <a href=\"http://www.naver.com\">네이버</a>,\n",
      " <a href=\"http://www.naver.com\">다음</a>,\n",
      " <a href=\"http://www.naver.com\">구글</a>,\n",
      " <a href=\"http://www.naver.com\">구글2</a>,\n",
      " <a href=\"http://www.naver.com\">구글3</a>,\n",
      " <a href=\"http://www.naver.com\">구글4</a>]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "result = soup.find_all(\"a\")              # 태그이름(a)\n",
    "# result = soup.find_all([\"a\", \"span\"])  # 태그이름(여러개)\n",
    "# result = soup.find_all(\"div\", attrs={\"class\":\"name\"})\n",
    "# result = soup.find_all(\"div\", attrs={\"class\":\"animal_info\", \"id\":\"animal1\"})\n",
    "# result = soup.find_all(\"a\", attrs={\"href\":\"https://www.coexaqua.com\"})\n",
    "\n",
    "# import re\n",
    "# result = soup.find_all(\"a\", attrs={\"href\":re.compile(r\".com$\")}) \n",
    "# 정규표현식 - .com 으로 끝나는 것만 출력됨.\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울 대공원 https://grandpark.seoul.go.kr/main/ko.do\n",
      "에버랜드 https://www.everland.com/web/everland/favorite/zootopia/index.html\n",
      "코엑스아쿠아리움 https://www.coexaqua.com\n",
      "네이버 http://www.naver.com\n",
      "다음 http://www.naver.com\n",
      "구글 http://www.naver.com\n",
      "구글2 http://www.naver.com\n",
      "구글3 http://www.naver.com\n",
      "구글4 http://www.naver.com\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for tag in result:\n",
    "    print(tag.text, tag['href'])\n",
    "    result_list.append([tag.text, tag['href']]) # list[text, href]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['서울 대공원', 'https://grandpark.seoul.go.kr/main/ko.do'],\n",
       " ['에버랜드',\n",
       "  'https://www.everland.com/web/everland/favorite/zootopia/index.html'],\n",
       " ['코엑스아쿠아리움', 'https://www.coexaqua.com'],\n",
       " ['네이버', 'http://www.naver.com'],\n",
       " ['다음', 'http://www.naver.com'],\n",
       " ['구글', 'http://www.naver.com'],\n",
       " ['구글2', 'http://www.naver.com'],\n",
       " ['구글3', 'http://www.naver.com'],\n",
       " ['구글4', 'http://www.naver.com']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selector를 이용해 조회\n",
    "- **select(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag들을 반환한다.\n",
    "- **select_one(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag를 반환한다.\n",
    "    - 일치하는 것이 여러개일 경우 첫번째 것 하나만 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://grandpark.seoul.go.kr/main/ko.do\">서울 대공원</a>,\n",
      " <a href=\"https://www.everland.com/web/everland/favorite/zootopia/index.html\">에버랜드</a>,\n",
      " <a href=\"https://www.coexaqua.com\">코엑스아쿠아리움</a>]\n"
     ]
    }
   ],
   "source": [
    "# result = soup.select(\"a\")             # 태그이름(a)\n",
    "# result = soup.select(\"a, span\")       # 태그이름(여러개)\n",
    "# result = soup.select(\"ul a\")          # ul 의 자손인 a 태그를 찾는다.\n",
    "# result = soup.select_one(\"#animal1\")  # 모든 태그중  id 속성 = animal1\n",
    "# result = soup.select(\"ul + div\")      # ul의 다음 형제 태그중 div\n",
    "# result = soup.select(\"body > div:nth-child(3)\")  # body 의 3번째 자식 div\n",
    "# result = soup.select(\"a[href]\")       # href 속성이 있는 a 태그들\n",
    "# result = soup.select(\"a[href='http://www.naver.com']\")  # href='http://www.naver.com' 속성을 가진 a태그 \n",
    "# result = soup.select('a[href$=\".do\"]')  # $ = href 속성값이 .do로 끝나는 a태그들\n",
    "result = soup.select('a[href^=\"https\"]') # =^ href 속성값이 https로 시작하는 a태그\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울 대공원 https://grandpark.seoul.go.kr/main/ko.do a\n",
      "에버랜드 https://www.everland.com/web/everland/favorite/zootopia/index.html a\n",
      "코엑스아쿠아리움 https://www.coexaqua.com a\n"
     ]
    }
   ],
   "source": [
    "for tag in result:\n",
    "    print(tag.text, tag['href'], tag.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
