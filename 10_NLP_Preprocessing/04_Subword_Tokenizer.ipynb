{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 형태소 분석 기반 토큰화의 문제\n",
    "- 형태소 분석기는 작성된 알고리즘 또는 학습된 내용을 바탕으로 토큰화를 하기 때문에 오탈자나 띄어쓰기 실수, 신조어, 외래어, 고유어 등이 사용된 경우 제대로 토큰화 하지 못한다.\n",
    "- 그래서 발생 할 수있는 잠재적 문제점\n",
    "    - 어휘사전을 크게 만든다.\n",
    "        - 같은 의미의 단어가 형태소 분석이 안되어 여러개 등록될 수있다.\n",
    "        - ex) 신조어 `돈쭐` 이라는 단어를 인식 못할 경우 `\"돈쭐내러\", \"돈쭐나\", \"돈쭐냄\"` 등이 다 등록 될 수 있다.\n",
    "    - OOV(Out Of Vocab)에 대응하기 어렵게 만든다.\n",
    "        - 같은 어근의 단어가 있지만 조사등이 바뀐 신조어등을 OOV로 인식할 수있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ### 어휘 사전(Vocabulary)과 Out Of Vocabulary (OOV)\n",
    "> \n",
    "> - 언어 모델링에서 **어휘 사전(Vocabulary)**은 모델이 처리할 수 있는 단어(토큰)들의 집합이다.  \n",
    "> - 어휘 사전은 보통 전체 데이터셋을 토큰화한 후, 각 토큰을 고유한 정수 인덱스로 매핑해 만든다.\n",
    ">    - 매핑된 정수는 모델에 입력되는 텍스트 데이터를 숫자 형식으로 변환해 모델이 처리할 수 있도록 돕는다.\n",
    ">    - 예시) {\"I\": 1, \"he\": 2, \"you\": 3, ...}\n",
    "> - **Out Of Vocabulary (OOV)**\n",
    ">    - 어휘 사전(Vocab): 코퍼스를 구성하는 모든 토큰의 집합.\n",
    ">    - **OOV**란 어휘 사전에 포함되지 않은 토큰을 의미하며, 모델이 해당 토큰을 처리할 수 없기 때문에 일반적으로 특별한 토큰(예: `[UNK]`)으로 대체되거나 다른 방식으로 처리된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization(하위 단어 토큰화)\n",
    "\n",
    "## 정의\n",
    "\n",
    "- Subword Tokenization은 단어를 더 작은 단위(subword)로 나누어 텍스트를 토큰화하는 방식이다.  \n",
    "    - subword는 하나의 단어를 구성하는 단어들을 말한다.(coworker: co, work, er)\n",
    "- 주로 자주 등장하는 단어의 일부를 공통된 토큰으로 만들고, 희귀하거나 복합적인 단어는 작은 조각(subword)으로 나누어 처리한다.\n",
    "- 단어 자체를 그대로 사용하기보다는 단어의 일부를 나누어 처리함으로써 새로운 단어나 미등록 단어(Out-of-Vocabulary) 문제를 줄일 수 있다.\n",
    "\n",
    "## 장점\n",
    "\n",
    "1. **미등록 단어 처리 가능**  \n",
    "   -  새로운 단어(신조어, 속어, 고유어등)가 등장해도 미리 정의된 subword를 조합해서 표현할 수 있어 OOV 문제를 줄일 수 있다.  \n",
    "\n",
    "2. **어휘 크기 축소**  \n",
    "   - 같은 subword를 여러 단어에서 공유함으로써, 완전한 단어를 사용하는 경우보다 어휘집의 크기를 작게 유지할 수 있다.\n",
    "\n",
    "\n",
    "## 종류\n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**  \n",
    "   - 자주 등장하는 문자 쌍을 반복적으로 병합해 서브워드를 생성하는 방식.\n",
    "   - OpenAI의 GPT 모델에 사용된 토크나이저이다.\n",
    "\n",
    "2. **Unigram**  \n",
    "   - 빈도기반 확률모델에 따라 subword 단위를 선택하는 방식이다.  \n",
    "   - BPE보다 유연하여 더 다양한 분할 결과를 얻을 수 있다.\n",
    "\n",
    "3. **WordPiece**  \n",
    "   - BPE와 유사하지만, 빈도수가 아니라, 가능성이 높은 조합(합쳐질 가능성이 높은 subword)에 기반해 subword들을 찾는다.\n",
    "   - Google의 BERT 모델에 사용된 토크나이저이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding 방식\n",
    "\n",
    "- 원래 Text data 압축을 위해 만들어진 방법으로 text 에서 많이 등장하는 두글자 쌍의 조합을 찾아 부호화하는 알고리즘이다. \n",
    "- 연속된 글자 쌍이 더 나타나지 않거나 정해진 어휘사전 크기에 도달 할 때 까지 조합을 찾아 부호화 하는 작업을 반복한다.\n",
    "\n",
    "## text 압축 방식의 예\n",
    "- 원문: abracadabra\n",
    "1. AracadAra: ab -> A :=> 원문에서 가장 빈도수 많은 ab를 A(부호로 아무 글자나 사용할 수 있다.)로 치환\n",
    "2. ABcadAB: ra -> B :=> 1에서 가장 빈도수가 많은 ra를 B로 치환\n",
    "3. CcadC: AB -> C :=> 2에서 가장 빈도수 맣은 AB를 C로 치환한다.(치환된 글자 쌍도 변환대상에 포함된다.)\n",
    "\n",
    "## BPE Tokenizer 방식\n",
    "BPE 토크나이저는 자주 등장하는 글자 쌍을 찾아 치환하는 대신 **단어 사전**에 추가한다.\n",
    "\n",
    "### 예)\n",
    "1. 말뭉치의 토큰들의 빈도수, 어휘사전은 아래와 같을 경우\n",
    "    - 빈도사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
    "    - 어휘사전: ['low', 'lower', 'newest', 'widest']\n",
    "2. 빈도 사전내의 모든 단어들을 글자 단위로 나눈다. (Pre Tokenization)\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "3. 빈도 사전을 기준으로 가장 자주 등장하는 글자 쌍(byte pair)를 찾는다.  위에서는 **'e'와 's'가 총 9번으로 가장 많이 등장함**. 'e'와 's'를 'es'로 합치고 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'es'**, 't', 6), ('w', 'i', 'd', **'es'**, 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**]\n",
    "4. 3 번의 과정을 계속 반복한다. 빈도수가 가장 많은 'es'와 't' 쌍을 'est'로 병합하고 'est'를 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**]\n",
    "5. 만약 10번 반복했다고 하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.\n",
    "    - 빈도 사전: (**'low'**, 5), (**'low'**, 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**, **'lo'**,**'low'**, **'low'**, **'ne'**, **'new'**, **'newest'**, **'wi'**, **'wid'**, **'widest'**]\n",
    "\n",
    "- 위와 같이 어휘 사전이 만들어 지면 원래 어휘서전에 없던 것들에 대한 처리를 할 수있다.\n",
    "    - ex)\n",
    "        - 'newer' :=> 'new', 'e', 'r', \n",
    "        - 'lowest' :=> 'low', 'est'\n",
    "        - 'wider' :=> 'wid', 'e', 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## comment_txt\n",
    "# 토큰화\n",
    "\n",
    "# word 토큰, id 토큰 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "- Byte Pair Encoding 이 빈도 기반이라면 wordpiece tokenizer는 확률 기반으로 글자 쌍을 병합한다.\n",
    "- 두개 글자 쌍의 빈도수를 각 개별 글자 빈도수의 곱으로 나눈 점수가 가장 높은 순서대로 글자쌍을 묶어 나간다.\n",
    "\n",
    "$$\n",
    "score = \\cfrac{f(x, y)}{f(x)\\cdot f(y)} \n",
    "$$\n",
    "\n",
    "함수 f는 빈도를 나타내며 x, y는 병합하려는 하위 단어이다.\n",
    "\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w')\n",
    "- 가장 빈도수가 높은 쌍은 'e','s'로 9번 등장한다. 이때 각 글자는 전체에서 각각 'e'는 17번, 's'는 9번 등장한다. 위 공식에 대입하면 score는 $\\frac{9}{17 \\times 9} \\approx 0.06$ 이다.\n",
    "- 'i'와 'd' 쌍은 3번만 등장하지만 전체에서 각각 'i' 3번, 'd' 3번 등장한다. 그래서 score는 $\\frac{3}{3 \\times 3} \\approx 0.33$ 이다.\n",
    "- 나타난 빈도수는 'es' 가 많치만 더 높은 score를 가지는 'id' 쌍을 병합한다.\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', **'id'**, 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'id'**)\n",
    "위의 작업을 반복해 연속된 글자 쌍이 더이상 나타나지 않거나 어휘 사전 max 크기에 도달할 때 까지 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram 방식\n",
    "- 빈도 기반 확률 모델을 사용하여 효율적으로 서브워드를 선택하고, 불필요한 서브워드를 제거해 최적의 어휘 크기를 찾는 알고리즘\n",
    "\n",
    "\n",
    "- **초기 어휘 집합 구성**\n",
    "    - 대상 text에 모든 단어와 그 서브스트링을 포함한 어휘 집합을 생성한다. 이 어휘 집합은 나올 수있는 모든 subword들을 다 모아놓은 것이다. \n",
    "    - 예를 들어 \"hug\" 단어의  [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"hug\"]  substring을 만든다. 이들이 subword 후보가 된다.\n",
    "- **각 Subword의 빈도수 기반 확률 계산**\n",
    "    -  $\\cfrac{subword가\\;나타난\\;횟수}{전체\\;빈도수}$ 로 각 subword들의 나타난 확률을 계산한다.\n",
    "- **가능한 분할에 대한 확률 계산**\n",
    "    - 단어를 여러 서브워드로 분할할 수 있는 경우, 각 분할에 대한 전체 확률을 계산한다.\n",
    "    - 확률 계산은 $ P(subword1)\\;\\times \\; P(subword2)\\;\\times\\; ..$ 으로 계산한다.\n",
    "    - 예를 들어 \"hug\" 를 분할 한다고 했을 때\n",
    "        1. \\[\"h\", \"u\", \"g\"\\]: $ P(h) \\times P(u) \\times P(g) $\n",
    "        2. \\[\"hu\", \"g\"\\]: $ P(hu) \\times P(g) $\n",
    "\n",
    "   - 각각의 확률을 계산한 후, **가장 높은 확률**을 가진 분할을 선택한다.\n",
    "     - 위 예에서 만약 1의 확률이 0.01 이고 2의 확률이 0.00001 이라면 첫번째 분할이 선택된다.\n",
    "\n",
    "- **서브워드 제거**\n",
    "    - 위의 훈견과정에서 불필요한 서브워드를 제거하면서 최적의 어휘 집합을 찾아간다. \n",
    "    - 제거 대상은 빈도수가 낮거나 조합에 크게 영향을 주지 않은 subword들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting korpora\n",
      "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from korpora)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (4.67.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (2.32.3)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from korpora) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests>=2.20.0->korpora) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.46.0->korpora) (0.4.6)\n",
      "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: dataclasses, korpora\n",
      "Successfully installed dataclasses-0.6 korpora-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\miniconda3\\envs\\ml\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 22.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Installing collected packages: huggingface-hub, tokenizers\n",
      "Successfully installed huggingface-hub-0.26.2 tokenizers-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2017-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-03\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-04\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-05\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-06\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-07\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2018-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\Playdata\\Korpora\\korean_petitions\\petitions_2019-03\n"
     ]
    }
   ],
   "source": [
    "# Korora 데이터셋 loading\n",
    "from Korpora import Korpora\n",
    "corpus = Korpora.load(\"korean_petitions\")  # 가져올 데이터셋 이름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433631"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions = corpus.get_all_texts()  #  개별 청원들을 list로 묶어서 반환\n",
    "len(petitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"현재 대한민국의 환경 및 식품 안전 문제에 빨간 신호등이 켜 진지 오래다. 그것은 암 환자 발생율 증가가 말해 주고 있다. 그러나 우리나라의 암환자 발생율의 추이가 공적으로 발표되어 인구에 회자된 적이 없다. 문재인 정부에서 이것을 지금 공론화 시키는 것이 좋다고 생각한다. 왜냐하면 앞으로 있을 대선기에 이 문제가 수구 야당으로 부터 이슈화될 수 있기 때문이다. 그리고 국민의 생명을 지키는 것은 집권당의 무한 책임이다. 또 이 문제에 대한 공론화가 '적폐 청산' 및 '나라다운 나라' 라는 현 정부의 이슈에도 부합하여 정치적으로 대단히 유리하다고 본다.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 파일로 저장\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/petitions_corpus.txt', 'wt', encoding='utf-8') as fw:\n",
    "    for petitions in petitions:\n",
    "        fw.write(petitions+\"\\n\")  # 한줄에 하나의 청원을 저장.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용,\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = open('data/petitions_corpus.txt', encoding='utf-8').read()\n",
    "p[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'습니다 네 물론 근로계약서에 사인을 하지 않으면 되죠 ? 그럼 저는 일자를 잃습니다 . 어느곳이든 갈곳이 없어지게 되겠죠 국가 공휴일 연차 대체 불가능 만들어 주십시오 교사가 당당히 연차 휴가서를 작성하게 해주십시오 사용하지 않은 연차는 연차 수당을 지급 받을수 있도록 해주십시오 보육교사 노예 해방 부탁드립니다 인권과 권리를 찾아주시길 바랍니다 !!!!!\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222421175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/petitions_corpus.txt', 'rt', encoding='utf-8') as fr:\n",
    "          petitions = fr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용, 기간제 교사, 영전강, 스강의 무기계약직화가 그들의 임용 절벽과는 전혀 무관한 일이라고 주장하고 있지만 조금만 생각해보면 전혀 설득력 없는 말이라고 생각합니다. 학교 수가 같고,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face tokenizers 패키지 사용해 토큰화 수행\n",
    "- Subword tokenization을 처리하는 다양한 패키지(라이브러리)가 있다.\n",
    "\n",
    "## 주요 라이브러리 \n",
    "- [tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "    - huggingface에서 개발한 tokenizer 라이브러리로 BPE, WordPiece, Unigram 알고리즘을 지원한다. \n",
    "    - 설치: `pip install tokenizers`\n",
    "- [Sentencepiece](https://github.com/google/sentencepiece)\n",
    "    - 구글에서 개발한 subword tokenizer 라이브러리로 BPE, Unigram 방식 지원.\n",
    "    - 설치: `pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### korpora 말뭉치\n",
    "> - 다양한 한글 데이터셋을 제공하는 패키지\n",
    "> - `pip install korpora`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face tokenizers 패키지이용\n",
    "\n",
    "- 설치: `pip install tokenizers`\n",
    "- Tokenizer 생성\n",
    "    - 토큰화 알고리즘을 지정해 instance 생성.\n",
    "- Trainer 생성\n",
    "    - 학습 파라미터를 설정해서 instance 생성\n",
    "- Tokenizer 학습\n",
    "    - train() 메소드: 학습 text 파일 경로를 지정해서 학습\n",
    "    - train_from_iterator() 메소드: 학습할 string들을 iterator를 통해 제공.\n",
    "- https://github.com/huggingface/tokenizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 생성\n",
    "# BPR() : 토큰화 방식, unk_token : 모르는 단어(OOV-단어사전에 없는 토큰)를 표시할 스페셜 토큰을 지정.\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))  \n",
    "# tokenizer에 Pre Tokenizer를 설정. (BPE 전에 일단 어떻게 나누고 시작할지를 지정.)\n",
    "tokenizer.pre_tokenizer = Whitespace()  # 공백 기준으로 토큰화. 토큰화한 결과를 기준으로 쌍을 찾는다.\n",
    "\n",
    "# Train 생성 -> Tokenizer 를 어떻게 학습시킬지 설정.\n",
    "trainer = BpeTrainer(   # subword 방식마다 Model과 Trainer class가 있다.\n",
    "    vocab_size = 10000, # 어휘사전 크기 (고유토큰 최대 개수.)\n",
    "    min_frequency = 5,  # 최소 출현 횟수. 지정한 횟수(5) 이하로 출현한 쌍(pair)은 제외한다.\n",
    "    special_tokens = [\"[UNK]\", \"[PAD]\"], \n",
    "    # Vocab 에 추가할 특수토큰들 지정. (학습 텍스트에서 찾는 토큰이 아니라 명시적으로 넣어주는 토큰)\n",
    "    ## 이 중 unk_token에 지정한 Unknown 토큰은 반드시 지정해야한다.\n",
    "    # [UNK] : \n",
    "    # [PAD] : 문장을 model 에 입력시, 토큰의 갯수를 강제하여 임의로 채워줌.\n",
    "    ## 특수 토큰은 특수 목적을 가지며, \"문장시작\", \"문장끝\", \"패딩처리\", \"문장중간비우기\" 등등이 있다. \n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.912415504455566 초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "s = time.time()\n",
    "tokenizer.train([\"data/petitions_corpus.txt\"], trainer=trainer) # 학습시킬 파일들의 경로, Trainer 설정.\n",
    "e = time.time()\n",
    "print(e-s, \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'saved_models/tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m####  토크나이저 저장\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_models/tokenizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/tokenizers/petitions_bpe.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'saved_models/tokenizers'"
     ]
    }
   ],
   "source": [
    "####  토크나이저 저장\n",
    "import os \n",
    "os.makedirs(\"saved_models/tokenizers\")\n",
    "\n",
    "\n",
    "tokenizer.save(\"saved_models/tokenizers/petitions_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 불러오기\n",
    "load_tokenizer = Tokenizer.from_file(\"saved_models/tokenizers/petitions_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장\n",
    "sports_txt = \"프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.\"\n",
    "petition_txt = \"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\"\n",
    "comment_txt = \"멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Encoding"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰으로 변환\n",
    "output = tokenizer.encode(sports_txt)  # 토큰화 word, 토큰화된 index(id)\n",
    "\n",
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '리',\n",
       " '미',\n",
       " '어',\n",
       " '리',\n",
       " '그',\n",
       " '역',\n",
       " '대',\n",
       " '개인',\n",
       " '최',\n",
       " '다',\n",
       " '골',\n",
       " '기록',\n",
       " '을',\n",
       " '보유',\n",
       " '하고',\n",
       " '있는',\n",
       " '시',\n",
       " '어',\n",
       " '러',\n",
       " '가',\n",
       " '손',\n",
       " '흥',\n",
       " '민',\n",
       " '의',\n",
       " '골',\n",
       " '결정',\n",
       " '력을',\n",
       " '재',\n",
       " '차',\n",
       " '극',\n",
       " '찬',\n",
       " '했다',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens # 문자로된 token (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7390,\n",
       " 5123,\n",
       " 5330,\n",
       " 6140,\n",
       " 5123,\n",
       " 4128,\n",
       " 6180,\n",
       " 4589,\n",
       " 8414,\n",
       " 6891,\n",
       " 4563,\n",
       " 4027,\n",
       " 9449,\n",
       " 6364,\n",
       " 9442,\n",
       " 8209,\n",
       " 8219,\n",
       " 5908,\n",
       " 6140,\n",
       " 4980,\n",
       " 3902,\n",
       " 5802,\n",
       " 7638,\n",
       " 5332,\n",
       " 6384,\n",
       " 4027,\n",
       " 8940,\n",
       " 8644,\n",
       " 6449,\n",
       " 6793,\n",
       " 4129,\n",
       " 6795,\n",
       " 8565,\n",
       " 15]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids  # 정수 토큰(id/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7390"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"프\")  # word ->id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'프'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(7390)  # id->word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.offsets[0]  # 지정한 위치의(index) 토큰이 text의 어디에 있는지 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7390, '프', '프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids[0], output.tokens[0], sports_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['멋', '진', '식', '사를', '즐', '기', '기에', '좋은', '장', '소', '-', '채', '식', '메', '뉴', '가', '정말', '훌', '륭', '했습니다', '.', '당', '근', '케', '이', '크', '는', '아', '마', '도', '내가', '먹', '어', '본', '디', '저', '트', '중', '최고', '였', '을', '거', '예', '요', '.']\n"
     ]
    }
   ],
   "source": [
    "## comment_txt\n",
    "# 토큰화\n",
    "output2 = tokenizer.encode(comment_txt)\n",
    "# word 토큰, id 토큰 조회\n",
    "print(output2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5199, 6640, 5909, 8338, 6621, 4153, 8419, 8531, 6442, 5799, 14, 6811, 5909, 5206, 4513, 3902, 8343, 7589, 5093, 8365, 15, 4582, 4130, 7007, 6396, 7091, 4521, 6070, 5140, 4660, 8993, 5189, 6140, 5460, 4778, 6472, 7245, 6587, 9568, 6196, 6364, 3956, 6204, 6267, 15]\n"
     ]
    }
   ],
   "source": [
    "print(output2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '글을', '쓴', '이유는', '다', '름', '아닌', \"'\", '전', '안', '법', \"'\", '시행', '반대', '를', '주장', '하기', '위해서', '입니다', '.', '먼저', ',', \"'\", '전', '안', '법', \"'\", '은', '전기', '용', '품', '및', '생활', '용', '품', '을', '판매', '하는', '업체', '에서', 'K', 'C', '인', '증', '마', '크', '를', '의무', '적으로', '받는', '것입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# petition_txt\n",
    "output3 = tokenizer.encode(petition_txt)\n",
    "print(output3.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6396,\n",
       " 8577,\n",
       " 6044,\n",
       " 9246,\n",
       " 4563,\n",
       " 5107,\n",
       " 8323,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 8656,\n",
       " 8658,\n",
       " 5101,\n",
       " 8953,\n",
       " 8301,\n",
       " 8580,\n",
       " 8212,\n",
       " 15,\n",
       " 8855,\n",
       " 13,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 6360,\n",
       " 9353,\n",
       " 6278,\n",
       " 7375,\n",
       " 5345,\n",
       " 8437,\n",
       " 6278,\n",
       " 7375,\n",
       " 6364,\n",
       " 9118,\n",
       " 8208,\n",
       " 8692,\n",
       " 8210,\n",
       " 44,\n",
       " 36,\n",
       " 6400,\n",
       " 6625,\n",
       " 5140,\n",
       " 7091,\n",
       " 5101,\n",
       " 8521,\n",
       " 8253,\n",
       " 8546,\n",
       " 8299,\n",
       " 15]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5199,\n",
       " 6640,\n",
       " 5909,\n",
       " 8338,\n",
       " 6621,\n",
       " 4153,\n",
       " 8419,\n",
       " 8531,\n",
       " 6442,\n",
       " 5799,\n",
       " 14,\n",
       " 6811,\n",
       " 5909,\n",
       " 5206,\n",
       " 4513,\n",
       " 3902,\n",
       " 8343,\n",
       " 7589,\n",
       " 5093,\n",
       " 8365,\n",
       " 15,\n",
       " 4582,\n",
       " 4130,\n",
       " 7007,\n",
       " 6396,\n",
       " 7091,\n",
       " 4521,\n",
       " 6070,\n",
       " 5140,\n",
       " 4660,\n",
       " 8993,\n",
       " 5189,\n",
       " 6140,\n",
       " 5460,\n",
       " 4778,\n",
       " 6472,\n",
       " 7245,\n",
       " 6587,\n",
       " 9568,\n",
       " 6196,\n",
       " 6364,\n",
       " 3956,\n",
       " 6204,\n",
       " 6267,\n",
       " 15]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다 름 아닌 ' 전 안 법 ' 시행 반대 를 주장 하기 위해서 입니다 . 먼저 , ' 전 안 법 ' 은 전기 용 품 및 생활 용 품 을 판매 하는 업체 에서 K C 인 증 마 크 를 의무 적으로 받는 것입니다 .\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id 토큰 리스트 문장으로  원복\n",
    "tokenizer.decode(output3.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "\n",
    "## WordPiece 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린시간: 31.19713068008423초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece  # , BPR\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer     # BpeTrainer\n",
    "\n",
    "\n",
    "# \n",
    "tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=10000, \n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SEP]\"])  # [SEP] : 문장을 나누는 구분자.\n",
    "    # , continuing_subword_prefix=\"##\"\n",
    "\n",
    "# 학습\n",
    "s = time.time()\n",
    "tokenizer.train([\"data/petitions_corpus.txt\"], trainer=trainer)\n",
    "e = time.time()\n",
    "print(f\"학습에 걸린시간: {e-s}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 저장\n",
    "\n",
    "tokenizer.save(\"saved_models/tokenizers/petitions_wordpiece.json\")\n",
    "\n",
    "# Tokenizer.from_file('경로')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이',\n",
       " '글',\n",
       " '##을',\n",
       " '쓴',\n",
       " '이',\n",
       " '##유',\n",
       " '##는',\n",
       " '다',\n",
       " '##름',\n",
       " '##아',\n",
       " '##닌',\n",
       " \"'\",\n",
       " '전',\n",
       " '##안',\n",
       " '##법',\n",
       " \"'\",\n",
       " '시',\n",
       " '##행',\n",
       " '반',\n",
       " '##대',\n",
       " '##를',\n",
       " '주',\n",
       " '##장',\n",
       " '##하',\n",
       " '##기',\n",
       " '위',\n",
       " '##해',\n",
       " '##서',\n",
       " '##입',\n",
       " '##니',\n",
       " '##다',\n",
       " '.',\n",
       " '먼',\n",
       " '##저',\n",
       " ',',\n",
       " \"'\",\n",
       " '전',\n",
       " '##안',\n",
       " '##법',\n",
       " \"'\",\n",
       " '은',\n",
       " '전',\n",
       " '##기',\n",
       " '##용',\n",
       " '##품',\n",
       " '및',\n",
       " '생',\n",
       " '##활',\n",
       " '##용',\n",
       " '##품',\n",
       " '##을',\n",
       " '판',\n",
       " '##매',\n",
       " '##하',\n",
       " '##는',\n",
       " '업',\n",
       " '##체',\n",
       " '##에',\n",
       " '##서',\n",
       " 'K',\n",
       " '##C',\n",
       " '##인',\n",
       " '##증',\n",
       " '##마',\n",
       " '##크',\n",
       " '##를',\n",
       " '의',\n",
       " '##무',\n",
       " '##적',\n",
       " '##으',\n",
       " '##로',\n",
       " '받',\n",
       " '##는',\n",
       " '것',\n",
       " '##입',\n",
       " '##니',\n",
       " '##다',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(petition_txt)\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글 ##을 쓴 이 ##유 ##는 다 ##름 ##아 ##닌 ' 전 ##안 ##법 ' 시 ##행 반 ##대 ##를 주 ##장 ##하 ##기 위 ##해 ##서 ##입 ##니 ##다 . 먼 ##저 , ' 전 ##안 ##법 ' 은 전 ##기 ##용 ##품 및 생 ##활 ##용 ##품 ##을 판 ##매 ##하 ##는 업 ##체 ##에 ##서 K ##C ##인 ##증 ##마 ##크 ##를 의 ##무 ##적 ##으 ##로 받 ##는 것 ##입 ##니 ##다 .\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린시간: 212.86667370796204초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram # WordPiece  # , BPR\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import UnigramTrainer # WordPieceTrainer     # BpeTrainer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(Unigram())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = UnigramTrainer(\n",
    "    vocab_size=10000, \n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SEP]\"])  # [SEP] : 문장을 나누는 구분자.\n",
    "    # , continuing_subword_prefix=\"##\"\n",
    "\n",
    "# 학습\n",
    "s = time.time()\n",
    "tokenizer.train([\"data/petitions_corpus.txt\"], trainer=trainer)\n",
    "e = time.time()\n",
    "print(f\"학습에 걸린시간: {e-s}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"saved_models/tokenizers/petitions_unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '리',\n",
       " '미',\n",
       " '어',\n",
       " '리',\n",
       " '그',\n",
       " '역',\n",
       " '대',\n",
       " '개인',\n",
       " '최',\n",
       " '다',\n",
       " '골',\n",
       " '기록',\n",
       " '을',\n",
       " '보유',\n",
       " '하고',\n",
       " '있는',\n",
       " '시',\n",
       " '어',\n",
       " '러',\n",
       " '가',\n",
       " '손',\n",
       " '흥',\n",
       " '민',\n",
       " '의',\n",
       " '골',\n",
       " '결정',\n",
       " '력',\n",
       " '을',\n",
       " '재',\n",
       " '차',\n",
       " '극',\n",
       " '찬',\n",
       " '했다',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(sports_txt)\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[753,\n",
       " 58,\n",
       " 111,\n",
       " 42,\n",
       " 58,\n",
       " 32,\n",
       " 273,\n",
       " 36,\n",
       " 453,\n",
       " 740,\n",
       " 25,\n",
       " 980,\n",
       " 1567,\n",
       " 5,\n",
       " 1608,\n",
       " 19,\n",
       " 45,\n",
       " 38,\n",
       " 42,\n",
       " 252,\n",
       " 9,\n",
       " 443,\n",
       " 1902,\n",
       " 318,\n",
       " 7,\n",
       " 980,\n",
       " 687,\n",
       " 322,\n",
       " 5,\n",
       " 95,\n",
       " 101,\n",
       " 607,\n",
       " 1629,\n",
       " 458,\n",
       " 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
