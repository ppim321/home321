{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시험시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 각각 다른 행렬로 구성한다.\n",
    "- 하나의 데이터 포인트의 입력/출력 값은 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 준다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "      \n",
    "\n",
    "\\begin{align}\n",
    "&\\large \\hat{y} = W\\cdot X + b\\\\\n",
    "&\\small \\hat{y}: \\text{모델추정값}\\\\\n",
    "&\\small W: \\text{가중치}\\\\\n",
    "&\\small X: \\text{Feature(입력값)}\\\\\n",
    "&\\small b: \\text{bias(편향)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset 구성\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `공부시간` 1개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다.   \n",
    "  이 문제에서 예측할 항목은 `시험점수` 한개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (3, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "study_hours = [[1], [2], [3]] # 하나의 값으로 구성된 데이터를 2차원으로 구성되어 묶어서 표현함.\n",
    "scores = [[20],[40],[60]] \n",
    "\n",
    "np.shape(study_hours), np.shape(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 정의\n",
    "import torch\n",
    "X_train = torch.tensor(study_hours, dtype=torch.float32)\n",
    "y_train = torch.tensor(scores, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n",
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.dtype, y_train.dtype)\n",
    "X_train.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 (weight, bias) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([1])\n",
      "tensor([[-0.1758]], requires_grad=True) tensor([0.4454], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(0), 시드값 \n",
    "# y_pred = X_train * weight = bias\n",
    "weight = torch.randn(1, 1, requires_grad=True)  # shape: (1:X의 feature개수, 1:출력값(예측)의 개수.)\n",
    "#           정규 분포 안에서 random 한 값으로 잡은 것.\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(weight.shape, bias.shape)\n",
    "print(weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2696],\n",
       "        [ 0.0939],\n",
       "        [-0.0819]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론 (w * x + b) , 행렬 곱\n",
    "pred = X_train @ weight + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.],\n",
       "        [40.],\n",
       "        [60.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train  # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-19.7304],\n",
       "        [-39.9061],\n",
       "        [-60.0819]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred - y_train  # 뺀 값은 0 이 되어야 좋은 값임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 389.2876],\n",
       "        [1592.4994],\n",
       "        [3609.8330]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차 계산 - MSE\n",
    "loss = (y_train - pred) ** 2\n",
    "loss\n",
    "\n",
    "# loss 를 합하여 하나의 값으로 업데이트 처리해야함.  오차의 평균값을 확인하여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1592.4994, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차의 평균값 계산\n",
    "loss = torch.mean(y_train - pred) ** 2\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight, bias의 gradient계산\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-159.6245]]), tensor([-79.8123]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad, bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight 변경 (update)\n",
    "## new_weight = weight - weight.grad * learning_rate \n",
    "# ** learning_rate - hyper parameter\n",
    "lr = 0.01\n",
    "new_weight = weight.data - weight.grad\n",
    "new_bias = bias.data - bias.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[159.4488]]) tensor([[-0.1758]])\n",
      "tensor([1.2435]) tensor([0.4454])\n"
     ]
    }
   ],
   "source": [
    "print(new_weight.data, weight.data)\n",
    "print(new_bias.data, bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = X_train @ new_weight + new_bias\n",
    "loss2 = torch.mean((y_train - pred2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 loss: tensor(1592.4994, grad_fn=<PowBackward0>)\n",
      "update후 loss: tensor(91442.9766)\n"
     ]
    }
   ],
   "source": [
    "print(\"이전 loss:\", loss)\n",
    "print(\"update후 loss:\", loss2)  # gradient 가 0이 될때까지 계속 반복함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(1, 1, requires_grad=True)  \n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추론하는 함수 (예측 모델)\n",
    "def linear_model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "# 오차계산 함수 (MSE)\n",
    "def loss_fn(pred, y):\n",
    "    return torch.mean((pred-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다.\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] loss: 2070.001221\n",
      "[101/1000] loss: 228.587204\n",
      "[201/1000] loss: 30.485535\n",
      "[301/1000] loss: 8.926572\n",
      "[401/1000] loss: 6.345324\n",
      "[501/1000] loss: 5.815186\n",
      "[601/1000] loss: 5.517218\n",
      "[701/1000] loss: 5.255522\n",
      "[801/1000] loss: 5.008505\n",
      "[901/1000] loss: 4.773336\n",
      "[1000/1000] loss: 4.551437\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000 # 반복 횟수\n",
    "lr = 0.001\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # 1. 추정\n",
    "    pred = linear_model(X_train)\n",
    "\n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y_train)\n",
    "\n",
    "    # 3. parameter 들의 gradient를 계산(loss에 대한 변화)\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. parameter update\n",
    "    weight.data = weight.data - weight.grad * lr    # .data:현재값, .grad: gradient 계산값.\n",
    "    bias.data = bias.data - bias.grad * lr\n",
    "\n",
    "    # 5. gradient 초기화 (반복됨으로 초기화하는 업데이트를 해야함)\n",
    "    weight.grad = None\n",
    "    bias.grad = None\n",
    "\n",
    "    ## 로그 - loss를 출력\n",
    "    if epoch % 100 == 0 or epoch == (epochs-1):\n",
    "        print(f\"[{epoch+1}/{epochs}] loss: {loss.item():05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[17.5225]]), tensor([5.6312]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data, bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[93.2438],\n",
       "        [75.7213],\n",
       "        [40.6763]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = torch.tensor([[5], [4], [2]], dtype=torch.float32)\n",
    "linear_model(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "\n",
    "2차원 행렬값 (5, 3)  --->   (5, 2)\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 **사과**와, **오렌지 수확량**에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - `개별 과일를 예측하기 위한 weight들 @ feature들` 의 계산 결과를  **Node, Unit, Neuron** 이라고 한다.\n",
    "        - 두 과일에 대한 Unit들을 묶어서 **Layer** 라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias 들**을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `온도, 강수량, 습도` 세개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다. 이 문제에서 예측할 항목은 `사과수확량, 오렌지 수확량` 2개의 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input: 생산환경 (temp, rainfall, humidity) : (5, 3)\n",
    "environs = [\n",
    "    [73, 67, 43], \n",
    "    [91, 88, 64], \n",
    "    [87, 134, 58], \n",
    "    [102, 43, 37], \n",
    "    [69, 96, 70]\n",
    "]\n",
    "\n",
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "apple_orange_output = [\n",
    "    [56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 2]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset을 torch.Tensor로 생성.\n",
    "X = torch.tensor(environs, dtype=torch.float32)\n",
    "y = torch.tensor(apple_orange_output, dtype=torch.float32)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight와 bias\n",
    "- weight: 각 feature들이 생산량에 영향을 주었는지의 가중치로 feature에 곱해줄 값.\n",
    "    - 사과, 오렌지의 생산량을 구해야 하므로 가중치가 두개가 된다.\n",
    "    - weight의 shape: `(3, 2)`   가중치 =2, feature = 3\n",
    "- bias는 모든 feature들이 0일때 생산량이 얼마일지를 나타내는 값으로 feature와 weight간의 가중합 결과에 더해줄 값이다.\n",
    "    - 사과, 오렌지의 생산량을 구하므로 bias가 두개가 된다.\n",
    "    - bias의 shape: `(2, )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 함수.\n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<center style=\"font-size:0.9em\">\n",
    "$w_{11},\\,w_{12},\\,w_{13}$: 사과 생산량 계산시 각 feature들(생산환경)에 곱할 가중치   <br>\n",
    "$w_{21},\\,w_{22},\\,w_{23}$: 오렌지 생산량 계산시 각 feature들(생산환경)에 곱할 가중치    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/3_unit_layer.png\">\n",
    "</center>\n",
    "# feature, parameter, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight/bias 를 정의 -> 초기값은 random 값을 이용해서 생성.\n",
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "weight.size(), bias.size()\n",
    "# weight : (3:input feature개수,  2: output  개수)\n",
    "# bias :  (2:output 개수, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3812,  0.9509],\n",
       "        [-0.2441,  0.2801],\n",
       "        [ 0.1050, -0.9635]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8184, -0.0584], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X @ parametet + bias = 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 89.8099,  46.6883],\n",
       "        [111.7509,  49.4513],\n",
       "        [ 94.3693,  64.3122],\n",
       "        [135.0937,  73.3231],\n",
       "        [ 80.0407,  24.9915]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한번 학습(parameter 최적화)  # 예측값\n",
    "##추론\n",
    "pred = X @ weight + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # 원래의 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1143.1067,   543.4376],\n",
       "        [  945.6174,  2657.2678],\n",
       "        [  606.6702,  4718.0093],\n",
       "        [12790.1895,  1319.3663],\n",
       "        [  527.1276,  8837.6064]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loss 계산(MSE)\n",
    "loss = (pred - y) ** 2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3408.8398, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loss 계산(MSE)\n",
    "loss = torch.mean((pred - y) ** 2) # 전체 추론한 결과의 평균오차를 계산.\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 (5개의 data를 예측해서 loss 를 줄이는? 평균 오차)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 를 가지고 parameter(weight들, bias들)의 gradient 계산.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3812,  0.9509],\n",
       "         [-0.2441,  0.2801],\n",
       "         [ 0.1050, -0.9635]]),\n",
       " tensor([[ 2614.9905, -3030.0322],\n",
       "         [  865.9539, -4553.0527],\n",
       "         [  914.1242, -2704.4114]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data, weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.8184, -0.0584]), tensor([ 26.0129, -40.2467]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.data, bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter update  ( 1번의 업데이트만 됨)\n",
    "lr = 0.00001\n",
    "weight.data = weight.data - lr * weight.grad\n",
    "bias.data = bias.data - lr * bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 업데이트된 파라미터로 추정 -> loss 계산\n",
    "pred2 = X @ weight + bias\n",
    "loss2 = torch.mean((pred2 - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408.83984375 2990.943603515625\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss2.item())  # 값이 0 에 가까워질수록 양쪽 값에 오차가 없다는 뜻.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "## 모델 정의\n",
    "def model(X):\n",
    "    return X @ weight + bias\n",
    "\n",
    "## loss 함수(MSE)\n",
    "def loss_fn(pred, y):\n",
    "    return torch.mean((pred - y)**2)  # 전체 오차의 평균."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch : 전체 데이터를 한번 학습하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5000] - 7299.95605\n",
      "[101/5000] - 36.23597\n",
      "[201/5000] - 15.27038\n",
      "[301/5000] - 8.57965\n",
      "[401/5000] - 6.03221\n",
      "[501/5000] - 4.76155\n",
      "[601/5000] - 3.94388\n",
      "[701/5000] - 3.33188\n",
      "[801/5000] - 2.84275\n",
      "[901/5000] - 2.44218\n",
      "[1001/5000] - 2.11133\n",
      "[1101/5000] - 1.83726\n",
      "[1201/5000] - 1.61000\n",
      "[1301/5000] - 1.42149\n",
      "[1401/5000] - 1.26511\n",
      "[1501/5000] - 1.13537\n",
      "[1601/5000] - 1.02774\n",
      "[1701/5000] - 0.93844\n",
      "[1801/5000] - 0.86437\n",
      "[1901/5000] - 0.80291\n",
      "[2001/5000] - 0.75192\n",
      "[2101/5000] - 0.70962\n",
      "[2201/5000] - 0.67453\n",
      "[2301/5000] - 0.64541\n",
      "[2401/5000] - 0.62126\n",
      "[2501/5000] - 0.60122\n",
      "[2601/5000] - 0.58459\n",
      "[2701/5000] - 0.57080\n",
      "[2801/5000] - 0.55935\n",
      "[2901/5000] - 0.54986\n",
      "[3001/5000] - 0.54199\n",
      "[3101/5000] - 0.53545\n",
      "[3201/5000] - 0.53004\n",
      "[3301/5000] - 0.52554\n",
      "[3401/5000] - 0.52180\n",
      "[3501/5000] - 0.51871\n",
      "[3601/5000] - 0.51614\n",
      "[3701/5000] - 0.51401\n",
      "[3801/5000] - 0.51225\n",
      "[3901/5000] - 0.51078\n",
      "[4001/5000] - 0.50956\n",
      "[4101/5000] - 0.50855\n",
      "[4201/5000] - 0.50771\n",
      "[4301/5000] - 0.50702\n",
      "[4401/5000] - 0.50644\n",
      "[4501/5000] - 0.50596\n",
      "[4601/5000] - 0.50557\n",
      "[4701/5000] - 0.50523\n",
      "[4801/5000] - 0.50496\n",
      "[4901/5000] - 0.50473\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000  # 전체를 한번 학습하는건\n",
    "lr = 0.00001  # 1e-5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 1. 추론\n",
    "    pred = model(X)\n",
    "\n",
    "    # 2. loss 계산  (5개의 데이터에 대한 예측 결과와 - 사과, 오렌지에 대한 예측 결과.)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # 3. 파라미터 들의 gradient 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. 파라이터 업데이트\n",
    "    weight.data = weight.data - lr * weight.grad\n",
    "    bias.data = bias.data - lr * bias.grad \n",
    "\n",
    "    # 5. gradient 초기화\n",
    "    weight.grad = None\n",
    "    bias.grad = None\n",
    "\n",
    "\n",
    "    ## 100 epoch, 마지막 epoch에서 loss를 출력 => 학습 과정 log 출력\n",
    "    if epoch % 100 == 0 or epoch == (epoch-1):\n",
    "        print(f\"[{epoch+1}/{epochs}] - {loss.item():.5f}\") \n",
    "        # 몇번째 epoch 를 돌리고 있는지 앞에 나타내줌. 예) 100/5000 5천번중에 100번째에폭.,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0003\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터로 추론\n",
    "print(f\"{3:04d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2016,  70.1415],\n",
       "        [ 82.1945, 100.8015],\n",
       "        [118.6634, 132.9260],\n",
       "        [ 21.0702,  37.0194],\n",
       "        [101.9499, 119.1705]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model(X)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = torch.tensor([[68, 82, 56]], dtype=torch.float32)\n",
    "new_X = new_X.unsqueeze(dim=0)  # dummy 축 늘림(dim=축번호)\n",
    "new_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[80.8626, 95.4789]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[73, 67, 43], \n",
    "     [91, 88, 64], \n",
    "     [87, 134, 58], \n",
    "     [102, 43, 37], \n",
    "     [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(\n",
    "    [[56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.nn.Linear(input feature개수, output개수)  # weight, bias, X@weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다.\n",
    "- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 모델을 정의. torch.nn.Linear 클래스\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(3, 2)   # 3: input feature 개수,  2: output 개수\n",
    "# def model(X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 함수\n",
    "loss_fn = torch.nn.functional.mse_loss   # 함수\n",
    "# loss_fn = torch.nn.MSELoss()   # 클래스 -> 객체를 생성해야함.\n",
    "# def loss_fn(pred, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer (torch.optim 모듈에 정의):   weight.data = weight.data - lr * weight.grad\n",
    "optimizer = torch.optim.SGD(            # Stochastic Gradient Descent (SGD)\n",
    "    model.parameters(),                 # 최적화 대상 파라미터들을 model에서 조회해서 전달.\n",
    "    lr = 0.00001,             # learning Rate\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5578, -0.0957,  0.1109],\n",
       "         [ 0.3675,  0.0442, -0.2038]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1041,  0.5252], requires_grad=True)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/5000] - 11454.5947265625\n",
      "[0101/5000] - 162.26736450195312\n",
      "[0201/5000] - 63.12005615234375\n",
      "[0301/5000] - 32.36638259887695\n",
      "[0401/5000] - 21.304096221923828\n",
      "[0501/5000] - 16.18185043334961\n",
      "[0601/5000] - 13.070210456848145\n",
      "[0701/5000] - 10.80805778503418\n",
      "[0801/5000] - 9.02091121673584\n",
      "[0901/5000] - 7.563387393951416\n",
      "[1001/5000] - 6.3612589836120605\n",
      "[1101/5000] - 5.365915298461914\n",
      "[1201/5000] - 4.540711402893066\n",
      "[1301/5000] - 3.8562519550323486\n",
      "[1401/5000] - 3.2884421348571777\n",
      "[1501/5000] - 2.8174004554748535\n",
      "[1601/5000] - 2.4266180992126465\n",
      "[1701/5000] - 2.102412462234497\n",
      "[1801/5000] - 1.8334251642227173\n",
      "[1901/5000] - 1.6102832555770874\n",
      "[2001/5000] - 1.4251471757888794\n",
      "[2101/5000] - 1.2715672254562378\n",
      "[2201/5000] - 1.1441470384597778\n",
      "[2301/5000] - 1.0384310483932495\n",
      "[2401/5000] - 0.9507253766059875\n",
      "[2501/5000] - 0.8779692649841309\n",
      "[2601/5000] - 0.8176106214523315\n",
      "[2701/5000] - 0.7675238847732544\n",
      "[2801/5000] - 0.7259787917137146\n",
      "[2901/5000] - 0.6915132403373718\n",
      "[3001/5000] - 0.6629174947738647\n",
      "[3101/5000] - 0.6391922235488892\n",
      "[3201/5000] - 0.6195104122161865\n",
      "[3301/5000] - 0.603183388710022\n",
      "[3401/5000] - 0.5896340608596802\n",
      "[3501/5000] - 0.5783973932266235\n",
      "[3601/5000] - 0.5690740346908569\n",
      "[3701/5000] - 0.5613347291946411\n",
      "[3801/5000] - 0.5549190044403076\n",
      "[3901/5000] - 0.5495961308479309\n",
      "[4001/5000] - 0.5451733469963074\n",
      "[4101/5000] - 0.5415146946907043\n",
      "[4201/5000] - 0.538470983505249\n",
      "[4301/5000] - 0.5359489321708679\n",
      "[4401/5000] - 0.5338534116744995\n",
      "[4501/5000] - 0.532119631767273\n",
      "[4601/5000] - 0.530678927898407\n",
      "[4701/5000] - 0.5294830799102783\n",
      "[4801/5000] - 0.5284916162490845\n",
      "[4901/5000] - 0.5276647210121155\n",
      "[5000/5000] - 0.526989221572876\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 추론\n",
    "    pred = model(inputs)\n",
    "\n",
    "    # loss 계산\n",
    "    loss = loss_fn(pred, targets)  # torch.nn.functional.mse_loss(pred, targets)   #(모델추정값, 정답) => 파라미터 작성순서.\n",
    "\n",
    "    # gradient 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 업데이트 : optimizer.step()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 파라미터 초기화     w.grad=None, b.grad=None\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ##############################################여기까지 한번의 학습이 끝남.\n",
    "\n",
    "    # 현재 eopch 학습 결과를 log로 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "         print(f\"[{epoch+1:04d}/{epochs}] - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 => Gradient 계산을 할 필요가 없다. => grad_fn 을 만들 필요가 없다.\n",
    "with torch.no_grad(): # => grad_fn 을 만들지 않음.  단, 학습에서는 사용해야함.\n",
    "    pred = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.1495,  70.4038],\n",
       "        [ 82.2079, 100.5584],\n",
       "        [118.7156, 133.0619],\n",
       "        [ 21.0892,  37.0372],\n",
       "        [101.8979, 119.0343]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 로직을 함수로 구현\n",
    "def train(inputs, targets, epochs, modal, loss_fn, optimizer):\n",
    "     \n",
    "     for epoch in range(epochs):\n",
    "        epochs = 5000\n",
    "        \n",
    "        # 추론\n",
    "        pred = model(inputs)\n",
    "\n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred, targets)  # torch.nn.functional.mse_loss(pred, targets)   #(모델추정값, 정답) => 파라미터 작성순서.\n",
    "\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 파라미터 업데이트 : optimizer.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 파라미터 초기화     w.grad=None, b.grad=None\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ##############################################여기까지 한번의 학습이 끝남.\n",
    "\n",
    "        # 현재 eopch 학습 결과를 log로 출력\n",
    "        if epoch % 100 == 0 or epoch == epochs-1:\n",
    "            print(f\"[{epoch+1:04d}/{epochs}] - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/5000] - 12045.296875\n",
      "[0101/5000] - 7.3861517906188965\n",
      "[0201/5000] - 1.5651946067810059\n",
      "[0301/5000] - 0.673272967338562\n",
      "[0401/5000] - 0.5365880727767944\n",
      "[0501/5000] - 0.5156328678131104\n",
      "[0601/5000] - 0.5124197006225586\n",
      "[0701/5000] - 0.5119235515594482\n",
      "[0801/5000] - 0.5118404030799866\n",
      "[0901/5000] - 0.5118255615234375\n",
      "[1001/5000] - 0.5118188261985779\n",
      "[1101/5000] - 0.5118154287338257\n",
      "[1201/5000] - 0.5118108987808228\n",
      "[1301/5000] - 0.5118058323860168\n",
      "[1401/5000] - 0.511800229549408\n",
      "[1501/5000] - 0.5117964148521423\n",
      "[1601/5000] - 0.5117897987365723\n",
      "[1701/5000] - 0.5117881298065186\n",
      "[1801/5000] - 0.511783242225647\n",
      "[1901/5000] - 0.5117788314819336\n",
      "[2001/5000] - 0.5117694139480591\n",
      "[2101/5000] - 0.5117672681808472\n",
      "[2201/5000] - 0.5117610692977905\n",
      "[2301/5000] - 0.5117601752281189\n",
      "[2401/5000] - 0.5117507576942444\n",
      "[2501/5000] - 0.511749267578125\n",
      "[2601/5000] - 0.5117443799972534\n",
      "[2701/5000] - 0.5117385983467102\n",
      "[2801/5000] - 0.5117336511611938\n",
      "[2901/5000] - 0.5117284655570984\n",
      "[3001/5000] - 0.5117241144180298\n",
      "[3101/5000] - 0.5117214918136597\n",
      "[3201/5000] - 0.5117148756980896\n",
      "[3301/5000] - 0.5117116570472717\n",
      "[3401/5000] - 0.5117093920707703\n",
      "[3501/5000] - 0.5117031335830688\n",
      "[3601/5000] - 0.5116971731185913\n",
      "[3701/5000] - 0.5116941332817078\n",
      "[3801/5000] - 0.5116869807243347\n",
      "[3901/5000] - 0.5116828680038452\n",
      "[4001/5000] - 0.5116795897483826\n",
      "[4101/5000] - 0.511675238609314\n",
      "[4201/5000] - 0.5116717219352722\n",
      "[4301/5000] - 0.5116641521453857\n",
      "[4401/5000] - 0.5116595029830933\n",
      "[4501/5000] - 0.5116540193557739\n",
      "[4601/5000] - 0.5116488933563232\n",
      "[4701/5000] - 0.5116462707519531\n",
      "[4801/5000] - 0.5116413831710815\n",
      "[4901/5000] - 0.5116358995437622\n",
      "[5000/5000] - 0.511631190776825\n"
     ]
    }
   ],
   "source": [
    "train(inputs, targets, 5000, model , nn.functional.mse_loss, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
