{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "-   학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다.\n",
    "-   파이토치는 **모델의 파라미터만 저장**하는 방법과 **모델 구조와 파라미터 모두를 저장**하는 두가지 방식을 제공한다.\n",
    "-   저장 함수\n",
    "    -   `torch.save(저장할 객체, 저장경로)`\n",
    "-   보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "-   저장하기\n",
    "    -   `torch.save(model, 저장경로)`\n",
    "-   불러오기\n",
    "    -   `load_model = torch.load(저장경로)`\n",
    "-   저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 용량이 크다는 점이 단점이지만, 한꺼번에 저장해서 불러오는 이점이 있음.   .pt    .pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)----> model의 parameter들만 모아놓은 형태\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint를 저장 및 불러오기\n",
    "\n",
    "-   학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "-   Dictionary에 저장하려는 값들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "# loading된 checkpoint 값 이용해 이전 학습상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear의 파라미터(weight, bias) 개수 : 입력 feature * 출력 feature + 출력 feature\n",
    "# (784, 128)  784 * 128 + 128 \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3, 4)  #  parameter 의 갯수 =  3 X 4 + 4\n",
    "        self.lr2 = nn.Linear(4, 2)\n",
    "        self.relu = nn.ReLU()  # activation 함수 -> parameter 가 없든 단순 계산함수 . relu(X) = max(X, 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 저장\n",
    "torch.save(model, \"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_14324\\1725091380.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_model = torch.load(\"saved_models/my_model.pt\")\n"
     ]
    }
   ],
   "source": [
    "## 저장된 모델 load\n",
    "load_model = torch.load(\"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 Layer들을 조회.  모델.instance변수명\n",
    "lr_layer = model.lr1\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer 에서 weight / bias  조회\n",
    "lr1_weight = lr_layer.weight\n",
    "lr1_bias = lr_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4765,  0.3049,  0.2840],\n",
       "        [-0.4651, -0.0372,  0.4326],\n",
       "        [-0.5045, -0.4025,  0.0210],\n",
       "        [-0.4264,  0.0053, -0.2605]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight   #  3 X 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3158, -0.5766, -0.4767,  0.2740], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4765,  0.3049,  0.2840],\n",
       "        [-0.4651, -0.0372,  0.4326],\n",
       "        [-0.5045, -0.4025,  0.0210],\n",
       "        [-0.4264,  0.0053, -0.2605]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3158, -0.5766, -0.4767,  0.2740], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.4765,  0.3049,  0.2840],\n",
       "                      [-0.4651, -0.0372,  0.4326],\n",
       "                      [-0.5045, -0.4025,  0.0210],\n",
       "                      [-0.4264,  0.0053, -0.2605]])),\n",
       "             ('lr1.bias', tensor([ 0.3158, -0.5766, -0.4767,  0.2740])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[ 0.2030,  0.4679,  0.4916,  0.4180],\n",
       "                      [-0.2838,  0.4287, -0.0552,  0.4987]])),\n",
       "             ('lr2.bias', tensor([-0.1722, -0.1312]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### 파라미터들(weight들, bias들)만 저장/불러오기\n",
    "# state_dict\n",
    "model.state_dict()  # 타입: collections, OrderdeDict  - 실제의 구조는 tuple  [(key, value), (key, value), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory saved/models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 저장\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved/models/my_model_parameter.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory saved/models does not exist."
     ]
    }
   ],
   "source": [
    "# 저장\n",
    "torch.save(state_dict, \"saved/models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(\"saved/models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 객체를 생성 -> load 한 state_dict 를 모델 파라미터에 덮어쓴다.\n",
    "new_model = MyModel()\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# putorch model 구조를 조사해주는 패키지.\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)\n",
    "# 갖고 있는 객체들을 반환.\n",
    "\n",
    "# Non-trainable params: 0 --> 업데이트 되지 않은 파라미터의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (100, 3))   # (model, (niput data의 shape)  (100:batch_size, 3:feature수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅈ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다. \n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regression(회귀)\n",
    "\n",
    "### Boston Housing Dataset\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dataset 준비\n",
    "## 데이터 불러오기 -> 전처리 -> Dataset -> DataLoader\n",
    "df = pd.read_csv(\"data/boston_hosing.csv\")\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## X, y 분리\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_boston \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m y_boston \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m y_boston \u001b[38;5;241m=\u001b[39m y_boston\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## X, y 분리\n",
    "X_boston = df.drop(columns=\"MEDV\").values\n",
    "y_boston = df['MEDV'].values\n",
    "y_boston = y_boston.reshape(-1, 1)\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 -> 선형회귀 기반: 전처리 - 연속형 : feature scaling,  범주형 : OneHotEncoding \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 102)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=404, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=102)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### 모델 정의\n",
    "# nn.Module 상속.\n",
    "# __init__(): layer 객체들 초기화, \n",
    "# forward(): 추론 계산 과정을 정의.\n",
    "class BostonHousingModeling(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # nn.Module을 초기화 하는 코드\n",
    "        self.lr1 = nn.Linear(13, 16)   # 13 개의 columns,  16 개의 features\n",
    "        self.lr2 = nn.Linear(16, 8)    # table 데이터의 feature수가 적으므로 늘렸다가 줄여가는 방법으로 비선형성을 줌.\n",
    "        self.lr3 = nn.Linear(8, 1)     # 최종 출력결과(집 값 중위수 1개를 return).\n",
    "        self.relu = nn.ReLU()          # activation 함수.\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        output = self.lr3(X)   \n",
    "        return output        # pred 입력값이 None 이됨.\n",
    "    \n",
    "        # 출력 Layer(output)은  activatoin 함수를 통과시키지 않음.\n",
    "        # 단, 값의 범위가 정해져있고  그 범위의 값을 반환하는 함수가 있을 경우에는 함수를 사용할 수 있다.\n",
    "        ## 출력값 범위:  0 ~ 1 의 실수 - logistic(sigmoid) 함수,  -1 ~ 1 사이의 실수 : tanh 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tochinfo - summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonHousingModeling                    [404, 1]                  --\n",
       "├─Linear: 1-1                            [404, 16]                 224\n",
       "├─ReLU: 1-2                              [404, 16]                 --\n",
       "├─Linear: 1-3                            [404, 8]                  136\n",
       "├─ReLU: 1-4                              [404, 8]                  --\n",
       "├─Linear: 1-5                            [404, 1]                  9\n",
       "==========================================================================================\n",
       "Total params: 369\n",
       "Trainable params: 369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### 모델 객체 생성\n",
    "boston_model = BostonHousingModeling().to(device)\n",
    "summary(boston_model, (404, 13), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 학습 : loss_fn(회귀 : nn.MSELoss, nn.functional.mse_loss), optimizer\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(boston_model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()  # 회귀: Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] - train loss: 13.19188404083252, valid loss: 28.544967651367188\n",
      "[0002/1000] - train loss: 13.175529479980469, valid loss: 28.522233963012695\n",
      "[0003/1000] - train loss: 13.159161567687988, valid loss: 28.499181747436523\n",
      "[0004/1000] - train loss: 13.142736434936523, valid loss: 28.476415634155273\n",
      "[0005/1000] - train loss: 13.126338958740234, valid loss: 28.453901290893555\n",
      "[0006/1000] - train loss: 13.10994815826416, valid loss: 28.4317626953125\n",
      "[0007/1000] - train loss: 13.093592643737793, valid loss: 28.409652709960938\n",
      "[0008/1000] - train loss: 13.077251434326172, valid loss: 28.38692283630371\n",
      "[0009/1000] - train loss: 13.060922622680664, valid loss: 28.364574432373047\n",
      "[0010/1000] - train loss: 13.044635772705078, valid loss: 28.342500686645508\n",
      "[0011/1000] - train loss: 13.02837085723877, valid loss: 28.320592880249023\n",
      "[0012/1000] - train loss: 13.012129783630371, valid loss: 28.298547744750977\n",
      "[0013/1000] - train loss: 12.995929718017578, valid loss: 28.276201248168945\n",
      "[0014/1000] - train loss: 12.979768753051758, valid loss: 28.25360870361328\n",
      "[0015/1000] - train loss: 12.96339225769043, valid loss: 28.23139190673828\n",
      "[0016/1000] - train loss: 12.947000503540039, valid loss: 28.209522247314453\n",
      "[0017/1000] - train loss: 12.930610656738281, valid loss: 28.187990188598633\n",
      "[0018/1000] - train loss: 12.914244651794434, valid loss: 28.1667537689209\n",
      "[0019/1000] - train loss: 12.897916793823242, valid loss: 28.145320892333984\n",
      "[0020/1000] - train loss: 12.881597518920898, valid loss: 28.123552322387695\n",
      "[0021/1000] - train loss: 12.865287780761719, valid loss: 28.101957321166992\n",
      "[0022/1000] - train loss: 12.849018096923828, valid loss: 28.080522537231445\n",
      "[0023/1000] - train loss: 12.83276081085205, valid loss: 28.05918312072754\n",
      "[0024/1000] - train loss: 12.816537857055664, valid loss: 28.037416458129883\n",
      "[0025/1000] - train loss: 12.800318717956543, valid loss: 28.015274047851562\n",
      "[0026/1000] - train loss: 12.784185409545898, valid loss: 27.99297523498535\n",
      "[0027/1000] - train loss: 12.76799488067627, valid loss: 27.970909118652344\n",
      "[0028/1000] - train loss: 12.751862525939941, valid loss: 27.949031829833984\n",
      "[0029/1000] - train loss: 12.73575496673584, valid loss: 27.92732048034668\n",
      "[0030/1000] - train loss: 12.719844818115234, valid loss: 27.90569305419922\n",
      "[0031/1000] - train loss: 12.703932762145996, valid loss: 27.883649826049805\n",
      "[0032/1000] - train loss: 12.687996864318848, valid loss: 27.861251831054688\n",
      "[0033/1000] - train loss: 12.67212200164795, valid loss: 27.839000701904297\n",
      "[0034/1000] - train loss: 12.656301498413086, valid loss: 27.816877365112305\n",
      "[0035/1000] - train loss: 12.6405029296875, valid loss: 27.79488754272461\n",
      "[0036/1000] - train loss: 12.624683380126953, valid loss: 27.773231506347656\n",
      "[0037/1000] - train loss: 12.608927726745605, valid loss: 27.751256942749023\n",
      "[0038/1000] - train loss: 12.593194961547852, valid loss: 27.728954315185547\n",
      "[0039/1000] - train loss: 12.577479362487793, valid loss: 27.706865310668945\n",
      "[0040/1000] - train loss: 12.561827659606934, valid loss: 27.68505096435547\n",
      "[0041/1000] - train loss: 12.546204566955566, valid loss: 27.663427352905273\n",
      "[0042/1000] - train loss: 12.530630111694336, valid loss: 27.641613006591797\n",
      "[0043/1000] - train loss: 12.515129089355469, valid loss: 27.619674682617188\n",
      "[0044/1000] - train loss: 12.49980640411377, valid loss: 27.598052978515625\n",
      "[0045/1000] - train loss: 12.484516143798828, valid loss: 27.57665252685547\n",
      "[0046/1000] - train loss: 12.469278335571289, valid loss: 27.55544662475586\n",
      "[0047/1000] - train loss: 12.453993797302246, valid loss: 27.534486770629883\n",
      "[0048/1000] - train loss: 12.438716888427734, valid loss: 27.51371955871582\n",
      "[0049/1000] - train loss: 12.423543930053711, valid loss: 27.492326736450195\n",
      "[0050/1000] - train loss: 12.408321380615234, valid loss: 27.470684051513672\n",
      "[0051/1000] - train loss: 12.393143653869629, valid loss: 27.448823928833008\n",
      "[0052/1000] - train loss: 12.37806224822998, valid loss: 27.427309036254883\n",
      "[0053/1000] - train loss: 12.36302375793457, valid loss: 27.406139373779297\n",
      "[0054/1000] - train loss: 12.34800910949707, valid loss: 27.38528060913086\n",
      "[0055/1000] - train loss: 12.333040237426758, valid loss: 27.364608764648438\n",
      "[0056/1000] - train loss: 12.318191528320312, valid loss: 27.34408950805664\n",
      "[0057/1000] - train loss: 12.303427696228027, valid loss: 27.323694229125977\n",
      "[0058/1000] - train loss: 12.288727760314941, valid loss: 27.302894592285156\n",
      "[0059/1000] - train loss: 12.274088859558105, valid loss: 27.281721115112305\n",
      "[0060/1000] - train loss: 12.259509086608887, valid loss: 27.260679244995117\n",
      "[0061/1000] - train loss: 12.244999885559082, valid loss: 27.239784240722656\n",
      "[0062/1000] - train loss: 12.230541229248047, valid loss: 27.21915054321289\n",
      "[0063/1000] - train loss: 12.216158866882324, valid loss: 27.198654174804688\n",
      "[0064/1000] - train loss: 12.201815605163574, valid loss: 27.178081512451172\n",
      "[0065/1000] - train loss: 12.187525749206543, valid loss: 27.156829833984375\n",
      "[0066/1000] - train loss: 12.173254013061523, valid loss: 27.135225296020508\n",
      "[0067/1000] - train loss: 12.159072875976562, valid loss: 27.113798141479492\n",
      "[0068/1000] - train loss: 12.144946098327637, valid loss: 27.092567443847656\n",
      "[0069/1000] - train loss: 12.13090991973877, valid loss: 27.071557998657227\n",
      "[0070/1000] - train loss: 12.116925239562988, valid loss: 27.050745010375977\n",
      "[0071/1000] - train loss: 12.103018760681152, valid loss: 27.02979278564453\n",
      "[0072/1000] - train loss: 12.089151382446289, valid loss: 27.008989334106445\n",
      "[0073/1000] - train loss: 12.075371742248535, valid loss: 26.988357543945312\n",
      "[0074/1000] - train loss: 12.061666488647461, valid loss: 26.96736717224121\n",
      "[0075/1000] - train loss: 12.047987937927246, valid loss: 26.94605255126953\n",
      "[0076/1000] - train loss: 12.034346580505371, valid loss: 26.924848556518555\n",
      "[0077/1000] - train loss: 12.020780563354492, valid loss: 26.903837203979492\n",
      "[0078/1000] - train loss: 12.007280349731445, valid loss: 26.883140563964844\n",
      "[0079/1000] - train loss: 11.99382495880127, valid loss: 26.862743377685547\n",
      "[0080/1000] - train loss: 11.98034954071045, valid loss: 26.842588424682617\n",
      "[0081/1000] - train loss: 11.966804504394531, valid loss: 26.82221221923828\n",
      "[0082/1000] - train loss: 11.95324420928955, valid loss: 26.802139282226562\n",
      "[0083/1000] - train loss: 11.93972396850586, valid loss: 26.781843185424805\n",
      "[0084/1000] - train loss: 11.926237106323242, valid loss: 26.761829376220703\n",
      "[0085/1000] - train loss: 11.912790298461914, valid loss: 26.742084503173828\n",
      "[0086/1000] - train loss: 11.899365425109863, valid loss: 26.72258949279785\n",
      "[0087/1000] - train loss: 11.88596248626709, valid loss: 26.703310012817383\n",
      "[0088/1000] - train loss: 11.872604370117188, valid loss: 26.684221267700195\n",
      "[0089/1000] - train loss: 11.859283447265625, valid loss: 26.664710998535156\n",
      "[0090/1000] - train loss: 11.846024513244629, valid loss: 26.64533805847168\n",
      "[0091/1000] - train loss: 11.8328275680542, valid loss: 26.626033782958984\n",
      "[0092/1000] - train loss: 11.819684982299805, valid loss: 26.60678482055664\n",
      "[0093/1000] - train loss: 11.806591033935547, valid loss: 26.58711051940918\n",
      "[0094/1000] - train loss: 11.793557167053223, valid loss: 26.56743621826172\n",
      "[0095/1000] - train loss: 11.780549049377441, valid loss: 26.54789924621582\n",
      "[0096/1000] - train loss: 11.767610549926758, valid loss: 26.528499603271484\n",
      "[0097/1000] - train loss: 11.754728317260742, valid loss: 26.509214401245117\n",
      "[0098/1000] - train loss: 11.74190616607666, valid loss: 26.489564895629883\n",
      "[0099/1000] - train loss: 11.729104042053223, valid loss: 26.469820022583008\n",
      "[0100/1000] - train loss: 11.716395378112793, valid loss: 26.450761795043945\n",
      "[0101/1000] - train loss: 11.703767776489258, valid loss: 26.431865692138672\n",
      "[0102/1000] - train loss: 11.691181182861328, valid loss: 26.413158416748047\n",
      "[0103/1000] - train loss: 11.678646087646484, valid loss: 26.39459991455078\n",
      "[0104/1000] - train loss: 11.666187286376953, valid loss: 26.376644134521484\n",
      "[0105/1000] - train loss: 11.653767585754395, valid loss: 26.358888626098633\n",
      "[0106/1000] - train loss: 11.641397476196289, valid loss: 26.34125328063965\n",
      "[0107/1000] - train loss: 11.6290922164917, valid loss: 26.3232364654541\n",
      "[0108/1000] - train loss: 11.616847038269043, valid loss: 26.30500030517578\n",
      "[0109/1000] - train loss: 11.604645729064941, valid loss: 26.286937713623047\n",
      "[0110/1000] - train loss: 11.592519760131836, valid loss: 26.26906967163086\n",
      "[0111/1000] - train loss: 11.580343246459961, valid loss: 26.251537322998047\n",
      "[0112/1000] - train loss: 11.568044662475586, valid loss: 26.234289169311523\n",
      "[0113/1000] - train loss: 11.555771827697754, valid loss: 26.21682357788086\n",
      "[0114/1000] - train loss: 11.543527603149414, valid loss: 26.199668884277344\n",
      "[0115/1000] - train loss: 11.531359672546387, valid loss: 26.18280792236328\n",
      "[0116/1000] - train loss: 11.519222259521484, valid loss: 26.16572380065918\n",
      "[0117/1000] - train loss: 11.507122039794922, valid loss: 26.14892578125\n",
      "[0118/1000] - train loss: 11.495078086853027, valid loss: 26.13199806213379\n",
      "[0119/1000] - train loss: 11.48300838470459, valid loss: 26.1153507232666\n",
      "[0120/1000] - train loss: 11.471002578735352, valid loss: 26.098539352416992\n",
      "[0121/1000] - train loss: 11.4590425491333, valid loss: 26.082077026367188\n",
      "[0122/1000] - train loss: 11.44711971282959, valid loss: 26.065919876098633\n",
      "[0123/1000] - train loss: 11.435254096984863, valid loss: 26.0499210357666\n",
      "[0124/1000] - train loss: 11.423602104187012, valid loss: 26.0335693359375\n",
      "[0125/1000] - train loss: 11.41197681427002, valid loss: 26.016904830932617\n",
      "[0126/1000] - train loss: 11.400424003601074, valid loss: 26.00046730041504\n",
      "[0127/1000] - train loss: 11.388911247253418, valid loss: 25.98424530029297\n",
      "[0128/1000] - train loss: 11.377427101135254, valid loss: 25.968236923217773\n",
      "[0129/1000] - train loss: 11.365979194641113, valid loss: 25.952417373657227\n",
      "[0130/1000] - train loss: 11.354619979858398, valid loss: 25.936779022216797\n",
      "[0131/1000] - train loss: 11.343323707580566, valid loss: 25.921323776245117\n",
      "[0132/1000] - train loss: 11.332075119018555, valid loss: 25.906034469604492\n",
      "[0133/1000] - train loss: 11.320882797241211, valid loss: 25.890453338623047\n",
      "[0134/1000] - train loss: 11.309731483459473, valid loss: 25.874576568603516\n",
      "[0135/1000] - train loss: 11.298611640930176, valid loss: 25.858884811401367\n",
      "[0136/1000] - train loss: 11.287566184997559, valid loss: 25.843366622924805\n",
      "[0137/1000] - train loss: 11.27652645111084, valid loss: 25.82803726196289\n",
      "[0138/1000] - train loss: 11.26551628112793, valid loss: 25.812877655029297\n",
      "[0139/1000] - train loss: 11.254581451416016, valid loss: 25.79787254333496\n",
      "[0140/1000] - train loss: 11.243630409240723, valid loss: 25.78261375427246\n",
      "[0141/1000] - train loss: 11.232726097106934, valid loss: 25.767541885375977\n",
      "[0142/1000] - train loss: 11.221848487854004, valid loss: 25.75264549255371\n",
      "[0143/1000] - train loss: 11.211013793945312, valid loss: 25.737512588500977\n",
      "[0144/1000] - train loss: 11.200178146362305, valid loss: 25.72258949279785\n",
      "[0145/1000] - train loss: 11.189393043518066, valid loss: 25.707874298095703\n",
      "[0146/1000] - train loss: 11.178638458251953, valid loss: 25.6933536529541\n",
      "[0147/1000] - train loss: 11.167924880981445, valid loss: 25.67898178100586\n",
      "[0148/1000] - train loss: 11.157316207885742, valid loss: 25.664228439331055\n",
      "[0149/1000] - train loss: 11.146706581115723, valid loss: 25.649133682250977\n",
      "[0150/1000] - train loss: 11.136122703552246, valid loss: 25.633790969848633\n",
      "[0151/1000] - train loss: 11.125614166259766, valid loss: 25.618730545043945\n",
      "[0152/1000] - train loss: 11.11513614654541, valid loss: 25.60395622253418\n",
      "[0153/1000] - train loss: 11.104717254638672, valid loss: 25.589414596557617\n",
      "[0154/1000] - train loss: 11.094326972961426, valid loss: 25.575109481811523\n",
      "[0155/1000] - train loss: 11.083963394165039, valid loss: 25.56100082397461\n",
      "[0156/1000] - train loss: 11.073643684387207, valid loss: 25.547082901000977\n",
      "[0157/1000] - train loss: 11.063353538513184, valid loss: 25.533340454101562\n",
      "[0158/1000] - train loss: 11.053096771240234, valid loss: 25.51975440979004\n",
      "[0159/1000] - train loss: 11.042886734008789, valid loss: 25.506330490112305\n",
      "[0160/1000] - train loss: 11.032754898071289, valid loss: 25.49251937866211\n",
      "[0161/1000] - train loss: 11.022613525390625, valid loss: 25.47836685180664\n",
      "[0162/1000] - train loss: 11.012481689453125, valid loss: 25.463912963867188\n",
      "[0163/1000] - train loss: 11.002376556396484, valid loss: 25.449708938598633\n",
      "[0164/1000] - train loss: 10.992348670959473, valid loss: 25.43573570251465\n",
      "[0165/1000] - train loss: 10.982349395751953, valid loss: 25.42198944091797\n",
      "[0166/1000] - train loss: 10.972379684448242, valid loss: 25.40842628479004\n",
      "[0167/1000] - train loss: 10.96248722076416, valid loss: 25.394439697265625\n",
      "[0168/1000] - train loss: 10.952746391296387, valid loss: 25.38051414489746\n",
      "[0169/1000] - train loss: 10.943111419677734, valid loss: 25.36662483215332\n",
      "[0170/1000] - train loss: 10.933558464050293, valid loss: 25.352245330810547\n",
      "[0171/1000] - train loss: 10.924031257629395, valid loss: 25.337356567382812\n",
      "[0172/1000] - train loss: 10.914536476135254, valid loss: 25.32213592529297\n",
      "[0173/1000] - train loss: 10.905071258544922, valid loss: 25.307144165039062\n",
      "[0174/1000] - train loss: 10.895655632019043, valid loss: 25.29242706298828\n",
      "[0175/1000] - train loss: 10.886252403259277, valid loss: 25.27796173095703\n",
      "[0176/1000] - train loss: 10.876861572265625, valid loss: 25.263755798339844\n",
      "[0177/1000] - train loss: 10.8674955368042, valid loss: 25.24979591369629\n",
      "[0178/1000] - train loss: 10.858153343200684, valid loss: 25.23556900024414\n",
      "[0179/1000] - train loss: 10.848834991455078, valid loss: 25.2210750579834\n",
      "[0180/1000] - train loss: 10.839557647705078, valid loss: 25.206890106201172\n",
      "[0181/1000] - train loss: 10.830317497253418, valid loss: 25.193008422851562\n",
      "[0182/1000] - train loss: 10.821090698242188, valid loss: 25.179386138916016\n",
      "[0183/1000] - train loss: 10.81188678741455, valid loss: 25.165489196777344\n",
      "[0184/1000] - train loss: 10.802721977233887, valid loss: 25.151844024658203\n",
      "[0185/1000] - train loss: 10.793597221374512, valid loss: 25.13845443725586\n",
      "[0186/1000] - train loss: 10.784509658813477, valid loss: 25.12480354309082\n",
      "[0187/1000] - train loss: 10.77542781829834, valid loss: 25.111412048339844\n",
      "[0188/1000] - train loss: 10.766407012939453, valid loss: 25.09785270690918\n",
      "[0189/1000] - train loss: 10.757411003112793, valid loss: 25.084049224853516\n",
      "[0190/1000] - train loss: 10.748428344726562, valid loss: 25.07003402709961\n",
      "[0191/1000] - train loss: 10.73950481414795, valid loss: 25.05637550354004\n",
      "[0192/1000] - train loss: 10.730613708496094, valid loss: 25.04304313659668\n",
      "[0193/1000] - train loss: 10.721759796142578, valid loss: 25.029979705810547\n",
      "[0194/1000] - train loss: 10.713022232055664, valid loss: 25.017127990722656\n",
      "[0195/1000] - train loss: 10.704338073730469, valid loss: 25.004467010498047\n",
      "[0196/1000] - train loss: 10.695697784423828, valid loss: 24.9914608001709\n",
      "[0197/1000] - train loss: 10.687058448791504, valid loss: 24.978168487548828\n",
      "[0198/1000] - train loss: 10.678436279296875, valid loss: 24.96512222290039\n",
      "[0199/1000] - train loss: 10.66982650756836, valid loss: 24.952314376831055\n",
      "[0200/1000] - train loss: 10.66125774383545, valid loss: 24.939697265625\n",
      "[0201/1000] - train loss: 10.652771949768066, valid loss: 24.927234649658203\n",
      "[0202/1000] - train loss: 10.64431095123291, valid loss: 24.914411544799805\n",
      "[0203/1000] - train loss: 10.635863304138184, valid loss: 24.901273727416992\n",
      "[0204/1000] - train loss: 10.627467155456543, valid loss: 24.888351440429688\n",
      "[0205/1000] - train loss: 10.619145393371582, valid loss: 24.8756160736084\n",
      "[0206/1000] - train loss: 10.61085033416748, valid loss: 24.863056182861328\n",
      "[0207/1000] - train loss: 10.602546691894531, valid loss: 24.850343704223633\n",
      "[0208/1000] - train loss: 10.594221115112305, valid loss: 24.837936401367188\n",
      "[0209/1000] - train loss: 10.585947036743164, valid loss: 24.825353622436523\n",
      "[0210/1000] - train loss: 10.577685356140137, valid loss: 24.812665939331055\n",
      "[0211/1000] - train loss: 10.569436073303223, valid loss: 24.800092697143555\n",
      "[0212/1000] - train loss: 10.561227798461914, valid loss: 24.788148880004883\n",
      "[0213/1000] - train loss: 10.55300521850586, valid loss: 24.77677345275879\n",
      "[0214/1000] - train loss: 10.544787406921387, valid loss: 24.76590919494629\n",
      "[0215/1000] - train loss: 10.536580085754395, valid loss: 24.755369186401367\n",
      "[0216/1000] - train loss: 10.528390884399414, valid loss: 24.7451114654541\n",
      "[0217/1000] - train loss: 10.520210266113281, valid loss: 24.735095977783203\n",
      "[0218/1000] - train loss: 10.512041091918945, valid loss: 24.72528648376465\n",
      "[0219/1000] - train loss: 10.503890991210938, valid loss: 24.715639114379883\n",
      "[0220/1000] - train loss: 10.495816230773926, valid loss: 24.705629348754883\n",
      "[0221/1000] - train loss: 10.487741470336914, valid loss: 24.695301055908203\n",
      "[0222/1000] - train loss: 10.47973918914795, valid loss: 24.684873580932617\n",
      "[0223/1000] - train loss: 10.471761703491211, valid loss: 24.674863815307617\n",
      "[0224/1000] - train loss: 10.463817596435547, valid loss: 24.66521644592285\n",
      "[0225/1000] - train loss: 10.455867767333984, valid loss: 24.655855178833008\n",
      "[0226/1000] - train loss: 10.447895050048828, valid loss: 24.646631240844727\n",
      "[0227/1000] - train loss: 10.439896583557129, valid loss: 24.63791847229004\n",
      "[0228/1000] - train loss: 10.431889533996582, valid loss: 24.629663467407227\n",
      "[0229/1000] - train loss: 10.423916816711426, valid loss: 24.621063232421875\n",
      "[0230/1000] - train loss: 10.416033744812012, valid loss: 24.611955642700195\n",
      "[0231/1000] - train loss: 10.408185958862305, valid loss: 24.602874755859375\n",
      "[0232/1000] - train loss: 10.4003324508667, valid loss: 24.59377670288086\n",
      "[0233/1000] - train loss: 10.392504692077637, valid loss: 24.58460235595703\n",
      "[0234/1000] - train loss: 10.384711265563965, valid loss: 24.575332641601562\n",
      "[0235/1000] - train loss: 10.376930236816406, valid loss: 24.56595802307129\n",
      "[0236/1000] - train loss: 10.369145393371582, valid loss: 24.55648422241211\n",
      "[0237/1000] - train loss: 10.361391067504883, valid loss: 24.546415328979492\n",
      "[0238/1000] - train loss: 10.35366439819336, valid loss: 24.536300659179688\n",
      "[0239/1000] - train loss: 10.346006393432617, valid loss: 24.526655197143555\n",
      "[0240/1000] - train loss: 10.338379859924316, valid loss: 24.517005920410156\n",
      "[0241/1000] - train loss: 10.330768585205078, valid loss: 24.507389068603516\n",
      "[0242/1000] - train loss: 10.323166847229004, valid loss: 24.49795913696289\n",
      "[0243/1000] - train loss: 10.315593719482422, valid loss: 24.488666534423828\n",
      "[0244/1000] - train loss: 10.308024406433105, valid loss: 24.479490280151367\n",
      "[0245/1000] - train loss: 10.300541877746582, valid loss: 24.469635009765625\n",
      "[0246/1000] - train loss: 10.29307746887207, valid loss: 24.45937156677246\n",
      "[0247/1000] - train loss: 10.28560733795166, valid loss: 24.449249267578125\n",
      "[0248/1000] - train loss: 10.278175354003906, valid loss: 24.43904685974121\n",
      "[0249/1000] - train loss: 10.270770072937012, valid loss: 24.429012298583984\n",
      "[0250/1000] - train loss: 10.263410568237305, valid loss: 24.419116973876953\n",
      "[0251/1000] - train loss: 10.25607967376709, valid loss: 24.409360885620117\n",
      "[0252/1000] - train loss: 10.248760223388672, valid loss: 24.399717330932617\n",
      "[0253/1000] - train loss: 10.24145793914795, valid loss: 24.390186309814453\n",
      "[0254/1000] - train loss: 10.2341947555542, valid loss: 24.38043975830078\n",
      "[0255/1000] - train loss: 10.226961135864258, valid loss: 24.370107650756836\n",
      "[0256/1000] - train loss: 10.219767570495605, valid loss: 24.35967445373535\n",
      "[0257/1000] - train loss: 10.212615966796875, valid loss: 24.34917640686035\n",
      "[0258/1000] - train loss: 10.205467224121094, valid loss: 24.338638305664062\n",
      "[0259/1000] - train loss: 10.198341369628906, valid loss: 24.328325271606445\n",
      "[0260/1000] - train loss: 10.191263198852539, valid loss: 24.318227767944336\n",
      "[0261/1000] - train loss: 10.184186935424805, valid loss: 24.308549880981445\n",
      "[0262/1000] - train loss: 10.177128791809082, valid loss: 24.299259185791016\n",
      "[0263/1000] - train loss: 10.17012882232666, valid loss: 24.2895565032959\n",
      "[0264/1000] - train loss: 10.163125038146973, valid loss: 24.279510498046875\n",
      "[0265/1000] - train loss: 10.156149864196777, valid loss: 24.26967430114746\n",
      "[0266/1000] - train loss: 10.149198532104492, valid loss: 24.26009178161621\n",
      "[0267/1000] - train loss: 10.142313003540039, valid loss: 24.25033187866211\n",
      "[0268/1000] - train loss: 10.135416030883789, valid loss: 24.240741729736328\n",
      "[0269/1000] - train loss: 10.128535270690918, valid loss: 24.231426239013672\n",
      "[0270/1000] - train loss: 10.121726036071777, valid loss: 24.221582412719727\n",
      "[0271/1000] - train loss: 10.114940643310547, valid loss: 24.211267471313477\n",
      "[0272/1000] - train loss: 10.108168601989746, valid loss: 24.2010440826416\n",
      "[0273/1000] - train loss: 10.101421356201172, valid loss: 24.19092559814453\n",
      "[0274/1000] - train loss: 10.094681739807129, valid loss: 24.180862426757812\n",
      "[0275/1000] - train loss: 10.087970733642578, valid loss: 24.170289993286133\n",
      "[0276/1000] - train loss: 10.08126163482666, valid loss: 24.160165786743164\n",
      "[0277/1000] - train loss: 10.07457447052002, valid loss: 24.150232315063477\n",
      "[0278/1000] - train loss: 10.067924499511719, valid loss: 24.13994026184082\n",
      "[0279/1000] - train loss: 10.061263084411621, valid loss: 24.129610061645508\n",
      "[0280/1000] - train loss: 10.054625511169434, valid loss: 24.1195011138916\n",
      "[0281/1000] - train loss: 10.048026084899902, valid loss: 24.109281539916992\n",
      "[0282/1000] - train loss: 10.041414260864258, valid loss: 24.0991268157959\n",
      "[0283/1000] - train loss: 10.03481674194336, valid loss: 24.089216232299805\n",
      "[0284/1000] - train loss: 10.028220176696777, valid loss: 24.07887840270996\n",
      "[0285/1000] - train loss: 10.021641731262207, valid loss: 24.06846046447754\n",
      "[0286/1000] - train loss: 10.015059471130371, valid loss: 24.058338165283203\n",
      "[0287/1000] - train loss: 10.008502960205078, valid loss: 24.04849624633789\n",
      "[0288/1000] - train loss: 10.001965522766113, valid loss: 24.038555145263672\n",
      "[0289/1000] - train loss: 9.995409965515137, valid loss: 24.02850914001465\n",
      "[0290/1000] - train loss: 9.988879203796387, valid loss: 24.018402099609375\n",
      "[0291/1000] - train loss: 9.982314109802246, valid loss: 24.008529663085938\n",
      "[0292/1000] - train loss: 9.975723266601562, valid loss: 23.99880027770996\n",
      "[0293/1000] - train loss: 9.96911907196045, valid loss: 23.989286422729492\n",
      "[0294/1000] - train loss: 9.962554931640625, valid loss: 23.9801082611084\n",
      "[0295/1000] - train loss: 9.956024169921875, valid loss: 23.970685958862305\n",
      "[0296/1000] - train loss: 9.94948673248291, valid loss: 23.96082305908203\n",
      "[0297/1000] - train loss: 9.942953109741211, valid loss: 23.951065063476562\n",
      "[0298/1000] - train loss: 9.936450004577637, valid loss: 23.941396713256836\n",
      "[0299/1000] - train loss: 9.929951667785645, valid loss: 23.93181037902832\n",
      "[0300/1000] - train loss: 9.92345905303955, valid loss: 23.922592163085938\n",
      "[0301/1000] - train loss: 9.917035102844238, valid loss: 23.913177490234375\n",
      "[0302/1000] - train loss: 9.910606384277344, valid loss: 23.903339385986328\n",
      "[0303/1000] - train loss: 9.90417766571045, valid loss: 23.893695831298828\n",
      "[0304/1000] - train loss: 9.897749900817871, valid loss: 23.88422393798828\n",
      "[0305/1000] - train loss: 9.891345024108887, valid loss: 23.87487030029297\n",
      "[0306/1000] - train loss: 9.88493824005127, valid loss: 23.86553192138672\n",
      "[0307/1000] - train loss: 9.878555297851562, valid loss: 23.856491088867188\n",
      "[0308/1000] - train loss: 9.872227668762207, valid loss: 23.847187042236328\n",
      "[0309/1000] - train loss: 9.865865707397461, valid loss: 23.837631225585938\n",
      "[0310/1000] - train loss: 9.859504699707031, valid loss: 23.828161239624023\n",
      "[0311/1000] - train loss: 9.853194236755371, valid loss: 23.818750381469727\n",
      "[0312/1000] - train loss: 9.846901893615723, valid loss: 23.80940818786621\n",
      "[0313/1000] - train loss: 9.84061336517334, valid loss: 23.80013084411621\n",
      "[0314/1000] - train loss: 9.834354400634766, valid loss: 23.790359497070312\n",
      "[0315/1000] - train loss: 9.828092575073242, valid loss: 23.78042221069336\n",
      "[0316/1000] - train loss: 9.821852684020996, valid loss: 23.77090835571289\n",
      "[0317/1000] - train loss: 9.815696716308594, valid loss: 23.761417388916016\n",
      "[0318/1000] - train loss: 9.80955696105957, valid loss: 23.751964569091797\n",
      "[0319/1000] - train loss: 9.803403854370117, valid loss: 23.742542266845703\n",
      "[0320/1000] - train loss: 9.797258377075195, valid loss: 23.733396530151367\n",
      "[0321/1000] - train loss: 9.791168212890625, valid loss: 23.723941802978516\n",
      "[0322/1000] - train loss: 9.785079956054688, valid loss: 23.714542388916016\n",
      "[0323/1000] - train loss: 9.779017448425293, valid loss: 23.70459747314453\n",
      "[0324/1000] - train loss: 9.772982597351074, valid loss: 23.694719314575195\n",
      "[0325/1000] - train loss: 9.766977310180664, valid loss: 23.684919357299805\n",
      "[0326/1000] - train loss: 9.761048316955566, valid loss: 23.675329208374023\n",
      "[0327/1000] - train loss: 9.755160331726074, valid loss: 23.665388107299805\n",
      "[0328/1000] - train loss: 9.749307632446289, valid loss: 23.655555725097656\n",
      "[0329/1000] - train loss: 9.743504524230957, valid loss: 23.64568328857422\n",
      "[0330/1000] - train loss: 9.737698554992676, valid loss: 23.635833740234375\n",
      "[0331/1000] - train loss: 9.731932640075684, valid loss: 23.62543487548828\n",
      "[0332/1000] - train loss: 9.726154327392578, valid loss: 23.61458969116211\n",
      "[0333/1000] - train loss: 9.720407485961914, valid loss: 23.603946685791016\n",
      "[0334/1000] - train loss: 9.714675903320312, valid loss: 23.593523025512695\n",
      "[0335/1000] - train loss: 9.708949089050293, valid loss: 23.58359146118164\n",
      "[0336/1000] - train loss: 9.703227996826172, valid loss: 23.57411766052246\n",
      "[0337/1000] - train loss: 9.697490692138672, valid loss: 23.565120697021484\n",
      "[0338/1000] - train loss: 9.691767692565918, valid loss: 23.556001663208008\n",
      "[0339/1000] - train loss: 9.686102867126465, valid loss: 23.546527862548828\n",
      "[0340/1000] - train loss: 9.680410385131836, valid loss: 23.537294387817383\n",
      "[0341/1000] - train loss: 9.674728393554688, valid loss: 23.528282165527344\n",
      "[0342/1000] - train loss: 9.669035911560059, valid loss: 23.519556045532227\n",
      "[0343/1000] - train loss: 9.66336727142334, valid loss: 23.51053237915039\n",
      "[0344/1000] - train loss: 9.657635688781738, valid loss: 23.501855850219727\n",
      "[0345/1000] - train loss: 9.65188217163086, valid loss: 23.4925479888916\n",
      "[0346/1000] - train loss: 9.646108627319336, valid loss: 23.483911514282227\n",
      "[0347/1000] - train loss: 9.640347480773926, valid loss: 23.475811004638672\n",
      "[0348/1000] - train loss: 9.634567260742188, valid loss: 23.46819496154785\n",
      "[0349/1000] - train loss: 9.628750801086426, valid loss: 23.460996627807617\n",
      "[0350/1000] - train loss: 9.622993469238281, valid loss: 23.453388214111328\n",
      "[0351/1000] - train loss: 9.617220878601074, valid loss: 23.445348739624023\n",
      "[0352/1000] - train loss: 9.61145305633545, valid loss: 23.437450408935547\n",
      "[0353/1000] - train loss: 9.605685234069824, valid loss: 23.429662704467773\n",
      "[0354/1000] - train loss: 9.599920272827148, valid loss: 23.421918869018555\n",
      "[0355/1000] - train loss: 9.594207763671875, valid loss: 23.414173126220703\n",
      "[0356/1000] - train loss: 9.588478088378906, valid loss: 23.406654357910156\n",
      "[0357/1000] - train loss: 9.582784652709961, valid loss: 23.39875030517578\n",
      "[0358/1000] - train loss: 9.577045440673828, valid loss: 23.390518188476562\n",
      "[0359/1000] - train loss: 9.571324348449707, valid loss: 23.38230323791504\n",
      "[0360/1000] - train loss: 9.565659523010254, valid loss: 23.374128341674805\n",
      "[0361/1000] - train loss: 9.559993743896484, valid loss: 23.366016387939453\n",
      "[0362/1000] - train loss: 9.55432415008545, valid loss: 23.35816192626953\n",
      "[0363/1000] - train loss: 9.548624992370605, valid loss: 23.35050392150879\n",
      "[0364/1000] - train loss: 9.54295539855957, valid loss: 23.342193603515625\n",
      "[0365/1000] - train loss: 9.537298202514648, valid loss: 23.333282470703125\n",
      "[0366/1000] - train loss: 9.531661987304688, valid loss: 23.324432373046875\n",
      "[0367/1000] - train loss: 9.526029586791992, valid loss: 23.31587028503418\n",
      "[0368/1000] - train loss: 9.520395278930664, valid loss: 23.30756187438965\n",
      "[0369/1000] - train loss: 9.514789581298828, valid loss: 23.29926872253418\n",
      "[0370/1000] - train loss: 9.509194374084473, valid loss: 23.290987014770508\n",
      "[0371/1000] - train loss: 9.503602027893066, valid loss: 23.28290367126465\n",
      "[0372/1000] - train loss: 9.498061180114746, valid loss: 23.2744083404541\n",
      "[0373/1000] - train loss: 9.492499351501465, valid loss: 23.265331268310547\n",
      "[0374/1000] - train loss: 9.486956596374512, valid loss: 23.256336212158203\n",
      "[0375/1000] - train loss: 9.481451988220215, valid loss: 23.2474365234375\n",
      "[0376/1000] - train loss: 9.475953102111816, valid loss: 23.2388973236084\n",
      "[0377/1000] - train loss: 9.470428466796875, valid loss: 23.230684280395508\n",
      "[0378/1000] - train loss: 9.46491813659668, valid loss: 23.222105026245117\n",
      "[0379/1000] - train loss: 9.459453582763672, valid loss: 23.213539123535156\n",
      "[0380/1000] - train loss: 9.454011917114258, valid loss: 23.20497703552246\n",
      "[0381/1000] - train loss: 9.448564529418945, valid loss: 23.196428298950195\n",
      "[0382/1000] - train loss: 9.44312572479248, valid loss: 23.18760108947754\n",
      "[0383/1000] - train loss: 9.437676429748535, valid loss: 23.17908477783203\n",
      "[0384/1000] - train loss: 9.432271957397461, valid loss: 23.17058753967285\n",
      "[0385/1000] - train loss: 9.426870346069336, valid loss: 23.161500930786133\n",
      "[0386/1000] - train loss: 9.421491622924805, valid loss: 23.152660369873047\n",
      "[0387/1000] - train loss: 9.416166305541992, valid loss: 23.14377212524414\n",
      "[0388/1000] - train loss: 9.410871505737305, valid loss: 23.135066986083984\n",
      "[0389/1000] - train loss: 9.405582427978516, valid loss: 23.126571655273438\n",
      "[0390/1000] - train loss: 9.400336265563965, valid loss: 23.117429733276367\n",
      "[0391/1000] - train loss: 9.395076751708984, valid loss: 23.107715606689453\n",
      "[0392/1000] - train loss: 9.3898344039917, valid loss: 23.098066329956055\n",
      "[0393/1000] - train loss: 9.384649276733398, valid loss: 23.088726043701172\n",
      "[0394/1000] - train loss: 9.379449844360352, valid loss: 23.07969093322754\n",
      "[0395/1000] - train loss: 9.374255180358887, valid loss: 23.07103729248047\n",
      "[0396/1000] - train loss: 9.3690767288208, valid loss: 23.062755584716797\n",
      "[0397/1000] - train loss: 9.363934516906738, valid loss: 23.054555892944336\n",
      "[0398/1000] - train loss: 9.358777046203613, valid loss: 23.04638671875\n",
      "[0399/1000] - train loss: 9.353694915771484, valid loss: 23.037641525268555\n",
      "[0400/1000] - train loss: 9.34858512878418, valid loss: 23.028383255004883\n",
      "[0401/1000] - train loss: 9.343460083007812, valid loss: 23.01865577697754\n",
      "[0402/1000] - train loss: 9.338375091552734, valid loss: 23.00943946838379\n",
      "[0403/1000] - train loss: 9.33333683013916, valid loss: 23.00065040588379\n",
      "[0404/1000] - train loss: 9.328329086303711, valid loss: 22.99228286743164\n",
      "[0405/1000] - train loss: 9.323317527770996, valid loss: 22.984317779541016\n",
      "[0406/1000] - train loss: 9.318327903747559, valid loss: 22.97665023803711\n",
      "[0407/1000] - train loss: 9.313360214233398, valid loss: 22.969024658203125\n",
      "[0408/1000] - train loss: 9.308374404907227, valid loss: 22.961441040039062\n",
      "[0409/1000] - train loss: 9.303426742553711, valid loss: 22.95394515991211\n",
      "[0410/1000] - train loss: 9.298530578613281, valid loss: 22.945934295654297\n",
      "[0411/1000] - train loss: 9.293595314025879, valid loss: 22.937705993652344\n",
      "[0412/1000] - train loss: 9.288649559020996, valid loss: 22.929367065429688\n",
      "[0413/1000] - train loss: 9.283681869506836, valid loss: 22.92129135131836\n",
      "[0414/1000] - train loss: 9.278779983520508, valid loss: 22.913375854492188\n",
      "[0415/1000] - train loss: 9.273879051208496, valid loss: 22.905996322631836\n",
      "[0416/1000] - train loss: 9.268953323364258, valid loss: 22.899097442626953\n",
      "[0417/1000] - train loss: 9.263998985290527, valid loss: 22.892641067504883\n",
      "[0418/1000] - train loss: 9.259032249450684, valid loss: 22.88626480102539\n",
      "[0419/1000] - train loss: 9.254103660583496, valid loss: 22.879940032958984\n",
      "[0420/1000] - train loss: 9.24918270111084, valid loss: 22.872745513916016\n",
      "[0421/1000] - train loss: 9.24423885345459, valid loss: 22.86539077758789\n",
      "[0422/1000] - train loss: 9.23928451538086, valid loss: 22.857860565185547\n",
      "[0423/1000] - train loss: 9.234335899353027, valid loss: 22.850757598876953\n",
      "[0424/1000] - train loss: 9.229433059692383, valid loss: 22.84372901916504\n",
      "[0425/1000] - train loss: 9.224529266357422, valid loss: 22.83681297302246\n",
      "[0426/1000] - train loss: 9.219616889953613, valid loss: 22.829936981201172\n",
      "[0427/1000] - train loss: 9.214693069458008, valid loss: 22.82306671142578\n",
      "[0428/1000] - train loss: 9.209817886352539, valid loss: 22.816530227661133\n",
      "[0429/1000] - train loss: 9.204938888549805, valid loss: 22.810256958007812\n",
      "[0430/1000] - train loss: 9.200057029724121, valid loss: 22.803625106811523\n",
      "[0431/1000] - train loss: 9.195202827453613, valid loss: 22.796632766723633\n",
      "[0432/1000] - train loss: 9.190323829650879, valid loss: 22.78989028930664\n",
      "[0433/1000] - train loss: 9.185501098632812, valid loss: 22.783172607421875\n",
      "[0434/1000] - train loss: 9.180704116821289, valid loss: 22.776460647583008\n",
      "[0435/1000] - train loss: 9.175853729248047, valid loss: 22.769851684570312\n",
      "[0436/1000] - train loss: 9.17100715637207, valid loss: 22.762680053710938\n",
      "[0437/1000] - train loss: 9.166166305541992, valid loss: 22.755577087402344\n",
      "[0438/1000] - train loss: 9.161325454711914, valid loss: 22.747745513916016\n",
      "[0439/1000] - train loss: 9.156583786010742, valid loss: 22.740280151367188\n",
      "[0440/1000] - train loss: 9.151848793029785, valid loss: 22.733163833618164\n",
      "[0441/1000] - train loss: 9.147116661071777, valid loss: 22.726293563842773\n",
      "[0442/1000] - train loss: 9.142382621765137, valid loss: 22.71945571899414\n",
      "[0443/1000] - train loss: 9.137673377990723, valid loss: 22.71210479736328\n",
      "[0444/1000] - train loss: 9.132964134216309, valid loss: 22.704883575439453\n",
      "[0445/1000] - train loss: 9.128256797790527, valid loss: 22.698022842407227\n",
      "[0446/1000] - train loss: 9.1235933303833, valid loss: 22.690841674804688\n",
      "[0447/1000] - train loss: 9.118922233581543, valid loss: 22.683778762817383\n",
      "[0448/1000] - train loss: 9.114230155944824, valid loss: 22.676971435546875\n",
      "[0449/1000] - train loss: 9.109559059143066, valid loss: 22.6697940826416\n",
      "[0450/1000] - train loss: 9.104835510253906, valid loss: 22.66317367553711\n",
      "[0451/1000] - train loss: 9.100115776062012, valid loss: 22.656970977783203\n",
      "[0452/1000] - train loss: 9.09538745880127, valid loss: 22.6513729095459\n",
      "[0453/1000] - train loss: 9.09067440032959, valid loss: 22.645431518554688\n",
      "[0454/1000] - train loss: 9.085952758789062, valid loss: 22.639217376708984\n",
      "[0455/1000] - train loss: 9.081258773803711, valid loss: 22.63330841064453\n",
      "[0456/1000] - train loss: 9.076550483703613, valid loss: 22.628049850463867\n",
      "[0457/1000] - train loss: 9.071823120117188, valid loss: 22.623048782348633\n",
      "[0458/1000] - train loss: 9.067097663879395, valid loss: 22.61852264404297\n",
      "[0459/1000] - train loss: 9.062418937683105, valid loss: 22.614126205444336\n",
      "[0460/1000] - train loss: 9.057705879211426, valid loss: 22.609834671020508\n",
      "[0461/1000] - train loss: 9.0530424118042, valid loss: 22.60529899597168\n",
      "[0462/1000] - train loss: 9.048440933227539, valid loss: 22.600332260131836\n",
      "[0463/1000] - train loss: 9.043831825256348, valid loss: 22.594934463500977\n",
      "[0464/1000] - train loss: 9.039193153381348, valid loss: 22.589570999145508\n",
      "[0465/1000] - train loss: 9.034578323364258, valid loss: 22.584238052368164\n",
      "[0466/1000] - train loss: 9.029947280883789, valid loss: 22.578960418701172\n",
      "[0467/1000] - train loss: 9.025359153747559, valid loss: 22.573963165283203\n",
      "[0468/1000] - train loss: 9.020753860473633, valid loss: 22.56894302368164\n",
      "[0469/1000] - train loss: 9.016079902648926, valid loss: 22.56423568725586\n",
      "[0470/1000] - train loss: 9.011423110961914, valid loss: 22.55927085876465\n",
      "[0471/1000] - train loss: 9.006828308105469, valid loss: 22.553821563720703\n",
      "[0472/1000] - train loss: 9.00221061706543, valid loss: 22.54853057861328\n",
      "[0473/1000] - train loss: 8.997590065002441, valid loss: 22.54338264465332\n",
      "[0474/1000] - train loss: 8.992981910705566, valid loss: 22.538330078125\n",
      "[0475/1000] - train loss: 8.988417625427246, valid loss: 22.53366470336914\n",
      "[0476/1000] - train loss: 8.983927726745605, valid loss: 22.52873992919922\n",
      "[0477/1000] - train loss: 8.979364395141602, valid loss: 22.523548126220703\n",
      "[0478/1000] - train loss: 8.974767684936523, valid loss: 22.518726348876953\n",
      "[0479/1000] - train loss: 8.970224380493164, valid loss: 22.51428985595703\n",
      "[0480/1000] - train loss: 8.965746879577637, valid loss: 22.509660720825195\n",
      "[0481/1000] - train loss: 8.961210250854492, valid loss: 22.50489044189453\n",
      "[0482/1000] - train loss: 8.956618309020996, valid loss: 22.500015258789062\n",
      "[0483/1000] - train loss: 8.952056884765625, valid loss: 22.494441986083984\n",
      "[0484/1000] - train loss: 8.94756031036377, valid loss: 22.488563537597656\n",
      "[0485/1000] - train loss: 8.943068504333496, valid loss: 22.483076095581055\n",
      "[0486/1000] - train loss: 8.93857192993164, valid loss: 22.47796058654785\n",
      "[0487/1000] - train loss: 8.934056282043457, valid loss: 22.473163604736328\n",
      "[0488/1000] - train loss: 8.929524421691895, valid loss: 22.468666076660156\n",
      "[0489/1000] - train loss: 8.925031661987305, valid loss: 22.464075088500977\n",
      "[0490/1000] - train loss: 8.920559883117676, valid loss: 22.45873260498047\n",
      "[0491/1000] - train loss: 8.916062355041504, valid loss: 22.452959060668945\n",
      "[0492/1000] - train loss: 8.911624908447266, valid loss: 22.447097778320312\n",
      "[0493/1000] - train loss: 8.907209396362305, valid loss: 22.441444396972656\n",
      "[0494/1000] - train loss: 8.90278148651123, valid loss: 22.43597984313965\n",
      "[0495/1000] - train loss: 8.898313522338867, valid loss: 22.430315017700195\n",
      "[0496/1000] - train loss: 8.893803596496582, valid loss: 22.424440383911133\n",
      "[0497/1000] - train loss: 8.889322280883789, valid loss: 22.4185733795166\n",
      "[0498/1000] - train loss: 8.884915351867676, valid loss: 22.41207504272461\n",
      "[0499/1000] - train loss: 8.880438804626465, valid loss: 22.405017852783203\n",
      "[0500/1000] - train loss: 8.875906944274902, valid loss: 22.39815330505371\n",
      "[0501/1000] - train loss: 8.871464729309082, valid loss: 22.3911075592041\n",
      "[0502/1000] - train loss: 8.867023468017578, valid loss: 22.383970260620117\n",
      "[0503/1000] - train loss: 8.862561225891113, valid loss: 22.376747131347656\n",
      "[0504/1000] - train loss: 8.858099937438965, valid loss: 22.36911964416504\n",
      "[0505/1000] - train loss: 8.853668212890625, valid loss: 22.36176109313965\n",
      "[0506/1000] - train loss: 8.84924030303955, valid loss: 22.354751586914062\n",
      "[0507/1000] - train loss: 8.844802856445312, valid loss: 22.348106384277344\n",
      "[0508/1000] - train loss: 8.840375900268555, valid loss: 22.34086036682129\n",
      "[0509/1000] - train loss: 8.835943222045898, valid loss: 22.333147048950195\n",
      "[0510/1000] - train loss: 8.831485748291016, valid loss: 22.325668334960938\n",
      "[0511/1000] - train loss: 8.82702922821045, valid loss: 22.318798065185547\n",
      "[0512/1000] - train loss: 8.822596549987793, valid loss: 22.31254768371582\n",
      "[0513/1000] - train loss: 8.818135261535645, valid loss: 22.30685043334961\n",
      "[0514/1000] - train loss: 8.81364917755127, valid loss: 22.301523208618164\n",
      "[0515/1000] - train loss: 8.809170722961426, valid loss: 22.29656410217285\n",
      "[0516/1000] - train loss: 8.804780006408691, valid loss: 22.290998458862305\n",
      "[0517/1000] - train loss: 8.800369262695312, valid loss: 22.284854888916016\n",
      "[0518/1000] - train loss: 8.79591178894043, valid loss: 22.278135299682617\n",
      "[0519/1000] - train loss: 8.791439056396484, valid loss: 22.271587371826172\n",
      "[0520/1000] - train loss: 8.787006378173828, valid loss: 22.265459060668945\n",
      "[0521/1000] - train loss: 8.782588958740234, valid loss: 22.259780883789062\n",
      "[0522/1000] - train loss: 8.778167724609375, valid loss: 22.254640579223633\n",
      "[0523/1000] - train loss: 8.773576736450195, valid loss: 22.250043869018555\n",
      "[0524/1000] - train loss: 8.768967628479004, valid loss: 22.24518585205078\n",
      "[0525/1000] - train loss: 8.76431941986084, valid loss: 22.239980697631836\n",
      "[0526/1000] - train loss: 8.75963306427002, valid loss: 22.234630584716797\n",
      "[0527/1000] - train loss: 8.75487232208252, valid loss: 22.229137420654297\n",
      "[0528/1000] - train loss: 8.7500638961792, valid loss: 22.22354507446289\n",
      "[0529/1000] - train loss: 8.745198249816895, valid loss: 22.21784210205078\n",
      "[0530/1000] - train loss: 8.740315437316895, valid loss: 22.211727142333984\n",
      "[0531/1000] - train loss: 8.735424995422363, valid loss: 22.205184936523438\n",
      "[0532/1000] - train loss: 8.730510711669922, valid loss: 22.19890785217285\n",
      "[0533/1000] - train loss: 8.725552558898926, valid loss: 22.19290542602539\n",
      "[0534/1000] - train loss: 8.720565795898438, valid loss: 22.187198638916016\n",
      "[0535/1000] - train loss: 8.71556282043457, valid loss: 22.181703567504883\n",
      "[0536/1000] - train loss: 8.710606575012207, valid loss: 22.176164627075195\n",
      "[0537/1000] - train loss: 8.705601692199707, valid loss: 22.170562744140625\n",
      "[0538/1000] - train loss: 8.70065975189209, valid loss: 22.164201736450195\n",
      "[0539/1000] - train loss: 8.695696830749512, valid loss: 22.15751838684082\n",
      "[0540/1000] - train loss: 8.69068717956543, valid loss: 22.15053939819336\n",
      "[0541/1000] - train loss: 8.685690879821777, valid loss: 22.14398956298828\n",
      "[0542/1000] - train loss: 8.680686950683594, valid loss: 22.137863159179688\n",
      "[0543/1000] - train loss: 8.6757173538208, valid loss: 22.1318416595459\n",
      "[0544/1000] - train loss: 8.670722961425781, valid loss: 22.125946044921875\n",
      "[0545/1000] - train loss: 8.665678024291992, valid loss: 22.120630264282227\n",
      "[0546/1000] - train loss: 8.66060733795166, valid loss: 22.11568832397461\n",
      "[0547/1000] - train loss: 8.655502319335938, valid loss: 22.111103057861328\n",
      "[0548/1000] - train loss: 8.650391578674316, valid loss: 22.105926513671875\n",
      "[0549/1000] - train loss: 8.645296096801758, valid loss: 22.100357055664062\n",
      "[0550/1000] - train loss: 8.640162467956543, valid loss: 22.094396591186523\n",
      "[0551/1000] - train loss: 8.635010719299316, valid loss: 22.087827682495117\n",
      "[0552/1000] - train loss: 8.629857063293457, valid loss: 22.08139991760254\n",
      "[0553/1000] - train loss: 8.624730110168457, valid loss: 22.075088500976562\n",
      "[0554/1000] - train loss: 8.619592666625977, valid loss: 22.06896209716797\n",
      "[0555/1000] - train loss: 8.614445686340332, valid loss: 22.063194274902344\n",
      "[0556/1000] - train loss: 8.609301567077637, valid loss: 22.057645797729492\n",
      "[0557/1000] - train loss: 8.60418701171875, valid loss: 22.051929473876953\n",
      "[0558/1000] - train loss: 8.599056243896484, valid loss: 22.045703887939453\n",
      "[0559/1000] - train loss: 8.593897819519043, valid loss: 22.039173126220703\n",
      "[0560/1000] - train loss: 8.588706970214844, valid loss: 22.032958984375\n",
      "[0561/1000] - train loss: 8.5835599899292, valid loss: 22.02729034423828\n",
      "[0562/1000] - train loss: 8.578397750854492, valid loss: 22.02205467224121\n",
      "[0563/1000] - train loss: 8.573234558105469, valid loss: 22.016504287719727\n",
      "[0564/1000] - train loss: 8.568063735961914, valid loss: 22.010639190673828\n",
      "[0565/1000] - train loss: 8.562910079956055, valid loss: 22.005617141723633\n",
      "[0566/1000] - train loss: 8.55776309967041, valid loss: 22.00094985961914\n",
      "[0567/1000] - train loss: 8.552618980407715, valid loss: 21.996368408203125\n",
      "[0568/1000] - train loss: 8.547411918640137, valid loss: 21.991697311401367\n",
      "[0569/1000] - train loss: 8.54226016998291, valid loss: 21.98650360107422\n",
      "[0570/1000] - train loss: 8.537093162536621, valid loss: 21.98084259033203\n",
      "[0571/1000] - train loss: 8.531906127929688, valid loss: 21.974720001220703\n",
      "[0572/1000] - train loss: 8.52674388885498, valid loss: 21.968639373779297\n",
      "[0573/1000] - train loss: 8.521656036376953, valid loss: 21.96225357055664\n",
      "[0574/1000] - train loss: 8.516554832458496, valid loss: 21.95561981201172\n",
      "[0575/1000] - train loss: 8.511432647705078, valid loss: 21.94917106628418\n",
      "[0576/1000] - train loss: 8.506293296813965, valid loss: 21.942747116088867\n",
      "[0577/1000] - train loss: 8.5011625289917, valid loss: 21.935640335083008\n",
      "[0578/1000] - train loss: 8.496072769165039, valid loss: 21.92889976501465\n",
      "[0579/1000] - train loss: 8.490991592407227, valid loss: 21.921815872192383\n",
      "[0580/1000] - train loss: 8.4859037399292, valid loss: 21.91483497619629\n",
      "[0581/1000] - train loss: 8.480827331542969, valid loss: 21.90825843811035\n",
      "[0582/1000] - train loss: 8.475747108459473, valid loss: 21.90203285217285\n",
      "[0583/1000] - train loss: 8.470742225646973, valid loss: 21.89501953125\n",
      "[0584/1000] - train loss: 8.465717315673828, valid loss: 21.887786865234375\n",
      "[0585/1000] - train loss: 8.460714340209961, valid loss: 21.881256103515625\n",
      "[0586/1000] - train loss: 8.455738067626953, valid loss: 21.87507438659668\n",
      "[0587/1000] - train loss: 8.450740814208984, valid loss: 21.868738174438477\n",
      "[0588/1000] - train loss: 8.44579792022705, valid loss: 21.862279891967773\n",
      "[0589/1000] - train loss: 8.440857887268066, valid loss: 21.85574722290039\n",
      "[0590/1000] - train loss: 8.435917854309082, valid loss: 21.848936080932617\n",
      "[0591/1000] - train loss: 8.431007385253906, valid loss: 21.841598510742188\n",
      "[0592/1000] - train loss: 8.426115036010742, valid loss: 21.83449363708496\n",
      "[0593/1000] - train loss: 8.421183586120605, valid loss: 21.82807159423828\n",
      "[0594/1000] - train loss: 8.416332244873047, valid loss: 21.82166290283203\n",
      "[0595/1000] - train loss: 8.411458015441895, valid loss: 21.81515121459961\n",
      "[0596/1000] - train loss: 8.40659236907959, valid loss: 21.808496475219727\n",
      "[0597/1000] - train loss: 8.4017333984375, valid loss: 21.801788330078125\n",
      "[0598/1000] - train loss: 8.396889686584473, valid loss: 21.79473304748535\n",
      "[0599/1000] - train loss: 8.392045974731445, valid loss: 21.787134170532227\n",
      "[0600/1000] - train loss: 8.387225151062012, valid loss: 21.780134201049805\n",
      "[0601/1000] - train loss: 8.382412910461426, valid loss: 21.773881912231445\n",
      "[0602/1000] - train loss: 8.377643585205078, valid loss: 21.76763153076172\n",
      "[0603/1000] - train loss: 8.37288761138916, valid loss: 21.76146697998047\n",
      "[0604/1000] - train loss: 8.368131637573242, valid loss: 21.754955291748047\n",
      "[0605/1000] - train loss: 8.36337661743164, valid loss: 21.748247146606445\n",
      "[0606/1000] - train loss: 8.358675956726074, valid loss: 21.7418155670166\n",
      "[0607/1000] - train loss: 8.353959083557129, valid loss: 21.735876083374023\n",
      "[0608/1000] - train loss: 8.349249839782715, valid loss: 21.729639053344727\n",
      "[0609/1000] - train loss: 8.344608306884766, valid loss: 21.723529815673828\n",
      "[0610/1000] - train loss: 8.339950561523438, valid loss: 21.717151641845703\n",
      "[0611/1000] - train loss: 8.335248947143555, valid loss: 21.710981369018555\n",
      "[0612/1000] - train loss: 8.330617904663086, valid loss: 21.70568084716797\n",
      "[0613/1000] - train loss: 8.326018333435059, valid loss: 21.70127296447754\n",
      "[0614/1000] - train loss: 8.321395874023438, valid loss: 21.697357177734375\n",
      "[0615/1000] - train loss: 8.316839218139648, valid loss: 21.692827224731445\n",
      "[0616/1000] - train loss: 8.312332153320312, valid loss: 21.687686920166016\n",
      "[0617/1000] - train loss: 8.307812690734863, valid loss: 21.682300567626953\n",
      "[0618/1000] - train loss: 8.303373336791992, valid loss: 21.677133560180664\n",
      "[0619/1000] - train loss: 8.298920631408691, valid loss: 21.67220115661621\n",
      "[0620/1000] - train loss: 8.294466018676758, valid loss: 21.666728973388672\n",
      "[0621/1000] - train loss: 8.290033340454102, valid loss: 21.660795211791992\n",
      "[0622/1000] - train loss: 8.285602569580078, valid loss: 21.654720306396484\n",
      "[0623/1000] - train loss: 8.281228065490723, valid loss: 21.649267196655273\n",
      "[0624/1000] - train loss: 8.27682113647461, valid loss: 21.644418716430664\n",
      "[0625/1000] - train loss: 8.272385597229004, valid loss: 21.63938331604004\n",
      "[0626/1000] - train loss: 8.267991065979004, valid loss: 21.63419532775879\n",
      "[0627/1000] - train loss: 8.263605117797852, valid loss: 21.628454208374023\n",
      "[0628/1000] - train loss: 8.259230613708496, valid loss: 21.623706817626953\n",
      "[0629/1000] - train loss: 8.254892349243164, valid loss: 21.619400024414062\n",
      "[0630/1000] - train loss: 8.25055980682373, valid loss: 21.614425659179688\n",
      "[0631/1000] - train loss: 8.246209144592285, valid loss: 21.60858726501465\n",
      "[0632/1000] - train loss: 8.241861343383789, valid loss: 21.602691650390625\n",
      "[0633/1000] - train loss: 8.237574577331543, valid loss: 21.597457885742188\n",
      "[0634/1000] - train loss: 8.233259201049805, valid loss: 21.592933654785156\n",
      "[0635/1000] - train loss: 8.22885513305664, valid loss: 21.58796501159668\n",
      "[0636/1000] - train loss: 8.224470138549805, valid loss: 21.582548141479492\n",
      "[0637/1000] - train loss: 8.220084190368652, valid loss: 21.577844619750977\n",
      "[0638/1000] - train loss: 8.215688705444336, valid loss: 21.573719024658203\n",
      "[0639/1000] - train loss: 8.211344718933105, valid loss: 21.56896209716797\n",
      "[0640/1000] - train loss: 8.207018852233887, valid loss: 21.563764572143555\n",
      "[0641/1000] - train loss: 8.202703475952148, valid loss: 21.558914184570312\n",
      "[0642/1000] - train loss: 8.198383331298828, valid loss: 21.554365158081055\n",
      "[0643/1000] - train loss: 8.194083213806152, valid loss: 21.54931640625\n",
      "[0644/1000] - train loss: 8.189830780029297, valid loss: 21.543548583984375\n",
      "[0645/1000] - train loss: 8.185525894165039, valid loss: 21.53744888305664\n",
      "[0646/1000] - train loss: 8.181224822998047, valid loss: 21.531753540039062\n",
      "[0647/1000] - train loss: 8.176935195922852, valid loss: 21.52631378173828\n",
      "[0648/1000] - train loss: 8.172640800476074, valid loss: 21.52039337158203\n",
      "[0649/1000] - train loss: 8.168352127075195, valid loss: 21.51360511779785\n",
      "[0650/1000] - train loss: 8.16405963897705, valid loss: 21.506698608398438\n",
      "[0651/1000] - train loss: 8.159811973571777, valid loss: 21.5007381439209\n",
      "[0652/1000] - train loss: 8.155532836914062, valid loss: 21.495662689208984\n",
      "[0653/1000] - train loss: 8.151254653930664, valid loss: 21.49007225036621\n",
      "[0654/1000] - train loss: 8.146988868713379, valid loss: 21.483963012695312\n",
      "[0655/1000] - train loss: 8.14268970489502, valid loss: 21.478384017944336\n",
      "[0656/1000] - train loss: 8.138386726379395, valid loss: 21.47296714782715\n",
      "[0657/1000] - train loss: 8.134085655212402, valid loss: 21.46656036376953\n",
      "[0658/1000] - train loss: 8.129825592041016, valid loss: 21.45951271057129\n",
      "[0659/1000] - train loss: 8.125630378723145, valid loss: 21.453256607055664\n",
      "[0660/1000] - train loss: 8.121432304382324, valid loss: 21.44780158996582\n",
      "[0661/1000] - train loss: 8.117206573486328, valid loss: 21.44241714477539\n",
      "[0662/1000] - train loss: 8.11302661895752, valid loss: 21.436264038085938\n",
      "[0663/1000] - train loss: 8.10880184173584, valid loss: 21.429201126098633\n",
      "[0664/1000] - train loss: 8.104548454284668, valid loss: 21.42244529724121\n",
      "[0665/1000] - train loss: 8.100369453430176, valid loss: 21.416030883789062\n",
      "[0666/1000] - train loss: 8.096165657043457, valid loss: 21.409133911132812\n",
      "[0667/1000] - train loss: 8.0919828414917, valid loss: 21.402929306030273\n",
      "[0668/1000] - train loss: 8.087785720825195, valid loss: 21.39620018005371\n",
      "[0669/1000] - train loss: 8.083614349365234, valid loss: 21.39006805419922\n",
      "[0670/1000] - train loss: 8.079463958740234, valid loss: 21.383655548095703\n",
      "[0671/1000] - train loss: 8.075356483459473, valid loss: 21.377817153930664\n",
      "[0672/1000] - train loss: 8.071196556091309, valid loss: 21.372217178344727\n",
      "[0673/1000] - train loss: 8.067118644714355, valid loss: 21.366191864013672\n",
      "[0674/1000] - train loss: 8.063014030456543, valid loss: 21.359657287597656\n",
      "[0675/1000] - train loss: 8.058860778808594, valid loss: 21.3526668548584\n",
      "[0676/1000] - train loss: 8.054729461669922, valid loss: 21.346330642700195\n",
      "[0677/1000] - train loss: 8.050629615783691, valid loss: 21.340669631958008\n",
      "[0678/1000] - train loss: 8.046500205993652, valid loss: 21.33527183532715\n",
      "[0679/1000] - train loss: 8.042381286621094, valid loss: 21.32926368713379\n",
      "[0680/1000] - train loss: 8.038297653198242, valid loss: 21.322704315185547\n",
      "[0681/1000] - train loss: 8.03419303894043, valid loss: 21.316499710083008\n",
      "[0682/1000] - train loss: 8.030111312866211, valid loss: 21.309486389160156\n",
      "[0683/1000] - train loss: 8.026022911071777, valid loss: 21.303544998168945\n",
      "[0684/1000] - train loss: 8.021920204162598, valid loss: 21.297374725341797\n",
      "[0685/1000] - train loss: 8.017797470092773, valid loss: 21.291908264160156\n",
      "[0686/1000] - train loss: 8.01369571685791, valid loss: 21.285934448242188\n",
      "[0687/1000] - train loss: 8.009617805480957, valid loss: 21.280731201171875\n",
      "[0688/1000] - train loss: 8.005528450012207, valid loss: 21.27606964111328\n",
      "[0689/1000] - train loss: 8.001469612121582, valid loss: 21.270706176757812\n",
      "[0690/1000] - train loss: 7.997416973114014, valid loss: 21.264419555664062\n",
      "[0691/1000] - train loss: 7.9933600425720215, valid loss: 21.258472442626953\n",
      "[0692/1000] - train loss: 7.989265441894531, valid loss: 21.252872467041016\n",
      "[0693/1000] - train loss: 7.985189437866211, valid loss: 21.24711799621582\n",
      "[0694/1000] - train loss: 7.981144905090332, valid loss: 21.241531372070312\n",
      "[0695/1000] - train loss: 7.9770965576171875, valid loss: 21.23605728149414\n",
      "[0696/1000] - train loss: 7.97303581237793, valid loss: 21.23043441772461\n",
      "[0697/1000] - train loss: 7.968982696533203, valid loss: 21.225736618041992\n",
      "[0698/1000] - train loss: 7.964884281158447, valid loss: 21.221864700317383\n",
      "[0699/1000] - train loss: 7.9607439041137695, valid loss: 21.218685150146484\n",
      "[0700/1000] - train loss: 7.956666946411133, valid loss: 21.21491241455078\n",
      "[0701/1000] - train loss: 7.95259428024292, valid loss: 21.210203170776367\n",
      "[0702/1000] - train loss: 7.9484734535217285, valid loss: 21.204927444458008\n",
      "[0703/1000] - train loss: 7.944317817687988, valid loss: 21.199207305908203\n",
      "[0704/1000] - train loss: 7.9402337074279785, valid loss: 21.194189071655273\n",
      "[0705/1000] - train loss: 7.936146259307861, valid loss: 21.189800262451172\n",
      "[0706/1000] - train loss: 7.932032585144043, valid loss: 21.1859130859375\n",
      "[0707/1000] - train loss: 7.927879333496094, valid loss: 21.18242073059082\n",
      "[0708/1000] - train loss: 7.923810005187988, valid loss: 21.178680419921875\n",
      "[0709/1000] - train loss: 7.9197211265563965, valid loss: 21.17441177368164\n",
      "[0710/1000] - train loss: 7.9156293869018555, valid loss: 21.16938591003418\n",
      "[0711/1000] - train loss: 7.911512851715088, valid loss: 21.1636962890625\n",
      "[0712/1000] - train loss: 7.9074296951293945, valid loss: 21.157684326171875\n",
      "[0713/1000] - train loss: 7.903312683105469, valid loss: 21.152362823486328\n",
      "[0714/1000] - train loss: 7.8991289138793945, valid loss: 21.147857666015625\n",
      "[0715/1000] - train loss: 7.894901275634766, valid loss: 21.143999099731445\n",
      "[0716/1000] - train loss: 7.890635967254639, valid loss: 21.140560150146484\n",
      "[0717/1000] - train loss: 7.88641357421875, valid loss: 21.136383056640625\n",
      "[0718/1000] - train loss: 7.882204055786133, valid loss: 21.131338119506836\n",
      "[0719/1000] - train loss: 7.878002166748047, valid loss: 21.125789642333984\n",
      "[0720/1000] - train loss: 7.873764991760254, valid loss: 21.12040138244629\n",
      "[0721/1000] - train loss: 7.86952543258667, valid loss: 21.11539649963379\n",
      "[0722/1000] - train loss: 7.865331172943115, valid loss: 21.11081314086914\n",
      "[0723/1000] - train loss: 7.861108779907227, valid loss: 21.106597900390625\n",
      "[0724/1000] - train loss: 7.856919288635254, valid loss: 21.102209091186523\n",
      "[0725/1000] - train loss: 7.852880001068115, valid loss: 21.09712791442871\n",
      "[0726/1000] - train loss: 7.848788738250732, valid loss: 21.091447830200195\n",
      "[0727/1000] - train loss: 7.844717502593994, valid loss: 21.08491325378418\n",
      "[0728/1000] - train loss: 7.840728282928467, valid loss: 21.078773498535156\n",
      "[0729/1000] - train loss: 7.836714744567871, valid loss: 21.072973251342773\n",
      "[0730/1000] - train loss: 7.832711219787598, valid loss: 21.0678768157959\n",
      "[0731/1000] - train loss: 7.828767776489258, valid loss: 21.06313133239746\n",
      "[0732/1000] - train loss: 7.824835300445557, valid loss: 21.058115005493164\n",
      "[0733/1000] - train loss: 7.820863723754883, valid loss: 21.052799224853516\n",
      "[0734/1000] - train loss: 7.8168535232543945, valid loss: 21.047197341918945\n",
      "[0735/1000] - train loss: 7.812958717346191, valid loss: 21.042402267456055\n",
      "[0736/1000] - train loss: 7.808987617492676, valid loss: 21.038379669189453\n",
      "[0737/1000] - train loss: 7.805080890655518, valid loss: 21.034530639648438\n",
      "[0738/1000] - train loss: 7.8011274337768555, valid loss: 21.030759811401367\n",
      "[0739/1000] - train loss: 7.797273635864258, valid loss: 21.025758743286133\n",
      "[0740/1000] - train loss: 7.793450355529785, valid loss: 21.0196590423584\n",
      "[0741/1000] - train loss: 7.789596080780029, valid loss: 21.012845993041992\n",
      "[0742/1000] - train loss: 7.785714626312256, valid loss: 21.005718231201172\n",
      "[0743/1000] - train loss: 7.781805992126465, valid loss: 20.99930763244629\n",
      "[0744/1000] - train loss: 7.777980804443359, valid loss: 20.993669509887695\n",
      "[0745/1000] - train loss: 7.77415132522583, valid loss: 20.988393783569336\n",
      "[0746/1000] - train loss: 7.770286560058594, valid loss: 20.983455657958984\n",
      "[0747/1000] - train loss: 7.766438007354736, valid loss: 20.978769302368164\n",
      "[0748/1000] - train loss: 7.762697219848633, valid loss: 20.9731388092041\n",
      "[0749/1000] - train loss: 7.758949279785156, valid loss: 20.966995239257812\n",
      "[0750/1000] - train loss: 7.755171775817871, valid loss: 20.96072769165039\n",
      "[0751/1000] - train loss: 7.751363277435303, valid loss: 20.954639434814453\n",
      "[0752/1000] - train loss: 7.747561454772949, valid loss: 20.9493350982666\n",
      "[0753/1000] - train loss: 7.743823528289795, valid loss: 20.944780349731445\n",
      "[0754/1000] - train loss: 7.740142345428467, valid loss: 20.940521240234375\n",
      "[0755/1000] - train loss: 7.736298561096191, valid loss: 20.936290740966797\n",
      "[0756/1000] - train loss: 7.732521057128906, valid loss: 20.93233299255371\n",
      "[0757/1000] - train loss: 7.728808879852295, valid loss: 20.927574157714844\n",
      "[0758/1000] - train loss: 7.725074291229248, valid loss: 20.922346115112305\n",
      "[0759/1000] - train loss: 7.721317768096924, valid loss: 20.916128158569336\n",
      "[0760/1000] - train loss: 7.717529296875, valid loss: 20.910184860229492\n",
      "[0761/1000] - train loss: 7.713805675506592, valid loss: 20.904834747314453\n",
      "[0762/1000] - train loss: 7.710096836090088, valid loss: 20.90024185180664\n",
      "[0763/1000] - train loss: 7.706380367279053, valid loss: 20.89636993408203\n",
      "[0764/1000] - train loss: 7.702683448791504, valid loss: 20.891712188720703\n",
      "[0765/1000] - train loss: 7.6989850997924805, valid loss: 20.886316299438477\n",
      "[0766/1000] - train loss: 7.695310115814209, valid loss: 20.881114959716797\n",
      "[0767/1000] - train loss: 7.691652774810791, valid loss: 20.876333236694336\n",
      "[0768/1000] - train loss: 7.6879377365112305, valid loss: 20.872251510620117\n",
      "[0769/1000] - train loss: 7.684229850769043, valid loss: 20.867469787597656\n",
      "[0770/1000] - train loss: 7.680738925933838, valid loss: 20.862838745117188\n",
      "[0771/1000] - train loss: 7.677178382873535, valid loss: 20.858627319335938\n",
      "[0772/1000] - train loss: 7.673577785491943, valid loss: 20.855382919311523\n",
      "[0773/1000] - train loss: 7.669957637786865, valid loss: 20.853025436401367\n",
      "[0774/1000] - train loss: 7.666266918182373, valid loss: 20.851102828979492\n",
      "[0775/1000] - train loss: 7.662527561187744, valid loss: 20.849130630493164\n",
      "[0776/1000] - train loss: 7.658890247344971, valid loss: 20.845975875854492\n",
      "[0777/1000] - train loss: 7.655296802520752, valid loss: 20.842876434326172\n",
      "[0778/1000] - train loss: 7.651696681976318, valid loss: 20.840312957763672\n",
      "[0779/1000] - train loss: 7.648093223571777, valid loss: 20.837608337402344\n",
      "[0780/1000] - train loss: 7.644486904144287, valid loss: 20.83391761779785\n",
      "[0781/1000] - train loss: 7.640900611877441, valid loss: 20.83051300048828\n",
      "[0782/1000] - train loss: 7.637253761291504, valid loss: 20.8264102935791\n",
      "[0783/1000] - train loss: 7.633670806884766, valid loss: 20.823076248168945\n",
      "[0784/1000] - train loss: 7.6300530433654785, valid loss: 20.820337295532227\n",
      "[0785/1000] - train loss: 7.626460075378418, valid loss: 20.81622886657715\n",
      "[0786/1000] - train loss: 7.6228532791137695, valid loss: 20.811203002929688\n",
      "[0787/1000] - train loss: 7.619231224060059, valid loss: 20.80658721923828\n",
      "[0788/1000] - train loss: 7.615633010864258, valid loss: 20.801992416381836\n",
      "[0789/1000] - train loss: 7.612049102783203, valid loss: 20.797740936279297\n",
      "[0790/1000] - train loss: 7.608520984649658, valid loss: 20.79431915283203\n",
      "[0791/1000] - train loss: 7.604994773864746, valid loss: 20.79071617126465\n",
      "[0792/1000] - train loss: 7.601413726806641, valid loss: 20.78696632385254\n",
      "[0793/1000] - train loss: 7.597798824310303, valid loss: 20.783920288085938\n",
      "[0794/1000] - train loss: 7.5942864418029785, valid loss: 20.781391143798828\n",
      "[0795/1000] - train loss: 7.590888023376465, valid loss: 20.77811622619629\n",
      "[0796/1000] - train loss: 7.587291240692139, valid loss: 20.773345947265625\n",
      "[0797/1000] - train loss: 7.583800792694092, valid loss: 20.767343521118164\n",
      "[0798/1000] - train loss: 7.5803446769714355, valid loss: 20.7612361907959\n",
      "[0799/1000] - train loss: 7.576897144317627, valid loss: 20.755708694458008\n",
      "[0800/1000] - train loss: 7.573416233062744, valid loss: 20.750320434570312\n",
      "[0801/1000] - train loss: 7.569939136505127, valid loss: 20.74544906616211\n",
      "[0802/1000] - train loss: 7.566476345062256, valid loss: 20.740205764770508\n",
      "[0803/1000] - train loss: 7.562995433807373, valid loss: 20.73430061340332\n",
      "[0804/1000] - train loss: 7.55950927734375, valid loss: 20.72925567626953\n",
      "[0805/1000] - train loss: 7.556097984313965, valid loss: 20.724550247192383\n",
      "[0806/1000] - train loss: 7.552652359008789, valid loss: 20.72026252746582\n",
      "[0807/1000] - train loss: 7.5492658615112305, valid loss: 20.71550750732422\n",
      "[0808/1000] - train loss: 7.5458855628967285, valid loss: 20.709779739379883\n",
      "[0809/1000] - train loss: 7.542504787445068, valid loss: 20.702831268310547\n",
      "[0810/1000] - train loss: 7.53908109664917, valid loss: 20.6960506439209\n",
      "[0811/1000] - train loss: 7.535693645477295, valid loss: 20.68977928161621\n",
      "[0812/1000] - train loss: 7.5323076248168945, valid loss: 20.684328079223633\n",
      "[0813/1000] - train loss: 7.528899192810059, valid loss: 20.679622650146484\n",
      "[0814/1000] - train loss: 7.5254974365234375, valid loss: 20.674697875976562\n",
      "[0815/1000] - train loss: 7.5220770835876465, valid loss: 20.669828414916992\n",
      "[0816/1000] - train loss: 7.518706321716309, valid loss: 20.665576934814453\n",
      "[0817/1000] - train loss: 7.515480041503906, valid loss: 20.660886764526367\n",
      "[0818/1000] - train loss: 7.512034893035889, valid loss: 20.6558837890625\n",
      "[0819/1000] - train loss: 7.508683204650879, valid loss: 20.650972366333008\n",
      "[0820/1000] - train loss: 7.50538969039917, valid loss: 20.64527130126953\n",
      "[0821/1000] - train loss: 7.502058506011963, valid loss: 20.63886260986328\n",
      "[0822/1000] - train loss: 7.49871301651001, valid loss: 20.63241195678711\n",
      "[0823/1000] - train loss: 7.495395183563232, valid loss: 20.62711524963379\n",
      "[0824/1000] - train loss: 7.492105484008789, valid loss: 20.623071670532227\n",
      "[0825/1000] - train loss: 7.4887919425964355, valid loss: 20.619796752929688\n",
      "[0826/1000] - train loss: 7.485468864440918, valid loss: 20.616958618164062\n",
      "[0827/1000] - train loss: 7.482130527496338, valid loss: 20.614351272583008\n",
      "[0828/1000] - train loss: 7.478809356689453, valid loss: 20.6105899810791\n",
      "[0829/1000] - train loss: 7.475475311279297, valid loss: 20.60608673095703\n",
      "[0830/1000] - train loss: 7.472145080566406, valid loss: 20.601438522338867\n",
      "[0831/1000] - train loss: 7.46881628036499, valid loss: 20.59667205810547\n",
      "[0832/1000] - train loss: 7.465522766113281, valid loss: 20.591365814208984\n",
      "[0833/1000] - train loss: 7.46217155456543, valid loss: 20.585927963256836\n",
      "[0834/1000] - train loss: 7.458827972412109, valid loss: 20.58078956604004\n",
      "[0835/1000] - train loss: 7.455530643463135, valid loss: 20.575908660888672\n",
      "[0836/1000] - train loss: 7.452218532562256, valid loss: 20.57120132446289\n",
      "[0837/1000] - train loss: 7.448914051055908, valid loss: 20.56720542907715\n",
      "[0838/1000] - train loss: 7.445664882659912, valid loss: 20.562902450561523\n",
      "[0839/1000] - train loss: 7.442305088043213, valid loss: 20.558124542236328\n",
      "[0840/1000] - train loss: 7.438997268676758, valid loss: 20.553863525390625\n",
      "[0841/1000] - train loss: 7.435714244842529, valid loss: 20.54966163635254\n",
      "[0842/1000] - train loss: 7.4325151443481445, valid loss: 20.5461483001709\n",
      "[0843/1000] - train loss: 7.429240703582764, valid loss: 20.543249130249023\n",
      "[0844/1000] - train loss: 7.425943851470947, valid loss: 20.539892196655273\n",
      "[0845/1000] - train loss: 7.4226460456848145, valid loss: 20.535675048828125\n",
      "[0846/1000] - train loss: 7.419368267059326, valid loss: 20.53067398071289\n",
      "[0847/1000] - train loss: 7.41615629196167, valid loss: 20.5255126953125\n",
      "[0848/1000] - train loss: 7.4129228591918945, valid loss: 20.520191192626953\n",
      "[0849/1000] - train loss: 7.409636497497559, valid loss: 20.51552391052246\n",
      "[0850/1000] - train loss: 7.4063720703125, valid loss: 20.511449813842773\n",
      "[0851/1000] - train loss: 7.403088092803955, valid loss: 20.507200241088867\n",
      "[0852/1000] - train loss: 7.399825096130371, valid loss: 20.502405166625977\n",
      "[0853/1000] - train loss: 7.396631717681885, valid loss: 20.498384475708008\n",
      "[0854/1000] - train loss: 7.393381118774414, valid loss: 20.495153427124023\n",
      "[0855/1000] - train loss: 7.390054702758789, valid loss: 20.491104125976562\n",
      "[0856/1000] - train loss: 7.386900424957275, valid loss: 20.486797332763672\n",
      "[0857/1000] - train loss: 7.383559226989746, valid loss: 20.4818058013916\n",
      "[0858/1000] - train loss: 7.38029146194458, valid loss: 20.477611541748047\n",
      "[0859/1000] - train loss: 7.37705659866333, valid loss: 20.473716735839844\n",
      "[0860/1000] - train loss: 7.373776435852051, valid loss: 20.469463348388672\n",
      "[0861/1000] - train loss: 7.3705315589904785, valid loss: 20.46547508239746\n",
      "[0862/1000] - train loss: 7.367243766784668, valid loss: 20.461950302124023\n",
      "[0863/1000] - train loss: 7.363936901092529, valid loss: 20.4580020904541\n",
      "[0864/1000] - train loss: 7.360564708709717, valid loss: 20.454036712646484\n",
      "[0865/1000] - train loss: 7.357273101806641, valid loss: 20.44930076599121\n",
      "[0866/1000] - train loss: 7.3539276123046875, valid loss: 20.4442195892334\n",
      "[0867/1000] - train loss: 7.350548267364502, valid loss: 20.439180374145508\n",
      "[0868/1000] - train loss: 7.347205638885498, valid loss: 20.43411636352539\n",
      "[0869/1000] - train loss: 7.343844413757324, valid loss: 20.428760528564453\n",
      "[0870/1000] - train loss: 7.340494155883789, valid loss: 20.424053192138672\n",
      "[0871/1000] - train loss: 7.337151050567627, valid loss: 20.418441772460938\n",
      "[0872/1000] - train loss: 7.333805561065674, valid loss: 20.413358688354492\n",
      "[0873/1000] - train loss: 7.330463886260986, valid loss: 20.40781593322754\n",
      "[0874/1000] - train loss: 7.327113151550293, valid loss: 20.402727127075195\n",
      "[0875/1000] - train loss: 7.323781967163086, valid loss: 20.398365020751953\n",
      "[0876/1000] - train loss: 7.320467948913574, valid loss: 20.3934383392334\n",
      "[0877/1000] - train loss: 7.317146301269531, valid loss: 20.387466430664062\n",
      "[0878/1000] - train loss: 7.3138556480407715, valid loss: 20.38155746459961\n",
      "[0879/1000] - train loss: 7.310652256011963, valid loss: 20.375995635986328\n",
      "[0880/1000] - train loss: 7.307356834411621, valid loss: 20.370664596557617\n",
      "[0881/1000] - train loss: 7.304080963134766, valid loss: 20.36518096923828\n",
      "[0882/1000] - train loss: 7.300793170928955, valid loss: 20.359739303588867\n",
      "[0883/1000] - train loss: 7.297560691833496, valid loss: 20.35519027709961\n",
      "[0884/1000] - train loss: 7.294344902038574, valid loss: 20.350112915039062\n",
      "[0885/1000] - train loss: 7.291068077087402, valid loss: 20.34467315673828\n",
      "[0886/1000] - train loss: 7.2878313064575195, valid loss: 20.338207244873047\n",
      "[0887/1000] - train loss: 7.284545421600342, valid loss: 20.330978393554688\n",
      "[0888/1000] - train loss: 7.281290531158447, valid loss: 20.324127197265625\n",
      "[0889/1000] - train loss: 7.278080940246582, valid loss: 20.318086624145508\n",
      "[0890/1000] - train loss: 7.2749199867248535, valid loss: 20.31269645690918\n",
      "[0891/1000] - train loss: 7.271618366241455, valid loss: 20.307815551757812\n",
      "[0892/1000] - train loss: 7.268379211425781, valid loss: 20.30316925048828\n",
      "[0893/1000] - train loss: 7.265129566192627, valid loss: 20.298707962036133\n",
      "[0894/1000] - train loss: 7.261872291564941, valid loss: 20.293420791625977\n",
      "[0895/1000] - train loss: 7.258664608001709, valid loss: 20.287879943847656\n",
      "[0896/1000] - train loss: 7.255470275878906, valid loss: 20.283693313598633\n",
      "[0897/1000] - train loss: 7.2522454261779785, valid loss: 20.280330657958984\n",
      "[0898/1000] - train loss: 7.249094009399414, valid loss: 20.276573181152344\n",
      "[0899/1000] - train loss: 7.245845794677734, valid loss: 20.271617889404297\n",
      "[0900/1000] - train loss: 7.242666244506836, valid loss: 20.266855239868164\n",
      "[0901/1000] - train loss: 7.239497184753418, valid loss: 20.263080596923828\n",
      "[0902/1000] - train loss: 7.236332416534424, valid loss: 20.258773803710938\n",
      "[0903/1000] - train loss: 7.233130931854248, valid loss: 20.253997802734375\n",
      "[0904/1000] - train loss: 7.229890823364258, valid loss: 20.250185012817383\n",
      "[0905/1000] - train loss: 7.226644515991211, valid loss: 20.2467098236084\n",
      "[0906/1000] - train loss: 7.223400592803955, valid loss: 20.242563247680664\n",
      "[0907/1000] - train loss: 7.220116138458252, valid loss: 20.238664627075195\n",
      "[0908/1000] - train loss: 7.216804027557373, valid loss: 20.23573875427246\n",
      "[0909/1000] - train loss: 7.213456153869629, valid loss: 20.233335494995117\n",
      "[0910/1000] - train loss: 7.210096836090088, valid loss: 20.231565475463867\n",
      "[0911/1000] - train loss: 7.2067670822143555, valid loss: 20.229530334472656\n",
      "[0912/1000] - train loss: 7.203405380249023, valid loss: 20.22659683227539\n",
      "[0913/1000] - train loss: 7.200009822845459, valid loss: 20.22280502319336\n",
      "[0914/1000] - train loss: 7.196659564971924, valid loss: 20.21936798095703\n",
      "[0915/1000] - train loss: 7.193360805511475, valid loss: 20.216339111328125\n",
      "[0916/1000] - train loss: 7.189960956573486, valid loss: 20.214277267456055\n",
      "[0917/1000] - train loss: 7.186628818511963, valid loss: 20.211334228515625\n",
      "[0918/1000] - train loss: 7.183313369750977, valid loss: 20.20758628845215\n",
      "[0919/1000] - train loss: 7.179965972900391, valid loss: 20.203075408935547\n",
      "[0920/1000] - train loss: 7.176610469818115, valid loss: 20.19878578186035\n",
      "[0921/1000] - train loss: 7.173294544219971, valid loss: 20.19523048400879\n",
      "[0922/1000] - train loss: 7.169941425323486, valid loss: 20.192323684692383\n",
      "[0923/1000] - train loss: 7.166598320007324, valid loss: 20.18913459777832\n",
      "[0924/1000] - train loss: 7.16328763961792, valid loss: 20.185333251953125\n",
      "[0925/1000] - train loss: 7.159984588623047, valid loss: 20.182506561279297\n",
      "[0926/1000] - train loss: 7.1566596031188965, valid loss: 20.17999839782715\n",
      "[0927/1000] - train loss: 7.153328895568848, valid loss: 20.177480697631836\n",
      "[0928/1000] - train loss: 7.149968147277832, valid loss: 20.17404556274414\n",
      "[0929/1000] - train loss: 7.146547317504883, valid loss: 20.16972541809082\n",
      "[0930/1000] - train loss: 7.143118381500244, valid loss: 20.166046142578125\n",
      "[0931/1000] - train loss: 7.1397318840026855, valid loss: 20.16338539123535\n",
      "[0932/1000] - train loss: 7.136344909667969, valid loss: 20.16147804260254\n",
      "[0933/1000] - train loss: 7.132882118225098, valid loss: 20.15938949584961\n",
      "[0934/1000] - train loss: 7.129542350769043, valid loss: 20.15604591369629\n",
      "[0935/1000] - train loss: 7.1261420249938965, valid loss: 20.151744842529297\n",
      "[0936/1000] - train loss: 7.122687816619873, valid loss: 20.146589279174805\n",
      "[0937/1000] - train loss: 7.119287967681885, valid loss: 20.141826629638672\n",
      "[0938/1000] - train loss: 7.1158952713012695, valid loss: 20.13771629333496\n",
      "[0939/1000] - train loss: 7.1125006675720215, valid loss: 20.133569717407227\n",
      "[0940/1000] - train loss: 7.109072208404541, valid loss: 20.13002586364746\n",
      "[0941/1000] - train loss: 7.10565710067749, valid loss: 20.12663459777832\n",
      "[0942/1000] - train loss: 7.10225248336792, valid loss: 20.122726440429688\n",
      "[0943/1000] - train loss: 7.098878383636475, valid loss: 20.118305206298828\n",
      "[0944/1000] - train loss: 7.09547233581543, valid loss: 20.112998962402344\n",
      "[0945/1000] - train loss: 7.09203577041626, valid loss: 20.108198165893555\n",
      "[0946/1000] - train loss: 7.088672161102295, valid loss: 20.104379653930664\n",
      "[0947/1000] - train loss: 7.085270881652832, valid loss: 20.099994659423828\n",
      "[0948/1000] - train loss: 7.081933975219727, valid loss: 20.095600128173828\n",
      "[0949/1000] - train loss: 7.0786027908325195, valid loss: 20.091161727905273\n",
      "[0950/1000] - train loss: 7.075296878814697, valid loss: 20.08556365966797\n",
      "[0951/1000] - train loss: 7.072077751159668, valid loss: 20.079383850097656\n",
      "[0952/1000] - train loss: 7.068672180175781, valid loss: 20.074050903320312\n",
      "[0953/1000] - train loss: 7.065446376800537, valid loss: 20.069665908813477\n",
      "[0954/1000] - train loss: 7.062246322631836, valid loss: 20.065004348754883\n",
      "[0955/1000] - train loss: 7.058940410614014, valid loss: 20.061132431030273\n",
      "[0956/1000] - train loss: 7.05552339553833, valid loss: 20.056867599487305\n",
      "[0957/1000] - train loss: 7.052214622497559, valid loss: 20.052732467651367\n",
      "[0958/1000] - train loss: 7.048933506011963, valid loss: 20.047658920288086\n",
      "[0959/1000] - train loss: 7.04583215713501, valid loss: 20.042768478393555\n",
      "[0960/1000] - train loss: 7.042559623718262, valid loss: 20.036972045898438\n",
      "[0961/1000] - train loss: 7.039218425750732, valid loss: 20.031641006469727\n",
      "[0962/1000] - train loss: 7.035999298095703, valid loss: 20.027372360229492\n",
      "[0963/1000] - train loss: 7.032783031463623, valid loss: 20.02290153503418\n",
      "[0964/1000] - train loss: 7.029542922973633, valid loss: 20.017589569091797\n",
      "[0965/1000] - train loss: 7.026347637176514, valid loss: 20.013200759887695\n",
      "[0966/1000] - train loss: 7.023153305053711, valid loss: 20.00965690612793\n",
      "[0967/1000] - train loss: 7.019917011260986, valid loss: 20.006256103515625\n",
      "[0968/1000] - train loss: 7.016743183135986, valid loss: 20.001909255981445\n",
      "[0969/1000] - train loss: 7.01353645324707, valid loss: 19.9967098236084\n",
      "[0970/1000] - train loss: 7.010316848754883, valid loss: 19.99125099182129\n",
      "[0971/1000] - train loss: 7.007170677185059, valid loss: 19.98649024963379\n",
      "[0972/1000] - train loss: 7.004005432128906, valid loss: 19.982805252075195\n",
      "[0973/1000] - train loss: 7.000816822052002, valid loss: 19.979904174804688\n",
      "[0974/1000] - train loss: 6.997603893280029, valid loss: 19.977087020874023\n",
      "[0975/1000] - train loss: 6.994592666625977, valid loss: 19.97316551208496\n",
      "[0976/1000] - train loss: 6.991398334503174, valid loss: 19.9681339263916\n",
      "[0977/1000] - train loss: 6.9882307052612305, valid loss: 19.962366104125977\n",
      "[0978/1000] - train loss: 6.985100746154785, valid loss: 19.956418991088867\n",
      "[0979/1000] - train loss: 6.981997489929199, valid loss: 19.95194435119629\n",
      "[0980/1000] - train loss: 6.978886127471924, valid loss: 19.94777488708496\n",
      "[0981/1000] - train loss: 6.975793361663818, valid loss: 19.944665908813477\n",
      "[0982/1000] - train loss: 6.9726738929748535, valid loss: 19.941917419433594\n",
      "[0983/1000] - train loss: 6.969509124755859, valid loss: 19.938365936279297\n",
      "[0984/1000] - train loss: 6.966348648071289, valid loss: 19.9340763092041\n",
      "[0985/1000] - train loss: 6.9631452560424805, valid loss: 19.929550170898438\n",
      "[0986/1000] - train loss: 6.959960460662842, valid loss: 19.92518424987793\n",
      "[0987/1000] - train loss: 6.956761360168457, valid loss: 19.92166519165039\n",
      "[0988/1000] - train loss: 6.953567981719971, valid loss: 19.918867111206055\n",
      "[0989/1000] - train loss: 6.950352668762207, valid loss: 19.915254592895508\n",
      "[0990/1000] - train loss: 6.947145938873291, valid loss: 19.911231994628906\n",
      "[0991/1000] - train loss: 6.943924903869629, valid loss: 19.9073486328125\n",
      "[0992/1000] - train loss: 6.940845012664795, valid loss: 19.90279197692871\n",
      "[0993/1000] - train loss: 6.937612533569336, valid loss: 19.8986873626709\n",
      "[0994/1000] - train loss: 6.934457302093506, valid loss: 19.89541244506836\n",
      "[0995/1000] - train loss: 6.931373596191406, valid loss: 19.89121437072754\n",
      "[0996/1000] - train loss: 6.928208351135254, valid loss: 19.88646125793457\n",
      "[0997/1000] - train loss: 6.925037384033203, valid loss: 19.88174819946289\n",
      "[0998/1000] - train loss: 6.92191743850708, valid loss: 19.877254486083984\n",
      "[0999/1000] - train loss: 6.9187822341918945, valid loss: 19.87151527404785\n",
      "[1000/1000] - train loss: 6.915660381317139, valid loss: 19.8660888671875\n",
      "총학습에 걸린시간(초): 3.621999979019165\n"
     ]
    }
   ],
   "source": [
    "#################### Train + Evaluation (epoch)\n",
    "import time\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):   # tqdm 라이브러리 : progress bar 생성.\n",
    "    ###############################################################\n",
    "    # Train\n",
    "    ###############################################################\n",
    "    # 1. 모델을 Train 모드로 변경\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # 2. Batch 단위로 학습 - 반복문.\n",
    "    for X_train, y_train in train_loader:\n",
    "\n",
    "        # 3. X, y 를 device 로 이동.\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        # 4. 추론\n",
    "        pred = boston_model(X_train)   # model.forward(X_train) 호출됨.\n",
    "\n",
    "        # 5. loss (오차) 계산\n",
    "        loss = loss_fn(pred, y_train)  # 함수에 입력해야할 값들은 다음과 같다. -> (추론한 값, 정답)\n",
    "\n",
    "        # 6. 파라미터들의 gradient 계산\n",
    "        loss. backward()\n",
    "\n",
    "        # 7. 파라미터들 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 8. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## 로그용 - Loss 저장\n",
    "        train_loss +=loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)  # train loss의 step별 평균을 계산 -> 1 epoch의 train loss\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # Evaluation\n",
    "    ###############################################################\n",
    "    # 1. eval 모드  / 학습과 추론에서의 방식이 다름을 지정해줌. \n",
    "    boston_model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # 2. gradient functoin 구하지 않도록 처리\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in test_loader: # step 단위로 평가\n",
    "\n",
    "            # 3. X, y를 device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "\n",
    "            # 4. 추론\n",
    "            pred_valid = boston_model(X_valid)\n",
    "\n",
    "            # 5. 검증(평가) - MSE\n",
    "            loss_valid = loss_fn(pred_valid, y_valid)\n",
    "            valid_loss += loss_valid.item()\n",
    "\n",
    "        valid_loss /= len(test_loader)  # 평균계산.\n",
    "\n",
    "    ## loss 계산한 것들을 list 에 추가. 로그 출력\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f\"[{epoch+1:04d}/{epochs}] - train loss: {train_loss}, valid loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "print(\"총학습에 걸린시간(초):\", (e-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 시각화\n",
    "plt.plot(range(epochs), train_losses, label=\"train set\")\n",
    "plt.plot(range(epochs), valid_losses, label=\"valid set\")\n",
    "plt.title(\"학습 loss\")\n",
    "# plt.ylim(10, 50)\n",
    "# plt.xlim(800, 2000)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최종 모델 저장.\n",
    "save_path = \"saved_models/boston_model.pth\"\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_16464\\1837872702.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(save_path)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonHousingModeling                    [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 16]                  224\n",
       "├─ReLU: 1-2                              [10, 16]                  --\n",
       "├─Linear: 1-3                            [10, 8]                   136\n",
       "├─ReLU: 1-4                              [10, 8]                   --\n",
       "├─Linear: 1-5                            [10, 1]                   9\n",
       "==========================================================================================\n",
       "Total params: 369\n",
       "Trainable params: 369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (10, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  application 에서 새로운 데이터 추론.\n",
    "new_data = testset[10][0]\n",
    "new_data = new_data.reshape(1, -1)  # batch 축을 늘려줌\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예상집값: tensor([[10.5539]])\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad(): # gradient 함수 만들 필요 없음을 작성해줌.\n",
    "    y_pred = model(new_data)\n",
    "    print(\"예상집값:\", y_pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset load\n",
    "\n",
    "data_root = \"datasets/fashion_mnist\"\n",
    "trainset = FashionMNIST(root=data_root, train=True, download=True)\n",
    "testset = FashionMNIST(root=data_root, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train을 분리해서 train / valid set 으로 나누리\n",
    "trainset, validset = random_split(trainset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(validset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### class 들 확인\n",
    "testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input 이미지를 확인.\n",
    "idx = 159\n",
    "x, y = testset[idx]\n",
    "# x, y\n",
    "plt.figure(figsize=(3, 3))\n",
    "# plt.imshow(x, cmap='gray')   # gray :   0(최소값) -black , 255(최대값) - white\n",
    "plt.imshow(x, cmap='Greys')   # greys :  0 - white,    255 - black\n",
    "plt.title(f\"{y} - {testset.classes[y]}\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APB7Czl1DUbayhGZbiVYk/3mIA/U19kJ4I8KR20FsPDukyR2vyx77VGKkDnJIJOepz1PNeNfH3w3pti+l6xpdnbwGSSW2vGgAUGQbWXKjjdgtk+wrxSlBwQRX1/8K4JYfhjoQnkaR5IHlLMck75HYfoa8H+Olk1p8T7yUni7t4JwPT5Ah/VK83or7Q8AbT8PvDe3p/ZkA/Hyxn9a8J/aGTb49sHxw+mR/pJJXktFfRnwP8YanqGgyaTdeTJDpq7IHIO/b1Ck5wQM4HHSvI/id4gvvEPjvUZL5kxZyNaQJGuFWNGIH4k5JPqfTArj6//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABd0lEQVR4AW1STU/CQBB9s91CagIRCGpijBjjwYvEi3LxYqLx5C8w0d/ibzEejDf/hBcP/gG/iBKRClEQQ+mOU+m2BNg07fS9mfd2ZhdI1umhhKXz3QRIg2q9H/DVI/8MblPQRg/89jF47bQa73xpMbLBcyEYuoX+t0auW4xBHX8ri31yAt8Q0ZBtgYqDI62Nm4V2nQzyWzFoyVpIyhiHzFyrrc8myO3MU0szc6jvrrEek3ZDSwf1GxoQh7nG2kJzgpRf/iSF0M3ZfMB6anhfwpMKu1k4E56MUpYJDJ7bkPdo2UpG1RHMIaidmEplgc1ITPrBLLIsRWCRXoWZkl0RXAoVl6dkGctG9iOe6WiTVoDiUFqRJyxMVQJe1IlSMN4MMhN1p4iQmUG6Iywcc7JDAF6imTKInbwtTUlBpE85UXd+muz+54kx2RmMGXhaps6KDNl7ZQUkaY/bnVbQb5r7BBwLLph7wfDXryRYeuzA/kkt5/eO/YT8A7/bfsGvXR44AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset load\n",
    "## transform 지정: 전처리 로직추가\n",
    "\n",
    "data_root = \"datasets/fashion_mnist\"\n",
    "trainset = FashionMNIST(root=data_root, train=True, download=True, transform=ToTensor())\n",
    "testset = FashionMNIST(root=data_root, train=False, download=True, transform=ToTensor())\n",
    "trainset, validset = random_split(trainset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(validset, batch_size=256)\n",
    "test_loader = DataLoader(testset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 40, 40, torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(valid_loader), len(test_loader), trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch 돌때의 step 수? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 클래스 구현\n",
    "class FashionMNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(784, 1024)  # 첫번째 Linear - in_features = 입력 feature 개수( 1 * 28 * 28) => 784\n",
    "        self.lr2 = nn.Linear(1024, 512)  # 특성 추출 갯수를? 줄여나감\n",
    "        self.lr3 = nn.Linear(512, 256)\n",
    "        self.lr4 = nn.Linear(256, 128)\n",
    "        self.lr5 = nn.Linear(128, 64)\n",
    "        self.lr6 = nn.Linear(64, 10)  # 마지막 Linear - out_features = 출력 결과 개수에 맞춰주면 된다(다중분류: class 개수)\n",
    "        # hidden layer 에 적용할 activation 함수                       # class 별 정답일 확률(을 계산할떄 사용할 값.)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # torch.flatten(X, start_dim=1)   # (256, 1, 28, 28) -> 256 * 1 * 28 * 28  (256[0]축은 유지한채로, 1번부터 시작설정)\n",
    "                                           # start_dim=1 -> (256, 1 x 28 x 28)\n",
    "        X = nn.Flatten()(X)  # start_dim=1 해서 flatten 처리하는 Layer 클래스   \n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "       \n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr3(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr4(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.lr5(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        output = self.lr6(X)  # 출력 결과 보는 함수에서는 relu  함수 적용하지 않음.\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (lr2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lr6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "## 모델 생성\n",
    "f_model = FashionMNISTModel().to(device)\n",
    "f_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [256, 10]                 --\n",
       "├─Linear: 1-1                            [256, 1024]               803,840\n",
       "├─ReLU: 1-2                              [256, 1024]               --\n",
       "├─Linear: 1-3                            [256, 512]                524,800\n",
       "├─ReLU: 1-4                              [256, 512]                --\n",
       "├─Linear: 1-5                            [256, 256]                131,328\n",
       "├─ReLU: 1-6                              [256, 256]                --\n",
       "├─Linear: 1-7                            [256, 128]                32,896\n",
       "├─ReLU: 1-8                              [256, 128]                --\n",
       "├─Linear: 1-9                            [256, 64]                 8,256\n",
       "├─ReLU: 1-10                             [256, 64]                 --\n",
       "├─Linear: 1-11                           [256, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 1,501,770\n",
       "Trainable params: 1,501,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 384.45\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 4.08\n",
       "Params size (MB): 6.01\n",
       "Estimated Total Size (MB): 10.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(f_model, (256, 1, 28, 28), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장.\n",
    "- 학습 도중 가장 좋은 성능의 모델이 나올 수 있다.\n",
    " - 모델을 저장할 경우 에폭 단위로 저장.\n",
    "    1. 모든 에폭이 끝나고 모델을 저장.\n",
    "    2. 가장 성능이 좋은 시점의 모델을 저장\n",
    "        -  best score 의 현재 epochdml 성능을 비교해서 성능 개선이 있으면 모델을 (덮어쓰기로) 저장한다.\n",
    "\n",
    "### 조기종료 (Early stopping)\n",
    "- 학습 도중 성능개선이 없으면 중간 epoch에서 학습을 멈추도록 한다.\n",
    "- epoch을 길게 잡아주고 성능이 개선될때 마다 저장, 특정 횟수의 에폭동안 성능 개선이 없으면 조기 종료하도록 구현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "f_model = FashionMNISTModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(f_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "### optimizer, loss 함수\n",
    "### optimizer\n",
    "\n",
    "optimizer = optim.Adam(f_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "## CrossEntropyLoss() - 다중 분류 학습시 사용하는 loss 함수. (categorical crossentropy)\n",
    "### (모델예측값, 정갑)\n",
    "#### 1. 모델 예측값을 softmax()함수를 이용해서 확률값을 반환\n",
    "#### 2. 정답은 ont hot encoding 처리\n",
    "#### 3. 1,2에서 변환한 값으로 loss를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] - train loss: 0.37227860719729694, valid loss: 0.35985728111118076, valid_acc: 0.8646\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Replacement index 3 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# log 출력\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     save_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>>>>>>>>> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Epoch에서 모델 저장: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m에서 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m로 loss가 개선되어 저장함.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msave_log\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loss\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     97\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m valid_loss  \u001b[38;5;66;03m# best score 를 현재 에폭의 valid loss 로 변경.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 성능이 개선되었으므로  trigger_cnt 를 0 으로 초기화.\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Replacement index 3 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "import time \n",
    "\n",
    "######################################\n",
    "# epoch 도중 성능이 개선되면 모델을 저장.\n",
    "# 조기 종료\n",
    "######################################\n",
    "save_path = \"saved_models/fashion_mnist_model.pt\"\n",
    "## 현 시점에 가장 좋은 지표를 저장할 변수.\n",
    "best_score = torch.inf     # validation loss기준으로 저장 여부를 확인.  loss가 낮아지면 성능이 개선됨.\n",
    "\n",
    "# 조기 종료 관련 변수\n",
    "patience = 5     # 성능이 개선되는지를 몇 epoch동안 기다려 볼 것인지 지정. (보통 5 ~ 10 epoch으로 지정함)\n",
    "tirgger_cnt = 0  # 성능이 개선되는지 몇 epoch 을 기다리고 있는지 저장할 변수. (patience == triggger_cnt   값이 같으면 다 수행하였으므로-> 조기종료)\n",
    "#####################################\n",
    "\n",
    "\n",
    "# epoch 별 검증결과를 저장할 리스트\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_acc_list = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ###################### Train - train_loader ######################  \n",
    "    f_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 train loss를 저항할 변수.\n",
    "    for X_train, y_train in train_loader:\n",
    "        # 1. device로 이동\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        \n",
    "        # 2. 추론\n",
    "        pred = f_model(X_train)  # f_model.forward(X_train) 호출 \n",
    "\n",
    "        # 3. loss\n",
    "        loss = loss_fn(pred, y_train)\n",
    "\n",
    "        # 4. gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. parameters 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 6. 파라미터의 gradient값 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loss 값을 train_loss 변수에 누적(저장)\n",
    "        train_loss = train_loss + loss.item()   # train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader) # 현재 epoch의 평균 train loss 를 계산.\n",
    "    \n",
    "    ## loss 계산한 것들을 list 에 추가. 로그 출력\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    ###################### 검증 - valid_loader ###################### \n",
    "    f_model.eval()\n",
    "    # 검증결과 저장할 변수\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    # weight 업데이트는 하지 않는다. 추론할때  grad_fn을 구할 필요가 없다.\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in valid_loader:\n",
    "\n",
    "            # 1. device 로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "\n",
    "            # 2. 추론\n",
    "            pred_valid = f_model(X_valid)\n",
    "\n",
    "            # 3. 검증 작업 - loss, accuracy\n",
    "            ## loss를 valid_loss 에 누적.\n",
    "            valid_loss = valid_loss + loss_fn(pred_valid, y_valid).item()\n",
    "            ## 정확도\n",
    "            pred_class = pred_valid.argmax(dim=-1)  # ((256, 10) -> 10당 max값의 index를 조회(256, 1)마지막 축을 기준으로 max을 구하도록함.\n",
    "            valid_acc = valid_acc + torch.sum(pred_class == y_valid).item()\n",
    "        \n",
    "        # 검증 결과를 계산 (평균)\n",
    "        valid_loss = valid_loss / len(valid_loader)  # 1 epoch 당 step수로 나누기\n",
    "        valid_acc = valid_acc / len(valid_loader.dataset)  # 총 데이터 개수로 나누기(맞은것의개수/총개수)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        print_log = \"[{}/{}] - train loss: {}, valid loss: {}, valid_acc: {}\"\n",
    "        print(print_log.format(epoch+1, epochs, train_loss, valid_loss, valid_acc))\n",
    "\n",
    "#####################검증 완료 -> 모델 저장 ###############################\n",
    "    if valid_loss < best_score:  # 성능 개선이 되었으면,\n",
    "        torch.save(f_model, save_path)\n",
    "    \n",
    "    \n",
    "    # log 출력\n",
    "        save_log = \">>>>>>>>>> {} Epoch에서 모델 저장: {}. {}에서 {}로 loss가 개선되어 저장함.\"\n",
    "        print(save_log.format(epoch+1, best_score, valid_loss))\n",
    "        best_score = valid_loss  # best score 를 현재 에폭의 valid loss 로 변경.\n",
    "    # 성능이 개선되었으므로  trigger_cnt 를 0 으로 초기화.\n",
    "        trigger_cnt = 0\n",
    "    else : # 성능 개선이 안된 epoch\n",
    "        trigger_cnt += 1\n",
    "    if patience == trigger_cnt: # 조기종료\n",
    "        print(f\"======{epoch+1}에서 조기종료함. {best_score}에서 개선이 안됨=======\")\n",
    "        break # 학습 반복 loop에서 빠져 나온다.\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "print(\"학습에 걸린 시간(초):\", e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 핛급결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(20), train_losses, label=\"Train set\")\n",
    "plt.plot(range(20), valid_losses, label=\"Validation set\")\n",
    "plt.title(\"loss\")\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(20), valid_acc_list)\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.grid(True, linestyle=\":\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argmin(valid_losses)   # epoch  8+1 에서 가장 적은 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int64(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlaid_lossed[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(f_model,.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_8688\\1734544863.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(save_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/fashion_mnist_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m####### 저장된 모델을 이용해 최종평가\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# test  dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## a모델 load\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m best_model\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/fashion_mnist_model.pt'"
     ]
    }
   ],
   "source": [
    "####### 저장된 모델을 이용해 최종평가\n",
    "# test  dataset\n",
    "## 모델 load\n",
    "best_model = torch.load(save_path)\n",
    "## 1`. eval 모드로 변경.\n",
    "best_model.eval()\n",
    "## 2. device 이동\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "## 평가 결과를 저장할 변수\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        # 1. device로 이동\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        # 2. cnfhs\n",
    "        pred_test = best_model(X_test)\n",
    "        # 3. 평가 (loss, accuracy)\n",
    "        ## loss (loss 를 누적)\n",
    "        test_loss = test_loss + loss_fn(pred_test, y_test).item()\n",
    "        ## accuracy (맞은 개수를 누적)\n",
    "        pred_test_class = pred_test.argmax(dim=-1)\n",
    "        test_acc = test_acc +torch.sum(y_test == pred_test_class).item()\n",
    "    #검증결과 계산(평균)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_loss\u001b[49m, test_acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m####### 새로운 데이터로 예측\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 테스트 셋에서 3개를 조회해서 추론\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# testset[[0, 2, 3 ]] slicing/ fancy indexing 이 안됨\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## 3개를 조회한 것을 저장할 변수\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# testset[0][0].shape\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# 1, 28, 28 짜리 세개를 저장할 빈 tensor를 생성.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m new_data[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m testset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m new_data[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m testset[\u001b[38;5;241m10\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "####### 새로운 데이터로 예측\n",
    "# 테스트 셋에서 3개를 조회해서 추론\n",
    "\n",
    "# testset[[0, 2, 3 ]] slicing/ fancy indexing 이 안됨\n",
    "## 3개를 조회한 것을 저장할 변수\n",
    "# testset[0][0].shape\n",
    "\n",
    "new_data = torch.empty(3, 1, 28, 28)  # 1, 28, 28 짜리 세개를 저장할 빈 tensor를 생성.\n",
    "new_data[0] = testset[0][0]\n",
    "new_data[1] = testset[10][0]\n",
    "new_data[2] = testset[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m(new_data)\n\u001b[0;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "p = best_model(new_data)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p_class \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m p_class\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p_class = p.argmax(dim=-1)\n",
    "p_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_prob = p.max(dimg=-1).values\n",
    "p_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = nn.Softmax(dim=-1)(p)\n",
    "p.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, device=\"cpu\"):\n",
    "    # model 로 X를 추정한 결과를 반환. (class_index, 정답의 확률)\n",
    "    model = model.to(device)\n",
    "    model_eval()\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        proba = nn.Softmax(dim=-1)(pred)  # 모델 추론 값을 확률로 변환. \n",
    "        m = proba.max(dim=-1)\n",
    "        return m.indices, m.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(best_model, new_data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_index, prov in zip(*result):\n",
    "    print(class_index.item(), testset.classes[class_index], prob.item(), sep=\" - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 확인\n",
    "testset[0][1], testset[10][1], testset[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "# X.shape, y.shape, X.dtype\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name <-> class index\n",
    "classes = np.array([\"악성종양\", \"양성종양\"])\n",
    "class_to_idx = {\"악성종약\":0, \"양성종양\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "## 모델의 weight, bias -> float32. X, y 는 weight, bias 와 계산을 하게 되기 때문에 타입을 맞춰준다.\n",
    "\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.classes = classes\n",
    "trainset.class_to_idx = class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 모델 정의\n",
    "class  BreastCancerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32)\n",
    "        self.lr2 = nn.Linear(32, 8)\n",
    "        self.lr3 = nn.Linear(8, 1) # 출력 Layer처리하는 함수로, out_features=1 : 2진 분류의 positive 일 확률 값을 정해줌.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logistic = nn.Sigmoid()  # 입력 값을  0 ~ 1 사이의 실수로 반환.\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        # 출력 Layer\n",
    "        output = self.lr3(X)\n",
    "        output = self.logistic(output)  # 0 ~ 1 까지의 확률값을 리턴하게 하는 함수\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape  #      lr = nn.Linear(30, 32) 30 의 feature  갯수를 맞춰줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (lr3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (logistic): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model =  BreastCancerModel()\n",
    "b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BreastCancerModel                        [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 32]                  992\n",
       "├─ReLU: 1-2                              [10, 32]                  --\n",
       "├─Linear: 1-3                            [10, 8]                   264\n",
       "├─ReLU: 1-4                              [10, 8]                   --\n",
       "├─Linear: 1-5                            [10, 1]                   9\n",
       "├─Sigmoid: 1-6                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 1,265\n",
       "Trainable params: 1,265\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(b_model, (10, 30), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4335],\n",
       "        [0.4174],\n",
       "        [0.4122],\n",
       "        [0.4135],\n",
       "        [0.4139],\n",
       "        [0.3801],\n",
       "        [0.4244],\n",
       "        [0.4004],\n",
       "        [0.4151],\n",
       "        [0.4501]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy data로 출력\n",
    "dummy_x = torch.randn(10, 30)\n",
    "# dummy_x.shape\n",
    "result = b_model(dummy_x)\n",
    "# result.shape\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).type(torch.int32)  # bool -> int (True: 1, False: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "######### 학습(Train)\n",
    "b_model = b_model.to(device)\n",
    "optimizer = optim.Adam(b_model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()  # 함수 이름 : binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] train loss:0.7371928989887238, valid loss:0.7337394952774048, valid accuracy:0.3706293706293706\n",
      ">>>>>> 1에서 성능이 개선되어 저장합니다.0.7337394952774048\n",
      "[0002/1000] train loss:0.7284724414348602, valid loss:0.7223255932331085, valid accuracy:0.3706293706293706\n",
      ">>>>>> 2에서 성능이 개선되어 저장합니다.0.7223255932331085\n",
      "[0003/1000] train loss:0.7109061777591705, valid loss:0.7112181782722473, valid accuracy:0.3706293706293706\n",
      ">>>>>> 3에서 성능이 개선되어 저장합니다.0.7112181782722473\n",
      "[0004/1000] train loss:0.7021308541297913, valid loss:0.700193852186203, valid accuracy:0.3706293706293706\n",
      ">>>>>> 4에서 성능이 개선되어 저장합니다.0.700193852186203\n",
      "[0005/1000] train loss:0.6918149888515472, valid loss:0.6892870366573334, valid accuracy:0.3706293706293706\n",
      ">>>>>> 5에서 성능이 개선되어 저장합니다.0.6892870366573334\n",
      "[0006/1000] train loss:0.6810141503810883, valid loss:0.6783824861049652, valid accuracy:0.3706293706293706\n",
      ">>>>>> 6에서 성능이 개선되어 저장합니다.0.6783824861049652\n",
      "[0007/1000] train loss:0.668532133102417, valid loss:0.6676412224769592, valid accuracy:0.3706293706293706\n",
      ">>>>>> 7에서 성능이 개선되어 저장합니다.0.6676412224769592\n",
      "[0008/1000] train loss:0.6578919589519501, valid loss:0.6571474373340607, valid accuracy:0.3706293706293706\n",
      ">>>>>> 8에서 성능이 개선되어 저장합니다.0.6571474373340607\n",
      "[0009/1000] train loss:0.6430439651012421, valid loss:0.646863728761673, valid accuracy:0.3776223776223776\n",
      ">>>>>> 9에서 성능이 개선되어 저장합니다.0.646863728761673\n",
      "[0010/1000] train loss:0.6383154988288879, valid loss:0.6370396316051483, valid accuracy:0.3776223776223776\n",
      ">>>>>> 10에서 성능이 개선되어 저장합니다.0.6370396316051483\n",
      "[0011/1000] train loss:0.6232902407646179, valid loss:0.6274192035198212, valid accuracy:0.3986013986013986\n",
      ">>>>>> 11에서 성능이 개선되어 저장합니다.0.6274192035198212\n",
      "[0012/1000] train loss:0.6151330173015594, valid loss:0.6177895367145538, valid accuracy:0.43356643356643354\n",
      ">>>>>> 12에서 성능이 개선되어 저장합니다.0.6177895367145538\n",
      "[0013/1000] train loss:0.6042275726795197, valid loss:0.608023077249527, valid accuracy:0.48951048951048953\n",
      ">>>>>> 13에서 성능이 개선되어 저장합니다.0.608023077249527\n",
      "[0014/1000] train loss:0.597091943025589, valid loss:0.5981874763965607, valid accuracy:0.5804195804195804\n",
      ">>>>>> 14에서 성능이 개선되어 저장합니다.0.5981874763965607\n",
      "[0015/1000] train loss:0.5845668613910675, valid loss:0.587943822145462, valid accuracy:0.6713286713286714\n",
      ">>>>>> 15에서 성능이 개선되어 저장합니다.0.587943822145462\n",
      "[0016/1000] train loss:0.5717726945877075, valid loss:0.5771433711051941, valid accuracy:0.7552447552447552\n",
      ">>>>>> 16에서 성능이 개선되어 저장합니다.0.5771433711051941\n",
      "[0017/1000] train loss:0.5656805336475372, valid loss:0.565828412771225, valid accuracy:0.8041958041958042\n",
      ">>>>>> 17에서 성능이 개선되어 저장합니다.0.565828412771225\n",
      "[0018/1000] train loss:0.5500165224075317, valid loss:0.5539475083351135, valid accuracy:0.8181818181818182\n",
      ">>>>>> 18에서 성능이 개선되어 저장합니다.0.5539475083351135\n",
      "[0019/1000] train loss:0.5408906042575836, valid loss:0.5414899587631226, valid accuracy:0.8461538461538461\n",
      ">>>>>> 19에서 성능이 개선되어 저장합니다.0.5414899587631226\n",
      "[0020/1000] train loss:0.5231011509895325, valid loss:0.5285000503063202, valid accuracy:0.8601398601398601\n",
      ">>>>>> 20에서 성능이 개선되어 저장합니다.0.5285000503063202\n",
      "[0021/1000] train loss:0.5115914344787598, valid loss:0.5149424076080322, valid accuracy:0.8601398601398601\n",
      ">>>>>> 21에서 성능이 개선되어 저장합니다.0.5149424076080322\n",
      "[0022/1000] train loss:0.4961360841989517, valid loss:0.5009763091802597, valid accuracy:0.8741258741258742\n",
      ">>>>>> 22에서 성능이 개선되어 저장합니다.0.5009763091802597\n",
      "[0023/1000] train loss:0.4800761044025421, valid loss:0.4866810739040375, valid accuracy:0.8811188811188811\n",
      ">>>>>> 23에서 성능이 개선되어 저장합니다.0.4866810739040375\n",
      "[0024/1000] train loss:0.4636767655611038, valid loss:0.47192399203777313, valid accuracy:0.8951048951048951\n",
      ">>>>>> 24에서 성능이 개선되어 저장합니다.0.47192399203777313\n",
      "[0025/1000] train loss:0.4436197429895401, valid loss:0.4568590521812439, valid accuracy:0.8881118881118881\n",
      ">>>>>> 25에서 성능이 개선되어 저장합니다.0.4568590521812439\n",
      "[0026/1000] train loss:0.42873385548591614, valid loss:0.4415923207998276, valid accuracy:0.8881118881118881\n",
      ">>>>>> 26에서 성능이 개선되어 저장합니다.0.4415923207998276\n",
      "[0027/1000] train loss:0.4150923043489456, valid loss:0.4262646436691284, valid accuracy:0.9020979020979021\n",
      ">>>>>> 27에서 성능이 개선되어 저장합니다.0.4262646436691284\n",
      "[0028/1000] train loss:0.3956913948059082, valid loss:0.4109092652797699, valid accuracy:0.9090909090909091\n",
      ">>>>>> 28에서 성능이 개선되어 저장합니다.0.4109092652797699\n",
      "[0029/1000] train loss:0.37904247641563416, valid loss:0.3955574333667755, valid accuracy:0.9090909090909091\n",
      ">>>>>> 29에서 성능이 개선되어 저장합니다.0.3955574333667755\n",
      "[0030/1000] train loss:0.3610909581184387, valid loss:0.380337655544281, valid accuracy:0.9090909090909091\n",
      ">>>>>> 30에서 성능이 개선되어 저장합니다.0.380337655544281\n",
      "[0031/1000] train loss:0.34432514011859894, valid loss:0.3653101325035095, valid accuracy:0.9090909090909091\n",
      ">>>>>> 31에서 성능이 개선되어 저장합니다.0.3653101325035095\n",
      "[0032/1000] train loss:0.33033835887908936, valid loss:0.35063815116882324, valid accuracy:0.9090909090909091\n",
      ">>>>>> 32에서 성능이 개선되어 저장합니다.0.35063815116882324\n",
      "[0033/1000] train loss:0.3216748833656311, valid loss:0.3362952023744583, valid accuracy:0.9090909090909091\n",
      ">>>>>> 33에서 성능이 개선되어 저장합니다.0.3362952023744583\n",
      "[0034/1000] train loss:0.30050668120384216, valid loss:0.32244250178337097, valid accuracy:0.916083916083916\n",
      ">>>>>> 34에서 성능이 개선되어 저장합니다.0.32244250178337097\n",
      "[0035/1000] train loss:0.28518329560756683, valid loss:0.3090322017669678, valid accuracy:0.9230769230769231\n",
      ">>>>>> 35에서 성능이 개선되어 저장합니다.0.3090322017669678\n",
      "[0036/1000] train loss:0.2735532075166702, valid loss:0.2961672842502594, valid accuracy:0.9230769230769231\n",
      ">>>>>> 36에서 성능이 개선되어 저장합니다.0.2961672842502594\n",
      "[0037/1000] train loss:0.2638741433620453, valid loss:0.2839774936437607, valid accuracy:0.9230769230769231\n",
      ">>>>>> 37에서 성능이 개선되어 저장합니다.0.2839774936437607\n",
      "[0038/1000] train loss:0.24150528758764267, valid loss:0.27245035767555237, valid accuracy:0.9230769230769231\n",
      ">>>>>> 38에서 성능이 개선되어 저장합니다.0.27245035767555237\n",
      "[0039/1000] train loss:0.23526804149150848, valid loss:0.2615024000406265, valid accuracy:0.9230769230769231\n",
      ">>>>>> 39에서 성능이 개선되어 저장합니다.0.2615024000406265\n",
      "[0040/1000] train loss:0.22321422398090363, valid loss:0.25128577649593353, valid accuracy:0.9230769230769231\n",
      ">>>>>> 40에서 성능이 개선되어 저장합니다.0.25128577649593353\n",
      "[0041/1000] train loss:0.21337907761335373, valid loss:0.24169211089611053, valid accuracy:0.9300699300699301\n",
      ">>>>>> 41에서 성능이 개선되어 저장합니다.0.24169211089611053\n",
      "[0042/1000] train loss:0.20832164585590363, valid loss:0.23269115388393402, valid accuracy:0.9370629370629371\n",
      ">>>>>> 42에서 성능이 개선되어 저장합니다.0.23269115388393402\n",
      "[0043/1000] train loss:0.19409476220607758, valid loss:0.22427357733249664, valid accuracy:0.9370629370629371\n",
      ">>>>>> 43에서 성능이 개선되어 저장합니다.0.22427357733249664\n",
      "[0044/1000] train loss:0.18691646307706833, valid loss:0.21636927872896194, valid accuracy:0.9370629370629371\n",
      ">>>>>> 44에서 성능이 개선되어 저장합니다.0.21636927872896194\n",
      "[0045/1000] train loss:0.16935387253761292, valid loss:0.20904278755187988, valid accuracy:0.9440559440559441\n",
      ">>>>>> 45에서 성능이 개선되어 저장합니다.0.20904278755187988\n",
      "[0046/1000] train loss:0.17037926614284515, valid loss:0.20229249447584152, valid accuracy:0.9440559440559441\n",
      ">>>>>> 46에서 성능이 개선되어 저장합니다.0.20229249447584152\n",
      "[0047/1000] train loss:0.15986650437116623, valid loss:0.19602859020233154, valid accuracy:0.9440559440559441\n",
      ">>>>>> 47에서 성능이 개선되어 저장합니다.0.19602859020233154\n",
      "[0048/1000] train loss:0.15598905831575394, valid loss:0.1901291385293007, valid accuracy:0.951048951048951\n",
      ">>>>>> 48에서 성능이 개선되어 저장합니다.0.1901291385293007\n",
      "[0049/1000] train loss:0.14979411661624908, valid loss:0.18461541086435318, valid accuracy:0.951048951048951\n",
      ">>>>>> 49에서 성능이 개선되어 저장합니다.0.18461541086435318\n",
      "[0050/1000] train loss:0.14099878817796707, valid loss:0.17952127754688263, valid accuracy:0.951048951048951\n",
      ">>>>>> 50에서 성능이 개선되어 저장합니다.0.17952127754688263\n",
      "[0051/1000] train loss:0.13875652104616165, valid loss:0.17473406344652176, valid accuracy:0.951048951048951\n",
      ">>>>>> 51에서 성능이 개선되어 저장합니다.0.17473406344652176\n",
      "[0052/1000] train loss:0.13014010712504387, valid loss:0.17033693939447403, valid accuracy:0.9440559440559441\n",
      ">>>>>> 52에서 성능이 개선되어 저장합니다.0.17033693939447403\n",
      "[0053/1000] train loss:0.12287040427327156, valid loss:0.16624468564987183, valid accuracy:0.9440559440559441\n",
      ">>>>>> 53에서 성능이 개선되어 저장합니다.0.16624468564987183\n",
      "[0054/1000] train loss:0.12265866994857788, valid loss:0.16243838518857956, valid accuracy:0.9440559440559441\n",
      ">>>>>> 54에서 성능이 개선되어 저장합니다.0.16243838518857956\n",
      "[0055/1000] train loss:0.11769530177116394, valid loss:0.15895792841911316, valid accuracy:0.9440559440559441\n",
      ">>>>>> 55에서 성능이 개선되어 저장합니다.0.15895792841911316\n",
      "[0056/1000] train loss:0.1192617230117321, valid loss:0.1556682214140892, valid accuracy:0.9370629370629371\n",
      ">>>>>> 56에서 성능이 개선되어 저장합니다.0.1556682214140892\n",
      "[0057/1000] train loss:0.10965460911393166, valid loss:0.1526237055659294, valid accuracy:0.9370629370629371\n",
      ">>>>>> 57에서 성능이 개선되어 저장합니다.0.1526237055659294\n",
      "[0058/1000] train loss:0.10691908374428749, valid loss:0.14982811361551285, valid accuracy:0.9370629370629371\n",
      ">>>>>> 58에서 성능이 개선되어 저장합니다.0.14982811361551285\n",
      "[0059/1000] train loss:0.10164959356188774, valid loss:0.14720193296670914, valid accuracy:0.9370629370629371\n",
      ">>>>>> 59에서 성능이 개선되어 저장합니다.0.14720193296670914\n",
      "[0060/1000] train loss:0.10278070718050003, valid loss:0.14473922550678253, valid accuracy:0.9370629370629371\n",
      ">>>>>> 60에서 성능이 개선되어 저장합니다.0.14473922550678253\n",
      "[0061/1000] train loss:0.09914803132414818, valid loss:0.1423884853720665, valid accuracy:0.9370629370629371\n",
      ">>>>>> 61에서 성능이 개선되어 저장합니다.0.1423884853720665\n",
      "[0062/1000] train loss:0.09880653768777847, valid loss:0.1402515396475792, valid accuracy:0.9370629370629371\n",
      ">>>>>> 62에서 성능이 개선되어 저장합니다.0.1402515396475792\n",
      "[0063/1000] train loss:0.09320712462067604, valid loss:0.1382482945919037, valid accuracy:0.9370629370629371\n",
      ">>>>>> 63에서 성능이 개선되어 저장합니다.0.1382482945919037\n",
      "[0064/1000] train loss:0.08835111185908318, valid loss:0.1363941803574562, valid accuracy:0.9370629370629371\n",
      ">>>>>> 64에서 성능이 개선되어 저장합니다.0.1363941803574562\n",
      "[0065/1000] train loss:0.08910541981458664, valid loss:0.13469254970550537, valid accuracy:0.9370629370629371\n",
      ">>>>>> 65에서 성능이 개선되어 저장합니다.0.13469254970550537\n",
      "[0066/1000] train loss:0.0862562470138073, valid loss:0.13304631412029266, valid accuracy:0.9370629370629371\n",
      ">>>>>> 66에서 성능이 개선되어 저장합니다.0.13304631412029266\n",
      "[0067/1000] train loss:0.07532377913594246, valid loss:0.13148735463619232, valid accuracy:0.9370629370629371\n",
      ">>>>>> 67에서 성능이 개선되어 저장합니다.0.13148735463619232\n",
      "[0068/1000] train loss:0.083656445145607, valid loss:0.1300201639533043, valid accuracy:0.9370629370629371\n",
      ">>>>>> 68에서 성능이 개선되어 저장합니다.0.1300201639533043\n",
      "[0069/1000] train loss:0.08317926153540611, valid loss:0.1286824457347393, valid accuracy:0.9370629370629371\n",
      ">>>>>> 69에서 성능이 개선되어 저장합니다.0.1286824457347393\n",
      "[0070/1000] train loss:0.07169243134558201, valid loss:0.12746039032936096, valid accuracy:0.9370629370629371\n",
      ">>>>>> 70에서 성능이 개선되어 저장합니다.0.12746039032936096\n",
      "[0071/1000] train loss:0.07679640874266624, valid loss:0.12632723525166512, valid accuracy:0.9370629370629371\n",
      ">>>>>> 71에서 성능이 개선되어 저장합니다.0.12632723525166512\n",
      "[0072/1000] train loss:0.07663847878575325, valid loss:0.12526001781225204, valid accuracy:0.9370629370629371\n",
      ">>>>>> 72에서 성능이 개선되어 저장합니다.0.12526001781225204\n",
      "[0073/1000] train loss:0.07232994213700294, valid loss:0.12430886924266815, valid accuracy:0.9370629370629371\n",
      ">>>>>> 73에서 성능이 개선되어 저장합니다.0.12430886924266815\n",
      "[0074/1000] train loss:0.06403700076043606, valid loss:0.12339988350868225, valid accuracy:0.9370629370629371\n",
      ">>>>>> 74에서 성능이 개선되어 저장합니다.0.12339988350868225\n",
      "[0075/1000] train loss:0.07182146236300468, valid loss:0.12254612892866135, valid accuracy:0.9370629370629371\n",
      ">>>>>> 75에서 성능이 개선되어 저장합니다.0.12254612892866135\n",
      "[0076/1000] train loss:0.07131610251963139, valid loss:0.12173927575349808, valid accuracy:0.9370629370629371\n",
      ">>>>>> 76에서 성능이 개선되어 저장합니다.0.12173927575349808\n",
      "[0077/1000] train loss:0.07135604321956635, valid loss:0.12098657339811325, valid accuracy:0.9370629370629371\n",
      ">>>>>> 77에서 성능이 개선되어 저장합니다.0.12098657339811325\n",
      "[0078/1000] train loss:0.06753752194344997, valid loss:0.12029638513922691, valid accuracy:0.9370629370629371\n",
      ">>>>>> 78에서 성능이 개선되어 저장합니다.0.12029638513922691\n",
      "[0079/1000] train loss:0.06802766397595406, valid loss:0.11964445188641548, valid accuracy:0.9370629370629371\n",
      ">>>>>> 79에서 성능이 개선되어 저장합니다.0.11964445188641548\n",
      "[0080/1000] train loss:0.06314620934426785, valid loss:0.11903278902173042, valid accuracy:0.9370629370629371\n",
      ">>>>>> 80에서 성능이 개선되어 저장합니다.0.11903278902173042\n",
      "[0081/1000] train loss:0.06531768292188644, valid loss:0.1184423603117466, valid accuracy:0.9370629370629371\n",
      ">>>>>> 81에서 성능이 개선되어 저장합니다.0.1184423603117466\n",
      "[0082/1000] train loss:0.058569807559251785, valid loss:0.11794112622737885, valid accuracy:0.9370629370629371\n",
      ">>>>>> 82에서 성능이 개선되어 저장합니다.0.11794112622737885\n",
      "[0083/1000] train loss:0.06386615708470345, valid loss:0.11745970696210861, valid accuracy:0.9370629370629371\n",
      ">>>>>> 83에서 성능이 개선되어 저장합니다.0.11745970696210861\n",
      "[0084/1000] train loss:0.059860141947865486, valid loss:0.11697736755013466, valid accuracy:0.9370629370629371\n",
      ">>>>>> 84에서 성능이 개선되어 저장합니다.0.11697736755013466\n",
      "[0085/1000] train loss:0.061177488416433334, valid loss:0.11651308462023735, valid accuracy:0.9370629370629371\n",
      ">>>>>> 85에서 성능이 개선되어 저장합니다.0.11651308462023735\n",
      "[0086/1000] train loss:0.05617075227200985, valid loss:0.11604204401373863, valid accuracy:0.9370629370629371\n",
      ">>>>>> 86에서 성능이 개선되어 저장합니다.0.11604204401373863\n",
      "[0087/1000] train loss:0.05725008249282837, valid loss:0.11559703201055527, valid accuracy:0.9370629370629371\n",
      ">>>>>> 87에서 성능이 개선되어 저장합니다.0.11559703201055527\n",
      "[0088/1000] train loss:0.048182470723986626, valid loss:0.11517585813999176, valid accuracy:0.9370629370629371\n",
      ">>>>>> 88에서 성능이 개선되어 저장합니다.0.11517585813999176\n",
      "[0089/1000] train loss:0.058303359895944595, valid loss:0.1147642731666565, valid accuracy:0.9370629370629371\n",
      ">>>>>> 89에서 성능이 개선되어 저장합니다.0.1147642731666565\n",
      "[0090/1000] train loss:0.05730048194527626, valid loss:0.11438146606087685, valid accuracy:0.9370629370629371\n",
      ">>>>>> 90에서 성능이 개선되어 저장합니다.0.11438146606087685\n",
      "[0091/1000] train loss:0.05670018307864666, valid loss:0.11398934200406075, valid accuracy:0.9370629370629371\n",
      ">>>>>> 91에서 성능이 개선되어 저장합니다.0.11398934200406075\n",
      "[0092/1000] train loss:0.05265004746615887, valid loss:0.11365567520260811, valid accuracy:0.9370629370629371\n",
      ">>>>>> 92에서 성능이 개선되어 저장합니다.0.11365567520260811\n",
      "[0093/1000] train loss:0.05500019155442715, valid loss:0.11334594339132309, valid accuracy:0.9440559440559441\n",
      ">>>>>> 93에서 성능이 개선되어 저장합니다.0.11334594339132309\n",
      "[0094/1000] train loss:0.0540090911090374, valid loss:0.11305776238441467, valid accuracy:0.9440559440559441\n",
      ">>>>>> 94에서 성능이 개선되어 저장합니다.0.11305776238441467\n",
      "[0095/1000] train loss:0.053967271000146866, valid loss:0.11277646198868752, valid accuracy:0.9440559440559441\n",
      ">>>>>> 95에서 성능이 개선되어 저장합니다.0.11277646198868752\n",
      "[0096/1000] train loss:0.053892336785793304, valid loss:0.11248204857110977, valid accuracy:0.9440559440559441\n",
      ">>>>>> 96에서 성능이 개선되어 저장합니다.0.11248204857110977\n",
      "[0097/1000] train loss:0.05135917663574219, valid loss:0.11219669878482819, valid accuracy:0.9440559440559441\n",
      ">>>>>> 97에서 성능이 개선되어 저장합니다.0.11219669878482819\n",
      "[0098/1000] train loss:0.03961630165576935, valid loss:0.11198339611291885, valid accuracy:0.951048951048951\n",
      ">>>>>> 98에서 성능이 개선되어 저장합니다.0.11198339611291885\n",
      "[0099/1000] train loss:0.04772801138460636, valid loss:0.11177809908986092, valid accuracy:0.951048951048951\n",
      ">>>>>> 99에서 성능이 개선되어 저장합니다.0.11177809908986092\n",
      "[0100/1000] train loss:0.04956739954650402, valid loss:0.11153854802250862, valid accuracy:0.951048951048951\n",
      ">>>>>> 100에서 성능이 개선되어 저장합니다.0.11153854802250862\n",
      "[0101/1000] train loss:0.04760423302650452, valid loss:0.11135372519493103, valid accuracy:0.951048951048951\n",
      ">>>>>> 101에서 성능이 개선되어 저장합니다.0.11135372519493103\n",
      "[0102/1000] train loss:0.048489321023225784, valid loss:0.11116810888051987, valid accuracy:0.951048951048951\n",
      ">>>>>> 102에서 성능이 개선되어 저장합니다.0.11116810888051987\n",
      "[0103/1000] train loss:0.04783453419804573, valid loss:0.1110086627304554, valid accuracy:0.951048951048951\n",
      ">>>>>> 103에서 성능이 개선되어 저장합니다.0.1110086627304554\n",
      "[0104/1000] train loss:0.04880823567509651, valid loss:0.11086253821849823, valid accuracy:0.9440559440559441\n",
      ">>>>>> 104에서 성능이 개선되어 저장합니다.0.11086253821849823\n",
      "[0105/1000] train loss:0.04731048829853535, valid loss:0.11066602170467377, valid accuracy:0.9440559440559441\n",
      ">>>>>> 105에서 성능이 개선되어 저장합니다.0.11066602170467377\n",
      "[0106/1000] train loss:0.048134672455489635, valid loss:0.1104595698416233, valid accuracy:0.9440559440559441\n",
      ">>>>>> 106에서 성능이 개선되어 저장합니다.0.1104595698416233\n",
      "[0107/1000] train loss:0.04527375474572182, valid loss:0.11028901487588882, valid accuracy:0.9440559440559441\n",
      ">>>>>> 107에서 성능이 개선되어 저장합니다.0.11028901487588882\n",
      "[0108/1000] train loss:0.04714689776301384, valid loss:0.1101144403219223, valid accuracy:0.9440559440559441\n",
      ">>>>>> 108에서 성능이 개선되어 저장합니다.0.1101144403219223\n",
      "[0109/1000] train loss:0.03684091009199619, valid loss:0.10989585146307945, valid accuracy:0.9440559440559441\n",
      ">>>>>> 109에서 성능이 개선되어 저장합니다.0.10989585146307945\n",
      "[0110/1000] train loss:0.04519657604396343, valid loss:0.10969426855444908, valid accuracy:0.9440559440559441\n",
      ">>>>>> 110에서 성능이 개선되어 저장합니다.0.10969426855444908\n",
      "[0111/1000] train loss:0.04591930843889713, valid loss:0.10950633883476257, valid accuracy:0.9440559440559441\n",
      ">>>>>> 111에서 성능이 개선되어 저장합니다.0.10950633883476257\n",
      "[0112/1000] train loss:0.04512829706072807, valid loss:0.10939774662256241, valid accuracy:0.9440559440559441\n",
      ">>>>>> 112에서 성능이 개선되어 저장합니다.0.10939774662256241\n",
      "[0113/1000] train loss:0.04133516736328602, valid loss:0.10918863117694855, valid accuracy:0.9440559440559441\n",
      ">>>>>> 113에서 성능이 개선되어 저장합니다.0.10918863117694855\n",
      "[0114/1000] train loss:0.04414813965559006, valid loss:0.10902946814894676, valid accuracy:0.9440559440559441\n",
      ">>>>>> 114에서 성능이 개선되어 저장합니다.0.10902946814894676\n",
      "[0115/1000] train loss:0.041906874626874924, valid loss:0.10891073197126389, valid accuracy:0.9440559440559441\n",
      ">>>>>> 115에서 성능이 개선되어 저장합니다.0.10891073197126389\n",
      "[0116/1000] train loss:0.04385592043399811, valid loss:0.10877792537212372, valid accuracy:0.9440559440559441\n",
      ">>>>>> 116에서 성능이 개선되어 저장합니다.0.10877792537212372\n",
      "[0117/1000] train loss:0.04174815583974123, valid loss:0.10862762853503227, valid accuracy:0.9440559440559441\n",
      ">>>>>> 117에서 성능이 개선되어 저장합니다.0.10862762853503227\n",
      "[0118/1000] train loss:0.04008249379694462, valid loss:0.10840623080730438, valid accuracy:0.9440559440559441\n",
      ">>>>>> 118에서 성능이 개선되어 저장합니다.0.10840623080730438\n",
      "[0119/1000] train loss:0.04132594261318445, valid loss:0.10823691636323929, valid accuracy:0.9440559440559441\n",
      ">>>>>> 119에서 성능이 개선되어 저장합니다.0.10823691636323929\n",
      "[0120/1000] train loss:0.0417109364643693, valid loss:0.10807689651846886, valid accuracy:0.9440559440559441\n",
      ">>>>>> 120에서 성능이 개선되어 저장합니다.0.10807689651846886\n",
      "[0121/1000] train loss:0.04082325100898743, valid loss:0.10797084867954254, valid accuracy:0.9440559440559441\n",
      ">>>>>> 121에서 성능이 개선되어 저장합니다.0.10797084867954254\n",
      "[0122/1000] train loss:0.03147854097187519, valid loss:0.10785999894142151, valid accuracy:0.9440559440559441\n",
      ">>>>>> 122에서 성능이 개선되어 저장합니다.0.10785999894142151\n",
      "[0123/1000] train loss:0.04109025560319424, valid loss:0.10769162327051163, valid accuracy:0.9440559440559441\n",
      ">>>>>> 123에서 성능이 개선되어 저장합니다.0.10769162327051163\n",
      "[0124/1000] train loss:0.040459190495312214, valid loss:0.10759837180376053, valid accuracy:0.9440559440559441\n",
      ">>>>>> 124에서 성능이 개선되어 저장합니다.0.10759837180376053\n",
      "[0125/1000] train loss:0.03687707148492336, valid loss:0.1075747162103653, valid accuracy:0.9440559440559441\n",
      ">>>>>> 125에서 성능이 개선되어 저장합니다.0.1075747162103653\n",
      "[0126/1000] train loss:0.03986874222755432, valid loss:0.10754595696926117, valid accuracy:0.9440559440559441\n",
      ">>>>>> 126에서 성능이 개선되어 저장합니다.0.10754595696926117\n",
      "[0127/1000] train loss:0.03780538123100996, valid loss:0.10758842527866364, valid accuracy:0.9440559440559441\n",
      "[0128/1000] train loss:0.03936826437711716, valid loss:0.10758472606539726, valid accuracy:0.9440559440559441\n",
      "[0129/1000] train loss:0.03892188984900713, valid loss:0.10761842504143715, valid accuracy:0.9440559440559441\n",
      "[0130/1000] train loss:0.027759026736021042, valid loss:0.10755979269742966, valid accuracy:0.9440559440559441\n",
      "[0131/1000] train loss:0.03522241674363613, valid loss:0.10750472545623779, valid accuracy:0.9440559440559441\n",
      ">>>>>> 131에서 성능이 개선되어 저장합니다.0.10750472545623779\n",
      "[0132/1000] train loss:0.0381021685898304, valid loss:0.10743383690714836, valid accuracy:0.9440559440559441\n",
      ">>>>>> 132에서 성능이 개선되어 저장합니다.0.10743383690714836\n",
      "[0133/1000] train loss:0.037432760931551456, valid loss:0.10743384808301926, valid accuracy:0.9440559440559441\n",
      "[0134/1000] train loss:0.035010757856070995, valid loss:0.1074136421084404, valid accuracy:0.9440559440559441\n",
      ">>>>>> 134에서 성능이 개선되어 저장합니다.0.1074136421084404\n",
      "[0135/1000] train loss:0.0339730903506279, valid loss:0.10735240951180458, valid accuracy:0.9440559440559441\n",
      ">>>>>> 135에서 성능이 개선되어 저장합니다.0.10735240951180458\n",
      "[0136/1000] train loss:0.03691413998603821, valid loss:0.1072930134832859, valid accuracy:0.9440559440559441\n",
      ">>>>>> 136에서 성능이 개선되어 저장합니다.0.1072930134832859\n",
      "[0137/1000] train loss:0.03621078561991453, valid loss:0.10728369280695915, valid accuracy:0.9440559440559441\n",
      ">>>>>> 137에서 성능이 개선되어 저장합니다.0.10728369280695915\n",
      "[0138/1000] train loss:0.035387568175792694, valid loss:0.1072642058134079, valid accuracy:0.9440559440559441\n",
      ">>>>>> 138에서 성능이 개선되어 저장합니다.0.1072642058134079\n",
      "[0139/1000] train loss:0.03366867080330849, valid loss:0.10715646296739578, valid accuracy:0.9440559440559441\n",
      ">>>>>> 139에서 성능이 개선되어 저장합니다.0.10715646296739578\n",
      "[0140/1000] train loss:0.035630011931061745, valid loss:0.10705291479825974, valid accuracy:0.9440559440559441\n",
      ">>>>>> 140에서 성능이 개선되어 저장합니다.0.10705291479825974\n",
      "[0141/1000] train loss:0.03574547450989485, valid loss:0.10694872960448265, valid accuracy:0.9440559440559441\n",
      ">>>>>> 141에서 성능이 개선되어 저장합니다.0.10694872960448265\n",
      "[0142/1000] train loss:0.034965154714882374, valid loss:0.10682116821408272, valid accuracy:0.9440559440559441\n",
      ">>>>>> 142에서 성능이 개선되어 저장합니다.0.10682116821408272\n",
      "[0143/1000] train loss:0.03491743374615908, valid loss:0.106818787753582, valid accuracy:0.9440559440559441\n",
      ">>>>>> 143에서 성능이 개선되어 저장합니다.0.106818787753582\n",
      "[0144/1000] train loss:0.033014170825481415, valid loss:0.10677601024508476, valid accuracy:0.9440559440559441\n",
      ">>>>>> 144에서 성능이 개선되어 저장합니다.0.10677601024508476\n",
      "[0145/1000] train loss:0.021671119146049023, valid loss:0.10672793164849281, valid accuracy:0.9440559440559441\n",
      ">>>>>> 145에서 성능이 개선되어 저장합니다.0.10672793164849281\n",
      "[0146/1000] train loss:0.0341257294639945, valid loss:0.10671539977192879, valid accuracy:0.9440559440559441\n",
      ">>>>>> 146에서 성능이 개선되어 저장합니다.0.10671539977192879\n",
      "[0147/1000] train loss:0.03404391277581453, valid loss:0.10666001588106155, valid accuracy:0.9440559440559441\n",
      ">>>>>> 147에서 성능이 개선되어 저장합니다.0.10666001588106155\n",
      "[0148/1000] train loss:0.033679510466754436, valid loss:0.10659119114279747, valid accuracy:0.951048951048951\n",
      ">>>>>> 148에서 성능이 개선되어 저장합니다.0.10659119114279747\n",
      "[0149/1000] train loss:0.033362952060997486, valid loss:0.10655352473258972, valid accuracy:0.951048951048951\n",
      ">>>>>> 149에서 성능이 개선되어 저장합니다.0.10655352473258972\n",
      "[0150/1000] train loss:0.03304037172347307, valid loss:0.10653695464134216, valid accuracy:0.951048951048951\n",
      ">>>>>> 150에서 성능이 개선되어 저장합니다.0.10653695464134216\n",
      "[0151/1000] train loss:0.02959386631846428, valid loss:0.10644685477018356, valid accuracy:0.951048951048951\n",
      ">>>>>> 151에서 성능이 개선되어 저장합니다.0.10644685477018356\n",
      "[0152/1000] train loss:0.030182271264493465, valid loss:0.10638843849301338, valid accuracy:0.951048951048951\n",
      ">>>>>> 152에서 성능이 개선되어 저장합니다.0.10638843849301338\n",
      "[0153/1000] train loss:0.03183732181787491, valid loss:0.1063755713403225, valid accuracy:0.951048951048951\n",
      ">>>>>> 153에서 성능이 개선되어 저장합니다.0.1063755713403225\n",
      "[0154/1000] train loss:0.03140386659651995, valid loss:0.10631183534860611, valid accuracy:0.951048951048951\n",
      ">>>>>> 154에서 성능이 개선되어 저장합니다.0.10631183534860611\n",
      "[0155/1000] train loss:0.029888149350881577, valid loss:0.10619472339749336, valid accuracy:0.951048951048951\n",
      ">>>>>> 155에서 성능이 개선되어 저장합니다.0.10619472339749336\n",
      "[0156/1000] train loss:0.0315501494333148, valid loss:0.10614366456866264, valid accuracy:0.951048951048951\n",
      ">>>>>> 156에서 성능이 개선되어 저장합니다.0.10614366456866264\n",
      "[0157/1000] train loss:0.031760855577886105, valid loss:0.10606113076210022, valid accuracy:0.951048951048951\n",
      ">>>>>> 157에서 성능이 개선되어 저장합니다.0.10606113076210022\n",
      "[0158/1000] train loss:0.03034509439021349, valid loss:0.1059940718114376, valid accuracy:0.951048951048951\n",
      ">>>>>> 158에서 성능이 개선되어 저장합니다.0.1059940718114376\n",
      "[0159/1000] train loss:0.030857885256409645, valid loss:0.10599355027079582, valid accuracy:0.951048951048951\n",
      ">>>>>> 159에서 성능이 개선되어 저장합니다.0.10599355027079582\n",
      "[0160/1000] train loss:0.02990719210356474, valid loss:0.1060018427670002, valid accuracy:0.951048951048951\n",
      "[0161/1000] train loss:0.03065673913806677, valid loss:0.10604942589998245, valid accuracy:0.951048951048951\n",
      "[0162/1000] train loss:0.03036918118596077, valid loss:0.10606499016284943, valid accuracy:0.951048951048951\n",
      "[0163/1000] train loss:0.02987050823867321, valid loss:0.10611804947257042, valid accuracy:0.951048951048951\n",
      "[0164/1000] train loss:0.029638202860951424, valid loss:0.10617832839488983, valid accuracy:0.951048951048951\n",
      "[0165/1000] train loss:0.029408281669020653, valid loss:0.10614283010363579, valid accuracy:0.951048951048951\n",
      "[0166/1000] train loss:0.029337133280932903, valid loss:0.10612216964364052, valid accuracy:0.951048951048951\n",
      "[0167/1000] train loss:0.02659889403730631, valid loss:0.10614950954914093, valid accuracy:0.951048951048951\n",
      "[0168/1000] train loss:0.02906052116304636, valid loss:0.10616262629628181, valid accuracy:0.951048951048951\n",
      "[0169/1000] train loss:0.028653896413743496, valid loss:0.10611528158187866, valid accuracy:0.951048951048951\n",
      "169 에폭에서 조기종료 합니다. 0.10599355027079582에서 개선되지 않음.\n",
      "걸린시간(초): 0.9818670749664307\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "### 모델 학습 (train) 로직 작성\n",
    "#### 검증 결과 -> train_loss, valid_loss, valid_accuracy\n",
    "### 모델 성능이 개선될 떄 마다 저장.\n",
    "### 조기종료  -10 epoch 동안 성능 개선이 없으면 조기종료\n",
    "\n",
    "save_path = \"saved_models/bc_model.pt\"\n",
    "best_score = torch.inf   # validation loss 기준으로 저장/ 조기종료 여부 확인.\n",
    "patience = 10\n",
    "trigger_cnt = 0 \n",
    "\n",
    "train_losses, valid_losses, valid_acces = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ################## Train ################## \n",
    "    b_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        pred = b_model(X_train)  # positive(양성)일 확률\n",
    "        loss = loss_fn(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)   \n",
    "\n",
    "\n",
    "    ################## validation ################## \n",
    "    b_model.eval()\n",
    "    valid_loss = valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            pred_test = b_model(X_test)  # positive(양성)일 확률\n",
    "            valid_loss +=loss_fn(pred_test, y_test).item()\n",
    "            # 이진 분류에서 accuracy\n",
    "            valid_acc += torch.sum((pred_test > 0.5).type(torch.int32) == y_test).item()\n",
    "        valid_loss /= len(test_loader)\n",
    "        valid_acc /= len(test_loader.dataset)        \n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acces.append(valid_acc)\n",
    "    log_template = \"[{:04}/{}] train loss:{}, valid loss:{}, valid accuracy:{}\"\n",
    "    print(log_template.format(epoch+1, epochs, train_loss, valid_loss, valid_acc))\n",
    "    # 모델 저장, 조기종료\n",
    "    if valid_loss < best_score: # 성능개선\n",
    "        print(f\">>>>>> {epoch+1}에서 성능이 개선되어 저장합니다.{valid_loss}\")\n",
    "        torch.save(b_model, save_path)\n",
    "        best_score = valid_loss\n",
    "        trigger_cnt = 0\n",
    "    else:\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"{epoch+1} 에폭에서 조기종료 합니다. {best_score}에서 개선되지 않음.\")\n",
    "            break\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "print(\"걸린시간(초):\", e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss, acc 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_13168\\334710317.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(save_path)\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bc(model, X, device=device):\n",
    "    # model로 X를 1추론한 결과를 반환\n",
    "    # label, 확률\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        pred_proba = model(X)\n",
    "        pred_class = (pred_proba > 0.5).type(torch.int32)\n",
    "        for class_index, proba in zip(pred_class, pred_proba):\n",
    "            # print(class_index, proba if class_index.item() == 1 else 1-proba)\n",
    "            result.append((class_index.item(), proba if class_index.item() == 1 else 1-proba))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, tensor([0.9965])),\n",
       " (0, tensor([1.0000])),\n",
       " (0, tensor([0.9993])),\n",
       " (1, tensor([0.9995])),\n",
       " (0, tensor([0.9974]))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n",
    "# print(new_data.shape)\n",
    "result = predict_bc(best_model, new_data, device)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m proba \u001b[38;5;241m=\u001b[39m f(y)\n\u001b[0;32m      3\u001b[0m proba\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "f = torch.nn.Softmax(dim=1)\n",
    "proba = f(y)\n",
    "proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proba)\n",
    "proba.sum(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
